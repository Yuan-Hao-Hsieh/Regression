{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <strong>第三次作品：分類器的原理與評比實驗(資料一)</strong>\n",
    "學號：410978002\n",
    "\n",
    "姓名：謝元皓\n",
    "\n",
    "<font color=skyblue>作品描述</font>：\n",
    " \n",
    "本專題計畫執行這篇講義描述的分類器比較，即採用三種分類器分別對三組資料進行分類學習與測試。\n",
    "其中分類器包括\n",
    "\n",
    "- 多元羅吉斯回歸 (Multinomial Logistic Regression)\n",
    "- 支援向量機 (Support Vector Machine)\n",
    "- 神經網路 (Neural Network)\n",
    "\n",
    "此作業包含共三筆資料，分成三份檔案呈現，每份資料分別運用三種分類器進行分類學習與測試。\n",
    "\n",
    "<font color=skyblue>資料描述</font>：\n",
    "第一筆為葡萄酒產地資料，來自 3 個產區，178 瓶葡萄酒，含 13 種葡萄酒成分\n",
    " <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料讀取\n",
    "- 將資料拆成分成訓練資料跟測試資料。\n",
    "- 先不對紅酒資料分組，單純考慮13個產區因子，因此排除最後一欄。\n",
    "- 訓練和測試資料比例為 $4:1$。\n",
    "- 先將原始資料標準化(必須將訓練和測試資料分開標準化)。\n",
    "- 再運用PCA降維創造出主成分資料。\n",
    "- 順便列印出訓練資料跟測試資料的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142, 13) (142,)\n",
      "(36, 13) (36,)\n",
      "(142, 5)\n",
      "(36, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Read data\n",
    "Wine_df = pd.read_excel('Wine.xlsx')\n",
    "X = np.array(Wine_df.iloc[:, :-1]) # 排 除 最 後 一 欄 標 籤\n",
    "y = np.array(Wine_df.iloc[:, -1])\n",
    "# Split data into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "# 主成分分析 (PCA)\n",
    "proportion = 0.8\n",
    "pca = PCA(proportion)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(X_train_scaled.shape, y_train.shape)\n",
    "print(X_test_scaled.shape, y_test.shape) \n",
    "print(X_train_pca.shape)\n",
    "print(X_test_pca.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分類器一: 多元邏吉斯回歸\n",
    "- 需使用套件 sklearn 中的 LogisticRegression\n",
    "- 運用Cross validation 的方式，找出最佳參數。\n",
    "- 需使用到sklearn 中 GridSearchCV套件。\n",
    "- 找出最好參數後，再用此訓練及測試資料。\n",
    "<hr> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step1 運用 Cross validation 的方式，找出最佳的參數\n",
    "#### 套件介紹\n",
    "1. LogisticRegression 為Scikit-learn庫中的Logistic Regression模型\n",
    "2. GridSearchCV, StratifiedShuffleSpli 為 Scikit-learn庫中的網格搜索(GridSearchCV)和分層洗牌分割(StratifiedShuffleSplit)模組。網格搜索用於選擇最佳的超參數，分層洗牌分割用於交叉驗證。\n",
    "3. datetime 為Python的datetime模組，用於處理日期和時間。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 程式碼解析\n",
    "- 讀取 \"Wine.xlsx\"文件，並創建一個文件名，用於存儲網格搜索的結果。文件名的格式是\"df_當前日期時間.csv\"。\n",
    "\n",
    "- 'opts = dict(tol = 1e-6, max_iter = int(1e6))': 這一行程式碼創建了一個包含Logistic Regression模型的參數的字典。tol是訓練的容忍度，max_iter是最大迭代次數。\n",
    "- parameters = {'solver':['lbfgs', 'liblinear', 'newton-cg','sag','saga'], 'C':[0.1, 1, 10]}: 這一行程式碼定義了要進行網格搜索的參數空間。solver是Logistic Regression模型的優化算法，C是正則化強度。\n",
    "\n",
    "- cv = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=0): 這一行程式碼創建了一個分層洗牌分割對象，用於交叉驗證。它將數據集分成5個部分，其中30%作為測試集。\n",
    "\n",
    "- \"grid = GridSearchCV(estimator=LogisticRegression(**opts), param_grid=parameters, cv=cv, scoring=['accuracy','f1_macro'], refit=\"accuracy\")\": 這一行程式碼創建了一個GridSearchCV對象，用於進行網格搜索。它使用了Logistic Regression模型，參數空間是parameters，交叉驗證方法是cv，評估指標包括準確率和f1_macro，並且最終使用準確率作為最佳模型的選擇標準。\n",
    "\n",
    "- grid.fit(X_train, y_train): 這一行程式碼對訓練數據進行網格搜索，找到最佳的超參數組合。\n",
    "\n",
    "- cv_logistic = pd.DataFrame(data = grid.cv_results_): 這一行程式碼將網格搜索的結果轉換為DataFrame格式。\n",
    "\n",
    "- cv_logistic.to_csv(results_file): 這一行程式碼將DataFrame保存為CSV文件，文件名為之前創建的results_file。\n",
    "\n",
    "- print(grid.best_params_): 這一行程式碼打印出最佳超參數的值。\n",
    "\n",
    "- print(grid.best_score_): 這一行程式碼打印出在最佳超參數下的最佳模型得分。\n",
    "\n",
    "- print(grid.best_estimator_): 這一行程式碼打印出最佳模型的具體信息，包括所使用的超參數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python_venv\\ml_evnv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\python_venv\\ml_evnv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\python_venv\\ml_evnv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import  GridSearchCV, \\\n",
    "                        StratifiedShuffleSplit\n",
    "\n",
    "from datetime import datetime\n",
    "df = pd.read_excel('Wine.xlsx')\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "# Format the date and time as a string\n",
    "now_str = now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "results_file = 'df' + now_str + '.csv'\n",
    "\n",
    "opts = dict(tol = 1e-6, max_iter = int(1e6)) # parameters for LogisticRegression\n",
    "parameters = {'solver':['lbfgs', 'liblinear', 'newton-cg','sag','saga'], \n",
    "              'C':[0.1, 1, 10]} # parameters for GridSearchCV\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, \\\n",
    "                            random_state=0) # 5-fold CV\n",
    "grid = GridSearchCV(estimator=LogisticRegression(**opts), \\\n",
    "                param_grid=parameters, cv=cv, \n",
    "                scoring=['accuracy','f1_macro'], refit=\"accuracy\")\n",
    "grid.fit(X_train, y_train)\n",
    "# grid.fit(X, y)\n",
    "cv_logistic = pd.DataFrame(data = grid.cv_results_)\n",
    "cv_logistic.to_csv(results_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'solver': 'lbfgs'}\n",
      "0.9655172413793103\n",
      "LogisticRegression(C=1, max_iter=1000000, tol=1e-06)\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< 結果與討論 >\n",
    "- 所列出最佳參數值為{C = 10, solver = lbfgs}\n",
    "- 參數最佳得分為 94.88%\n",
    "- 故我們需要用c=10，最大迭代次數為1000000，收斂值為1e-06\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step2 測試資料之準確率回報\n",
    "- 運用上一步找出最佳參數訓練資料，計算各項準確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using original data: 0.9722222222222222\n",
      "Accuracy using principal components: 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# 初始化羅吉斯回歸分類器並使用上一步找出的最佳參數\n",
    "logreg = LogisticRegression(C=1, solver='lbfgs',tol = 1e-6, max_iter = int(1e6), verbose=1)\n",
    "\n",
    "# 使用原始資料進行訓練及測試\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "accuracy_orig = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# 使用主成分資料進行訓練及測試\n",
    "logreg.fit(X_train_pca, y_train)\n",
    "y_pred_pca = logreg.predict(X_test_pca)\n",
    "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
    "\n",
    "print(\"Accuracy using original data:\", accuracy_orig)\n",
    "print(\"Accuracy using principal components:\", accuracy_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< 結果與討論 >\n",
    "- 上述這段程式碼將使用原始資料和使用PCA降維後的主成分資料來訓練和測試多元羅吉斯回歸分類器。\n",
    "- 它會列印出兩種情況下的準確率(原始資料的準確率為97.22%，主成分後的資料準確率為91.66%)\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分類器二: 支援向量機 (Support Vector Machine)\n",
    "支持向量機 (Support Vector Machine, SVM) 是一種用於分類和回歸分析的機器學習方法。以下是 SVM 的原理概述：\n",
    "\n",
    "1. **分類目標**：\n",
    "   - SVM 的目標是找到一個最佳的超平面，可以將不同類別的數據分開。這個超平面被稱為分類邊界。\n",
    "\n",
    "2. **超平面**：\n",
    "   - 在二維空間中，超平面是一個直線；在更高維度的空間中，它是一個超平面。對於二分類問題，超平面將數據分成兩個部分，每一部分代表一個類別。\n",
    "\n",
    "3. **支持向量**：\n",
    "   - 支持向量是離超平面最近的數據點。這些數據點對於定義超平面是至關重要的。\n",
    "\n",
    "4. **最大邊界**：\n",
    "   - SVM 的目標是找到一個分類邊界，使得兩個類別的支持向量到分類邊界的距離最大化。這種最大化邊界的方法被稱為最大間隔分類。\n",
    "\n",
    "5. **軟間隔和硬間隔**：\n",
    "   - 在實際應用中，數據通常是非線性可分的。為了允許一些錯誤的分類，SVM 引入了軟間隔，這使得分類器更加鬆散，能夠更好地泛化到未見過的數據。相反，硬間隔 SVM 要求所有數據都正確分類。\n",
    "\n",
    "6. **核函數**：\n",
    "   - 當數據不是線性可分時，SVM 使用核函數將數據映射到更高維度的空間中，從而使得數據在該空間中變得線性可分。常見的核函數包括線性核、多項式核和高斯核。\n",
    "\n",
    "7. **學習算法**：\n",
    "   - SVM 通常使用優化算法，如序列最小優化 (Sequential Minimal Optimization, SMO) 或梯度下降，來找到最優的超平面。\n",
    "\n",
    "8. **優點**：\n",
    "   - SVM 在處理高維數據上效果良好，且對於特徵數量大於樣本數的情況也表現出色。它能夠處理非線性分類問題，並且對於較小的數據集也能夠給出較好的結果。\n",
    "\n",
    "9. **缺點**：\n",
    "   - SVM 的訓練時間可能較長，特別是在大型數據集上。對於非常大的數據集，核函數的計算也可能變得非常昂貴。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step1 運用 Cross validation 的方式，找出最佳的參數\n",
    "#### 套件介紹\n",
    "1. SVC 是 Scikit-learn 中的支持向量機分類器，\n",
    "2. GridSearchCV 是用於超參數優化的網格搜索類別，\n",
    "3. StratifiedShuffleSplit 用於生成分層隨機分割的交叉驗證集，\n",
    "4. datetime 是 Python 的日期和時間處理類別，\n",
    "5. pandas 是 Python 中用於數據處理的庫。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python_venv\\ml_evnv\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\python_venv\\ml_evnv\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\python_venv\\ml_evnv\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "d:\\python_venv\\ml_evnv\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import  GridSearchCV, \\\n",
    "                        StratifiedShuffleSplit\n",
    "\n",
    "from datetime import datetime\n",
    "df = pd.read_excel('Wine.xlsx')\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "# Format the date and time as a string\n",
    "now_str = now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "results_file = 'df_svm' + now_str + '.csv'\n",
    "\n",
    "opts = dict(tol = 1e-6, max_iter = int(1e6)) # parameters for LogisticRegression\n",
    "parameters = {'kernel':['linear', 'rbf', 'poly', 'sigmoid'], \n",
    "              'C':[0.1, 1, 10]} # parameters for GridSearchCV\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, \\\n",
    "                            random_state=0) # 5-fold CV\n",
    "grid = GridSearchCV(estimator=SVC(**opts), \\\n",
    "                param_grid=parameters, cv=cv, \n",
    "                scoring=['accuracy','f1_macro'], refit=\"accuracy\")\n",
    "grid.fit(X_train, y_train)\n",
    "# grid.fit(X, y)\n",
    "cv_logistic = pd.DataFrame(data = grid.cv_results_)\n",
    "cv_logistic.to_csv(results_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1, 'kernel': 'linear'}\n",
      "0.9448275862068967\n",
      "SVC(C=0.1, kernel='linear', max_iter=1000000, tol=1e-06)\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< 結果與討論 >\n",
    "- 所列出最佳參數值為{C = 0.1, 'kernel'='linear'}\n",
    "- 參數最佳得分為 94.48%\n",
    "- 故我們需要用c=0.1，最大迭代次數為1000000，收斂值為1e-06\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step2 測試資料之準確率回報\n",
    "- 運用上一步找出最佳參數練資料，同時計算各項準確率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 程式碼解析\n",
    "這段程式碼演示了如何使用 Scikit-learn 中的支持向量機 (SVM) 來進行分類任務，並使用 `classification_report` 函數生成分類報告。讓我們一步步來解釋它：\n",
    "\n",
    "1. **導入必要的庫和類別**：\n",
    "   ```python\n",
    "   from sklearn.svm import SVC, LinearSVC\n",
    "   from sklearn.metrics import classification_report\n",
    "   ```\n",
    "   這些語句導入了必要的庫和類別。`SVC` 是 Scikit-learn 中的支持向量機分類器，`LinearSVC` 是用於線性支持向量機的類別，`classification_report` 是用於生成分類報告的函數。\n",
    "\n",
    "2. **設置模型參數**：\n",
    "   ```python\n",
    "   C = 0.1  # SVM regularization parameter\n",
    "   opts = dict(C=C, tol=1e-6, max_iter=int(1e6))\n",
    "   ```\n",
    "   這裡設置了一個正則化參數 `C`，以及一些其他的 SVC 參數，如容忍度 `tol` 和最大迭代次數 `max_iter`。這些參數將用於初始化 SVC 分類器。\n",
    "\n",
    "3. **初始化 SVC 分類器**：\n",
    "   ```python\n",
    "   clf_svm = SVC(kernel='linear', **opts)\n",
    "   ```\n",
    "   這行程式碼初始化了一個 SVC 分類器。`kernel` 參數指定了要使用的核函數，這裡使用線性核函數 ('linear')。`**opts` 將之前設置的參數傳遞給 SVC 分類器。\n",
    "\n",
    "4. **模型訓練**：\n",
    "   ```python\n",
    "   clf_svm.fit(X_train, y_train)\n",
    "   ```\n",
    "   這行程式碼使用訓練集 `X_train` 和對應的標籤 `y_train` 來訓練 SVC 分類器。\n",
    "\n",
    "5. **進行預測**：\n",
    "   ```python\n",
    "   predictions = clf_svm.predict(X_test)\n",
    "   ```\n",
    "   這行程式碼使用訓練好的 SVC 分類器對測試集 `X_test` 進行預測，並將預測結果存儲在 `predictions` 變量中。\n",
    "\n",
    "6. **生成分類報告**：\n",
    "   ```python\n",
    "   print(classification_report(y_test, predictions))\n",
    "   ```\n",
    "   這行程式碼使用 `classification_report` 函數生成一個分類報告，該報告包含了精度、召回率、F1 值等指標，用於評估模型的性能。`y_test` 是測試集的真實標籤，`predictions` 是模型的預測結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        14\n",
      "           3       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#原始資料標準化後的生成分類報告\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "C = 0.1 # SVM regularization parameter\n",
    "opts = dict(C = C, tol = 1e-6, max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel='linear', **opts)\n",
    "clf_svm.fit(X_train_scaled, y_train)\n",
    "predictions = clf_svm.predict(X_test_scaled)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      1.00      0.96        12\n",
      "           2       1.00      0.93      0.96        14\n",
      "           3       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.97        36\n",
      "   macro avg       0.97      0.98      0.97        36\n",
      "weighted avg       0.97      0.97      0.97        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 主成分資料的生成分類報告\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "C = 0.1 # SVM regularization parameter\n",
    "opts = dict(C = C, tol = 1e-6, max_iter = int(1e6))\n",
    "clf_svm = SVC(kernel='linear', **opts)\n",
    "clf_svm.fit(X_train_pca, y_train)\n",
    "predictions = clf_svm.predict(X_test_pca)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 各項指標的描述 \n",
    "這是分類報告，用於評估支持向量機 (SVM) 分類器在測試集上的性能。以下是報告中各個指標的解釋：\n",
    "\n",
    "1. **precision**（精度）：指的是在所有被分類為某類的樣本中，真正屬於該類別的樣本所占的比例。比如對類別1而言，精度為0.92，意味著有92%的真正屬於類別1的樣本被成功地分類為類別1。\n",
    "\n",
    "2. **recall**（召回率）：指的是在所有真正屬於某類的樣本中，被正確分類為該類別的樣本所占的比例。比如對類別2而言，召回率為0.93，意味著有93%的真正屬於類別2的樣本被成功地分類為類別2。\n",
    "\n",
    "3. **f1-score**（F1 值）：精度和召回率的加權調和平均值，是一個綜合考慮精度和召回率的指標。F1 值越高，模型的性能越好。\n",
    "\n",
    "4. **support**（支援度）：每個類別在測試集中的樣本數量。\n",
    "\n",
    "5. **accuracy**（準確率）：模型在測試集上的整體準確率，即所有正確預測的樣本數佔所有樣本數的比例。\n",
    "\n",
    "6. **macro avg**（宏平均）：對所有類別的精度、召回率和 F1 值進行算術平均。它對每個類別的性能給予相同的權重，不考慮類別不平衡的問題。\n",
    "\n",
    "7. **weighted avg**（加權平均）：對所有類別的精度、召回率和 F1 值進行加權平均，權重為每個類別的支援度（樣本數量）。這反映了不同類別在測試集中的重要性。\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分類器三: 神經網路 (Neural Network)\n",
    "\n",
    "神經網路（Neural Network）是一種模仿人腦結構和功能的機器學習模型，通常用於解決分類和回歸問題。以下是神經網路的原理：\n",
    "\n",
    "1. **神經元**：\n",
    "   - 神經網路由神經元組成，每個神經元接收多個輸入，通過權重加權後，將它們與偏差相加並通過激活函數進行處理，最終產生輸出。\n",
    "\n",
    "2. **權重**：\n",
    "   - 權重是神經元對輸入的影響程度，它決定了每個輸入的重要性。神經網路通過學習過程中不斷調整權重來提高模型的性能。\n",
    "\n",
    "3. **偏差**：\n",
    "   - 偏差是神經元的一個常數項，它用於調整神經元的激活閾值，從而影響神經元的輸出。\n",
    "\n",
    "4. **激活函數**：\n",
    "   - 激活函數是神經元內部的非線性轉換函數，它將加權和的結果映射到某個特定的範圍內，以產生輸出。常用的激活函數包括 Sigmoid、ReLU、tanh 等。\n",
    "\n",
    "5. **層次結構**：\n",
    "   - 神經網路由多個層次組成，通常包括輸入層、隱藏層和輸出層。輸入層接收原始數據，隱藏層進行特徵提取和表示學習，輸出層生成最終的預測結果。\n",
    "\n",
    "6. **前向傳播**：\n",
    "   - 在神經網路中，數據通過層與層之間的前向傳播來進行處理。每個層將前一層的輸出作為輸入，通過權重和激活函數進行處理，最終生成輸出。\n",
    "\n",
    "7. **反向傳播**：\n",
    "   - 反向傳播是神經網路中的學習算法，它通過計算損失函數對每個權重的梯度，然後利用梯度下降算法來更新權重，從而最小化損失函數，提高模型的性能。\n",
    "\n",
    "8. **損失函數**：\n",
    "   - 損失函數用於衡量模型預測與真實標籤之間的差距。常用的損失函數包括均方誤差（MSE）、交叉熵等。\n",
    "\n",
    "9. **學習算法**：\n",
    "   - 神經網路通常使用反向傳播算法進行訓練，但也有其他優化算法，如隨機梯度下降（SGD）、Adam、RMSProp 等。\n",
    "\n",
    "10. **過擬合和正則化**：\n",
    "    - 神經網路容易過擬合，因此常使用正則化技術來防止過擬合，例如 L1 正則化、L2 正則化等。\n",
    "\n",
    "總的來說，神經網路通過模擬人類大腦的工作方式，以階層結構和學習算法來處理複雜的非線性問題，並在訓練過程中逐步提高模型的性能。\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 程式碼解析\n",
    "這段程式碼使用了Scikit-learn中的MLPClassifier來建立一個多層感知器（MLP）分類模型，並使用該模型對測試數據進行預測，最後印出分類報告。\n",
    "\n",
    "讓我們一步步解釋這段程式碼：\n",
    "\n",
    "1. `from sklearn.neural_network import MLPClassifier`: 導入MLPClassifier類別，該類別實現了多層感知器分類器，用於構建和訓練MLP模型。\n",
    "\n",
    "2. `from sklearn.metrics import classification_report`: 導入classification_report函數，該函數用於生成分類模型的分類報告，其中包含準確率、召回率、F1分數等指標。\n",
    "\n",
    "3. `hidden_layers = (30,)` 和 `activation = 'logistic'`: 這裡指定了MLP模型的一些參數，包括隱藏層的大小和激活函數。在這個例子中，隱藏層只有30個單元，激活函數為logistic。\n",
    "\n",
    "4. `opts = dict(hidden_layer_sizes=hidden_layers, verbose=True, activation=activation, tol=1e-6, max_iter=int(1e6))`: 定義了一個字典opts，其中包含了MLPClassifier的參數設置。這些參數包括隱藏層的大小、是否打印訓練信息、激活函數、容忍誤差以及最大迭代次數等。\n",
    "\n",
    "5. `solver = 'adam'`: 指定了MLP模型的優化器為adam。adam是一種常用的優化算法，通常在深度學習中表現良好。\n",
    "\n",
    "6. `clf_MLP = MLPClassifier(solver=solver, **opts)`: 通過MLPClassifier類別創建了一個MLP分類器，並傳入了solver參數以及opts字典中的其他參數。\n",
    "\n",
    "7. `clf_MLP.fit(X_train_scaled, y_train)`: 使用訓練數據X_train_scaled和標籤y_train來訓練MLP模型。\n",
    "\n",
    "8. `predictions = clf_MLP.predict(X_test_scaled)`: 使用訓練好的模型對測試數據X_test_scaled進行預測，並將預測結果保存在predictions變量中。\n",
    "\n",
    "9. `print(classification_report(y_test, predictions))`: 印出分類報告，其中包含了模型在測試數據上的準確率、召回率、F1分數等評估指標。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.13956239\n",
      "Iteration 2, loss = 1.13129288\n",
      "Iteration 3, loss = 1.12324128\n",
      "Iteration 4, loss = 1.11541031\n",
      "Iteration 5, loss = 1.10779969\n",
      "Iteration 6, loss = 1.10040826\n",
      "Iteration 7, loss = 1.09323518\n",
      "Iteration 8, loss = 1.08627857\n",
      "Iteration 9, loss = 1.07953454\n",
      "Iteration 10, loss = 1.07299714\n",
      "Iteration 11, loss = 1.06665875\n",
      "Iteration 12, loss = 1.06051072\n",
      "Iteration 13, loss = 1.05454364\n",
      "Iteration 14, loss = 1.04874743\n",
      "Iteration 15, loss = 1.04311122\n",
      "Iteration 16, loss = 1.03762332\n",
      "Iteration 17, loss = 1.03227131\n",
      "Iteration 18, loss = 1.02704224\n",
      "Iteration 19, loss = 1.02192284\n",
      "Iteration 20, loss = 1.01689983\n",
      "Iteration 21, loss = 1.01196017\n",
      "Iteration 22, loss = 1.00709132\n",
      "Iteration 23, loss = 1.00228144\n",
      "Iteration 24, loss = 0.99751954\n",
      "Iteration 25, loss = 0.99279559\n",
      "Iteration 26, loss = 0.98810064\n",
      "Iteration 27, loss = 0.98342675\n",
      "Iteration 28, loss = 0.97876705\n",
      "Iteration 29, loss = 0.97411566\n",
      "Iteration 30, loss = 0.96946763\n",
      "Iteration 31, loss = 0.96481886\n",
      "Iteration 32, loss = 0.96016607\n",
      "Iteration 33, loss = 0.95550667\n",
      "Iteration 34, loss = 0.95083872\n",
      "Iteration 35, loss = 0.94616086\n",
      "Iteration 36, loss = 0.94147219\n",
      "Iteration 37, loss = 0.93677224\n",
      "Iteration 38, loss = 0.93206087\n",
      "Iteration 39, loss = 0.92733818\n",
      "Iteration 40, loss = 0.92260447\n",
      "Iteration 41, loss = 0.91786017\n",
      "Iteration 42, loss = 0.91310580\n",
      "Iteration 43, loss = 0.90834189\n",
      "Iteration 44, loss = 0.90356903\n",
      "Iteration 45, loss = 0.89878775\n",
      "Iteration 46, loss = 0.89399858\n",
      "Iteration 47, loss = 0.88920198\n",
      "Iteration 48, loss = 0.88439838\n",
      "Iteration 49, loss = 0.87958816\n",
      "Iteration 50, loss = 0.87477165\n",
      "Iteration 51, loss = 0.86994914\n",
      "Iteration 52, loss = 0.86512087\n",
      "Iteration 53, loss = 0.86028706\n",
      "Iteration 54, loss = 0.85544792\n",
      "Iteration 55, loss = 0.85060364\n",
      "Iteration 56, loss = 0.84575442\n",
      "Iteration 57, loss = 0.84090046\n",
      "Iteration 58, loss = 0.83604198\n",
      "Iteration 59, loss = 0.83117922\n",
      "Iteration 60, loss = 0.82631246\n",
      "Iteration 61, loss = 0.82144200\n",
      "Iteration 62, loss = 0.81656819\n",
      "Iteration 63, loss = 0.81169139\n",
      "Iteration 64, loss = 0.80681204\n",
      "Iteration 65, loss = 0.80193060\n",
      "Iteration 66, loss = 0.79704754\n",
      "Iteration 67, loss = 0.79216341\n",
      "Iteration 68, loss = 0.78727876\n",
      "Iteration 69, loss = 0.78239418\n",
      "Iteration 70, loss = 0.77751029\n",
      "Iteration 71, loss = 0.77262772\n",
      "Iteration 72, loss = 0.76774713\n",
      "Iteration 73, loss = 0.76286920\n",
      "Iteration 74, loss = 0.75799459\n",
      "Iteration 75, loss = 0.75312401\n",
      "Iteration 76, loss = 0.74825816\n",
      "Iteration 77, loss = 0.74339773\n",
      "Iteration 78, loss = 0.73854343\n",
      "Iteration 79, loss = 0.73369595\n",
      "Iteration 80, loss = 0.72885601\n",
      "Iteration 81, loss = 0.72402429\n",
      "Iteration 82, loss = 0.71920149\n",
      "Iteration 83, loss = 0.71438828\n",
      "Iteration 84, loss = 0.70958535\n",
      "Iteration 85, loss = 0.70479336\n",
      "Iteration 86, loss = 0.70001297\n",
      "Iteration 87, loss = 0.69524482\n",
      "Iteration 88, loss = 0.69048956\n",
      "Iteration 89, loss = 0.68574782\n",
      "Iteration 90, loss = 0.68102020\n",
      "Iteration 91, loss = 0.67630731\n",
      "Iteration 92, loss = 0.67160975\n",
      "Iteration 93, loss = 0.66692809\n",
      "Iteration 94, loss = 0.66226290\n",
      "Iteration 95, loss = 0.65761473\n",
      "Iteration 96, loss = 0.65298413\n",
      "Iteration 97, loss = 0.64837162\n",
      "Iteration 98, loss = 0.64377770\n",
      "Iteration 99, loss = 0.63920289\n",
      "Iteration 100, loss = 0.63464766\n",
      "Iteration 101, loss = 0.63011249\n",
      "Iteration 102, loss = 0.62559782\n",
      "Iteration 103, loss = 0.62110410\n",
      "Iteration 104, loss = 0.61663175\n",
      "Iteration 105, loss = 0.61218118\n",
      "Iteration 106, loss = 0.60775280\n",
      "Iteration 107, loss = 0.60334698\n",
      "Iteration 108, loss = 0.59896408\n",
      "Iteration 109, loss = 0.59460446\n",
      "Iteration 110, loss = 0.59026846\n",
      "Iteration 111, loss = 0.58595640\n",
      "Iteration 112, loss = 0.58166858\n",
      "Iteration 113, loss = 0.57740529\n",
      "Iteration 114, loss = 0.57316683\n",
      "Iteration 115, loss = 0.56895344\n",
      "Iteration 116, loss = 0.56476538\n",
      "Iteration 117, loss = 0.56060288\n",
      "Iteration 118, loss = 0.55646618\n",
      "Iteration 119, loss = 0.55235546\n",
      "Iteration 120, loss = 0.54827094\n",
      "Iteration 121, loss = 0.54421278\n",
      "Iteration 122, loss = 0.54018116\n",
      "Iteration 123, loss = 0.53617623\n",
      "Iteration 124, loss = 0.53219813\n",
      "Iteration 125, loss = 0.52824700\n",
      "Iteration 126, loss = 0.52432295\n",
      "Iteration 127, loss = 0.52042608\n",
      "Iteration 128, loss = 0.51655648\n",
      "Iteration 129, loss = 0.51271425\n",
      "Iteration 130, loss = 0.50889945\n",
      "Iteration 131, loss = 0.50511213\n",
      "Iteration 132, loss = 0.50135235\n",
      "Iteration 133, loss = 0.49762015\n",
      "Iteration 134, loss = 0.49391555\n",
      "Iteration 135, loss = 0.49023857\n",
      "Iteration 136, loss = 0.48658922\n",
      "Iteration 137, loss = 0.48296750\n",
      "Iteration 138, loss = 0.47937340\n",
      "Iteration 139, loss = 0.47580690\n",
      "Iteration 140, loss = 0.47226797\n",
      "Iteration 141, loss = 0.46875658\n",
      "Iteration 142, loss = 0.46527269\n",
      "Iteration 143, loss = 0.46181624\n",
      "Iteration 144, loss = 0.45838718\n",
      "Iteration 145, loss = 0.45498544\n",
      "Iteration 146, loss = 0.45161094\n",
      "Iteration 147, loss = 0.44826362\n",
      "Iteration 148, loss = 0.44494338\n",
      "Iteration 149, loss = 0.44165013\n",
      "Iteration 150, loss = 0.43838378\n",
      "Iteration 151, loss = 0.43514423\n",
      "Iteration 152, loss = 0.43193136\n",
      "Iteration 153, loss = 0.42874507\n",
      "Iteration 154, loss = 0.42558523\n",
      "Iteration 155, loss = 0.42245172\n",
      "Iteration 156, loss = 0.41934443\n",
      "Iteration 157, loss = 0.41626321\n",
      "Iteration 158, loss = 0.41320793\n",
      "Iteration 159, loss = 0.41017846\n",
      "Iteration 160, loss = 0.40717464\n",
      "Iteration 161, loss = 0.40419635\n",
      "Iteration 162, loss = 0.40124342\n",
      "Iteration 163, loss = 0.39831571\n",
      "Iteration 164, loss = 0.39541306\n",
      "Iteration 165, loss = 0.39253531\n",
      "Iteration 166, loss = 0.38968232\n",
      "Iteration 167, loss = 0.38685391\n",
      "Iteration 168, loss = 0.38404992\n",
      "Iteration 169, loss = 0.38127019\n",
      "Iteration 170, loss = 0.37851456\n",
      "Iteration 171, loss = 0.37578284\n",
      "Iteration 172, loss = 0.37307488\n",
      "Iteration 173, loss = 0.37039050\n",
      "Iteration 174, loss = 0.36772952\n",
      "Iteration 175, loss = 0.36509179\n",
      "Iteration 176, loss = 0.36247712\n",
      "Iteration 177, loss = 0.35988533\n",
      "Iteration 178, loss = 0.35731626\n",
      "Iteration 179, loss = 0.35476973\n",
      "Iteration 180, loss = 0.35224555\n",
      "Iteration 181, loss = 0.34974356\n",
      "Iteration 182, loss = 0.34726357\n",
      "Iteration 183, loss = 0.34480542\n",
      "Iteration 184, loss = 0.34236892\n",
      "Iteration 185, loss = 0.33995389\n",
      "Iteration 186, loss = 0.33756017\n",
      "Iteration 187, loss = 0.33518756\n",
      "Iteration 188, loss = 0.33283591\n",
      "Iteration 189, loss = 0.33050503\n",
      "Iteration 190, loss = 0.32819474\n",
      "Iteration 191, loss = 0.32590488\n",
      "Iteration 192, loss = 0.32363526\n",
      "Iteration 193, loss = 0.32138572\n",
      "Iteration 194, loss = 0.31915608\n",
      "Iteration 195, loss = 0.31694618\n",
      "Iteration 196, loss = 0.31475583\n",
      "Iteration 197, loss = 0.31258486\n",
      "Iteration 198, loss = 0.31043312\n",
      "Iteration 199, loss = 0.30830043\n",
      "Iteration 200, loss = 0.30618662\n",
      "Iteration 201, loss = 0.30409153\n",
      "Iteration 202, loss = 0.30201499\n",
      "Iteration 203, loss = 0.29995683\n",
      "Iteration 204, loss = 0.29791690\n",
      "Iteration 205, loss = 0.29589502\n",
      "Iteration 206, loss = 0.29389104\n",
      "Iteration 207, loss = 0.29190480\n",
      "Iteration 208, loss = 0.28993614\n",
      "Iteration 209, loss = 0.28798490\n",
      "Iteration 210, loss = 0.28605092\n",
      "Iteration 211, loss = 0.28413405\n",
      "Iteration 212, loss = 0.28223413\n",
      "Iteration 213, loss = 0.28035100\n",
      "Iteration 214, loss = 0.27848453\n",
      "Iteration 215, loss = 0.27663455\n",
      "Iteration 216, loss = 0.27480091\n",
      "Iteration 217, loss = 0.27298347\n",
      "Iteration 218, loss = 0.27118207\n",
      "Iteration 219, loss = 0.26939658\n",
      "Iteration 220, loss = 0.26762684\n",
      "Iteration 221, loss = 0.26587271\n",
      "Iteration 222, loss = 0.26413405\n",
      "Iteration 223, loss = 0.26241072\n",
      "Iteration 224, loss = 0.26070257\n",
      "Iteration 225, loss = 0.25900947\n",
      "Iteration 226, loss = 0.25733127\n",
      "Iteration 227, loss = 0.25566784\n",
      "Iteration 228, loss = 0.25401904\n",
      "Iteration 229, loss = 0.25238475\n",
      "Iteration 230, loss = 0.25076481\n",
      "Iteration 231, loss = 0.24915911\n",
      "Iteration 232, loss = 0.24756750\n",
      "Iteration 233, loss = 0.24598986\n",
      "Iteration 234, loss = 0.24442606\n",
      "Iteration 235, loss = 0.24287597\n",
      "Iteration 236, loss = 0.24133946\n",
      "Iteration 237, loss = 0.23981641\n",
      "Iteration 238, loss = 0.23830668\n",
      "Iteration 239, loss = 0.23681017\n",
      "Iteration 240, loss = 0.23532673\n",
      "Iteration 241, loss = 0.23385625\n",
      "Iteration 242, loss = 0.23239862\n",
      "Iteration 243, loss = 0.23095370\n",
      "Iteration 244, loss = 0.22952138\n",
      "Iteration 245, loss = 0.22810154\n",
      "Iteration 246, loss = 0.22669406\n",
      "Iteration 247, loss = 0.22529883\n",
      "Iteration 248, loss = 0.22391574\n",
      "Iteration 249, loss = 0.22254466\n",
      "Iteration 250, loss = 0.22118549\n",
      "Iteration 251, loss = 0.21983810\n",
      "Iteration 252, loss = 0.21850240\n",
      "Iteration 253, loss = 0.21717827\n",
      "Iteration 254, loss = 0.21586560\n",
      "Iteration 255, loss = 0.21456428\n",
      "Iteration 256, loss = 0.21327421\n",
      "Iteration 257, loss = 0.21199527\n",
      "Iteration 258, loss = 0.21072736\n",
      "Iteration 259, loss = 0.20947039\n",
      "Iteration 260, loss = 0.20822423\n",
      "Iteration 261, loss = 0.20698879\n",
      "Iteration 262, loss = 0.20576397\n",
      "Iteration 263, loss = 0.20454966\n",
      "Iteration 264, loss = 0.20334577\n",
      "Iteration 265, loss = 0.20215219\n",
      "Iteration 266, loss = 0.20096883\n",
      "Iteration 267, loss = 0.19979559\n",
      "Iteration 268, loss = 0.19863237\n",
      "Iteration 269, loss = 0.19747907\n",
      "Iteration 270, loss = 0.19633560\n",
      "Iteration 271, loss = 0.19520186\n",
      "Iteration 272, loss = 0.19407777\n",
      "Iteration 273, loss = 0.19296322\n",
      "Iteration 274, loss = 0.19185813\n",
      "Iteration 275, loss = 0.19076240\n",
      "Iteration 276, loss = 0.18967594\n",
      "Iteration 277, loss = 0.18859867\n",
      "Iteration 278, loss = 0.18753048\n",
      "Iteration 279, loss = 0.18647131\n",
      "Iteration 280, loss = 0.18542105\n",
      "Iteration 281, loss = 0.18437963\n",
      "Iteration 282, loss = 0.18334695\n",
      "Iteration 283, loss = 0.18232293\n",
      "Iteration 284, loss = 0.18130749\n",
      "Iteration 285, loss = 0.18030054\n",
      "Iteration 286, loss = 0.17930200\n",
      "Iteration 287, loss = 0.17831178\n",
      "Iteration 288, loss = 0.17732982\n",
      "Iteration 289, loss = 0.17635601\n",
      "Iteration 290, loss = 0.17539030\n",
      "Iteration 291, loss = 0.17443259\n",
      "Iteration 292, loss = 0.17348280\n",
      "Iteration 293, loss = 0.17254087\n",
      "Iteration 294, loss = 0.17160671\n",
      "Iteration 295, loss = 0.17068025\n",
      "Iteration 296, loss = 0.16976141\n",
      "Iteration 297, loss = 0.16885012\n",
      "Iteration 298, loss = 0.16794630\n",
      "Iteration 299, loss = 0.16704988\n",
      "Iteration 300, loss = 0.16616078\n",
      "Iteration 301, loss = 0.16527894\n",
      "Iteration 302, loss = 0.16440428\n",
      "Iteration 303, loss = 0.16353674\n",
      "Iteration 304, loss = 0.16267623\n",
      "Iteration 305, loss = 0.16182270\n",
      "Iteration 306, loss = 0.16097608\n",
      "Iteration 307, loss = 0.16013629\n",
      "Iteration 308, loss = 0.15930327\n",
      "Iteration 309, loss = 0.15847695\n",
      "Iteration 310, loss = 0.15765727\n",
      "Iteration 311, loss = 0.15684416\n",
      "Iteration 312, loss = 0.15603755\n",
      "Iteration 313, loss = 0.15523739\n",
      "Iteration 314, loss = 0.15444361\n",
      "Iteration 315, loss = 0.15365615\n",
      "Iteration 316, loss = 0.15287493\n",
      "Iteration 317, loss = 0.15209991\n",
      "Iteration 318, loss = 0.15133103\n",
      "Iteration 319, loss = 0.15056821\n",
      "Iteration 320, loss = 0.14981140\n",
      "Iteration 321, loss = 0.14906055\n",
      "Iteration 322, loss = 0.14831559\n",
      "Iteration 323, loss = 0.14757647\n",
      "Iteration 324, loss = 0.14684313\n",
      "Iteration 325, loss = 0.14611550\n",
      "Iteration 326, loss = 0.14539355\n",
      "Iteration 327, loss = 0.14467720\n",
      "Iteration 328, loss = 0.14396641\n",
      "Iteration 329, loss = 0.14326112\n",
      "Iteration 330, loss = 0.14256128\n",
      "Iteration 331, loss = 0.14186683\n",
      "Iteration 332, loss = 0.14117772\n",
      "Iteration 333, loss = 0.14049390\n",
      "Iteration 334, loss = 0.13981532\n",
      "Iteration 335, loss = 0.13914192\n",
      "Iteration 336, loss = 0.13847366\n",
      "Iteration 337, loss = 0.13781048\n",
      "Iteration 338, loss = 0.13715234\n",
      "Iteration 339, loss = 0.13649919\n",
      "Iteration 340, loss = 0.13585098\n",
      "Iteration 341, loss = 0.13520765\n",
      "Iteration 342, loss = 0.13456917\n",
      "Iteration 343, loss = 0.13393548\n",
      "Iteration 344, loss = 0.13330654\n",
      "Iteration 345, loss = 0.13268231\n",
      "Iteration 346, loss = 0.13206273\n",
      "Iteration 347, loss = 0.13144777\n",
      "Iteration 348, loss = 0.13083738\n",
      "Iteration 349, loss = 0.13023150\n",
      "Iteration 350, loss = 0.12963011\n",
      "Iteration 351, loss = 0.12903316\n",
      "Iteration 352, loss = 0.12844060\n",
      "Iteration 353, loss = 0.12785240\n",
      "Iteration 354, loss = 0.12726850\n",
      "Iteration 355, loss = 0.12668888\n",
      "Iteration 356, loss = 0.12611348\n",
      "Iteration 357, loss = 0.12554228\n",
      "Iteration 358, loss = 0.12497522\n",
      "Iteration 359, loss = 0.12441226\n",
      "Iteration 360, loss = 0.12385338\n",
      "Iteration 361, loss = 0.12329853\n",
      "Iteration 362, loss = 0.12274768\n",
      "Iteration 363, loss = 0.12220077\n",
      "Iteration 364, loss = 0.12165779\n",
      "Iteration 365, loss = 0.12111868\n",
      "Iteration 366, loss = 0.12058342\n",
      "Iteration 367, loss = 0.12005196\n",
      "Iteration 368, loss = 0.11952428\n",
      "Iteration 369, loss = 0.11900033\n",
      "Iteration 370, loss = 0.11848008\n",
      "Iteration 371, loss = 0.11796350\n",
      "Iteration 372, loss = 0.11745055\n",
      "Iteration 373, loss = 0.11694120\n",
      "Iteration 374, loss = 0.11643541\n",
      "Iteration 375, loss = 0.11593315\n",
      "Iteration 376, loss = 0.11543439\n",
      "Iteration 377, loss = 0.11493910\n",
      "Iteration 378, loss = 0.11444724\n",
      "Iteration 379, loss = 0.11395877\n",
      "Iteration 380, loss = 0.11347368\n",
      "Iteration 381, loss = 0.11299193\n",
      "Iteration 382, loss = 0.11251349\n",
      "Iteration 383, loss = 0.11203832\n",
      "Iteration 384, loss = 0.11156640\n",
      "Iteration 385, loss = 0.11109769\n",
      "Iteration 386, loss = 0.11063217\n",
      "Iteration 387, loss = 0.11016982\n",
      "Iteration 388, loss = 0.10971059\n",
      "Iteration 389, loss = 0.10925446\n",
      "Iteration 390, loss = 0.10880140\n",
      "Iteration 391, loss = 0.10835139\n",
      "Iteration 392, loss = 0.10790440\n",
      "Iteration 393, loss = 0.10746039\n",
      "Iteration 394, loss = 0.10701935\n",
      "Iteration 395, loss = 0.10658125\n",
      "Iteration 396, loss = 0.10614605\n",
      "Iteration 397, loss = 0.10571373\n",
      "Iteration 398, loss = 0.10528428\n",
      "Iteration 399, loss = 0.10485765\n",
      "Iteration 400, loss = 0.10443383\n",
      "Iteration 401, loss = 0.10401279\n",
      "Iteration 402, loss = 0.10359451\n",
      "Iteration 403, loss = 0.10317895\n",
      "Iteration 404, loss = 0.10276611\n",
      "Iteration 405, loss = 0.10235595\n",
      "Iteration 406, loss = 0.10194844\n",
      "Iteration 407, loss = 0.10154357\n",
      "Iteration 408, loss = 0.10114132\n",
      "Iteration 409, loss = 0.10074165\n",
      "Iteration 410, loss = 0.10034455\n",
      "Iteration 411, loss = 0.09995000\n",
      "Iteration 412, loss = 0.09955796\n",
      "Iteration 413, loss = 0.09916843\n",
      "Iteration 414, loss = 0.09878137\n",
      "Iteration 415, loss = 0.09839677\n",
      "Iteration 416, loss = 0.09801460\n",
      "Iteration 417, loss = 0.09763484\n",
      "Iteration 418, loss = 0.09725748\n",
      "Iteration 419, loss = 0.09688249\n",
      "Iteration 420, loss = 0.09650985\n",
      "Iteration 421, loss = 0.09613954\n",
      "Iteration 422, loss = 0.09577154\n",
      "Iteration 423, loss = 0.09540583\n",
      "Iteration 424, loss = 0.09504240\n",
      "Iteration 425, loss = 0.09468121\n",
      "Iteration 426, loss = 0.09432226\n",
      "Iteration 427, loss = 0.09396552\n",
      "Iteration 428, loss = 0.09361097\n",
      "Iteration 429, loss = 0.09325860\n",
      "Iteration 430, loss = 0.09290839\n",
      "Iteration 431, loss = 0.09256032\n",
      "Iteration 432, loss = 0.09221436\n",
      "Iteration 433, loss = 0.09187051\n",
      "Iteration 434, loss = 0.09152875\n",
      "Iteration 435, loss = 0.09118905\n",
      "Iteration 436, loss = 0.09085140\n",
      "Iteration 437, loss = 0.09051579\n",
      "Iteration 438, loss = 0.09018219\n",
      "Iteration 439, loss = 0.08985059\n",
      "Iteration 440, loss = 0.08952098\n",
      "Iteration 441, loss = 0.08919332\n",
      "Iteration 442, loss = 0.08886762\n",
      "Iteration 443, loss = 0.08854386\n",
      "Iteration 444, loss = 0.08822201\n",
      "Iteration 445, loss = 0.08790206\n",
      "Iteration 446, loss = 0.08758400\n",
      "Iteration 447, loss = 0.08726780\n",
      "Iteration 448, loss = 0.08695347\n",
      "Iteration 449, loss = 0.08664097\n",
      "Iteration 450, loss = 0.08633030\n",
      "Iteration 451, loss = 0.08602144\n",
      "Iteration 452, loss = 0.08571437\n",
      "Iteration 453, loss = 0.08540909\n",
      "Iteration 454, loss = 0.08510557\n",
      "Iteration 455, loss = 0.08480380\n",
      "Iteration 456, loss = 0.08450377\n",
      "Iteration 457, loss = 0.08420547\n",
      "Iteration 458, loss = 0.08390887\n",
      "Iteration 459, loss = 0.08361397\n",
      "Iteration 460, loss = 0.08332075\n",
      "Iteration 461, loss = 0.08302921\n",
      "Iteration 462, loss = 0.08273931\n",
      "Iteration 463, loss = 0.08245107\n",
      "Iteration 464, loss = 0.08216445\n",
      "Iteration 465, loss = 0.08187944\n",
      "Iteration 466, loss = 0.08159604\n",
      "Iteration 467, loss = 0.08131424\n",
      "Iteration 468, loss = 0.08103401\n",
      "Iteration 469, loss = 0.08075534\n",
      "Iteration 470, loss = 0.08047823\n",
      "Iteration 471, loss = 0.08020267\n",
      "Iteration 472, loss = 0.07992863\n",
      "Iteration 473, loss = 0.07965611\n",
      "Iteration 474, loss = 0.07938509\n",
      "Iteration 475, loss = 0.07911557\n",
      "Iteration 476, loss = 0.07884753\n",
      "Iteration 477, loss = 0.07858097\n",
      "Iteration 478, loss = 0.07831586\n",
      "Iteration 479, loss = 0.07805220\n",
      "Iteration 480, loss = 0.07778998\n",
      "Iteration 481, loss = 0.07752918\n",
      "Iteration 482, loss = 0.07726980\n",
      "Iteration 483, loss = 0.07701182\n",
      "Iteration 484, loss = 0.07675524\n",
      "Iteration 485, loss = 0.07650004\n",
      "Iteration 486, loss = 0.07624621\n",
      "Iteration 487, loss = 0.07599375\n",
      "Iteration 488, loss = 0.07574263\n",
      "Iteration 489, loss = 0.07549286\n",
      "Iteration 490, loss = 0.07524442\n",
      "Iteration 491, loss = 0.07499730\n",
      "Iteration 492, loss = 0.07475149\n",
      "Iteration 493, loss = 0.07450698\n",
      "Iteration 494, loss = 0.07426377\n",
      "Iteration 495, loss = 0.07402184\n",
      "Iteration 496, loss = 0.07378118\n",
      "Iteration 497, loss = 0.07354178\n",
      "Iteration 498, loss = 0.07330364\n",
      "Iteration 499, loss = 0.07306674\n",
      "Iteration 500, loss = 0.07283107\n",
      "Iteration 501, loss = 0.07259663\n",
      "Iteration 502, loss = 0.07236341\n",
      "Iteration 503, loss = 0.07213140\n",
      "Iteration 504, loss = 0.07190059\n",
      "Iteration 505, loss = 0.07167096\n",
      "Iteration 506, loss = 0.07144252\n",
      "Iteration 507, loss = 0.07121525\n",
      "Iteration 508, loss = 0.07098915\n",
      "Iteration 509, loss = 0.07076420\n",
      "Iteration 510, loss = 0.07054040\n",
      "Iteration 511, loss = 0.07031774\n",
      "Iteration 512, loss = 0.07009621\n",
      "Iteration 513, loss = 0.06987580\n",
      "Iteration 514, loss = 0.06965651\n",
      "Iteration 515, loss = 0.06943833\n",
      "Iteration 516, loss = 0.06922124\n",
      "Iteration 517, loss = 0.06900524\n",
      "Iteration 518, loss = 0.06879033\n",
      "Iteration 519, loss = 0.06857650\n",
      "Iteration 520, loss = 0.06836373\n",
      "Iteration 521, loss = 0.06815202\n",
      "Iteration 522, loss = 0.06794136\n",
      "Iteration 523, loss = 0.06773175\n",
      "Iteration 524, loss = 0.06752318\n",
      "Iteration 525, loss = 0.06731564\n",
      "Iteration 526, loss = 0.06710912\n",
      "Iteration 527, loss = 0.06690362\n",
      "Iteration 528, loss = 0.06669913\n",
      "Iteration 529, loss = 0.06649564\n",
      "Iteration 530, loss = 0.06629314\n",
      "Iteration 531, loss = 0.06609163\n",
      "Iteration 532, loss = 0.06589111\n",
      "Iteration 533, loss = 0.06569156\n",
      "Iteration 534, loss = 0.06549297\n",
      "Iteration 535, loss = 0.06529535\n",
      "Iteration 536, loss = 0.06509868\n",
      "Iteration 537, loss = 0.06490296\n",
      "Iteration 538, loss = 0.06470818\n",
      "Iteration 539, loss = 0.06451434\n",
      "Iteration 540, loss = 0.06432143\n",
      "Iteration 541, loss = 0.06412943\n",
      "Iteration 542, loss = 0.06393836\n",
      "Iteration 543, loss = 0.06374819\n",
      "Iteration 544, loss = 0.06355892\n",
      "Iteration 545, loss = 0.06337056\n",
      "Iteration 546, loss = 0.06318308\n",
      "Iteration 547, loss = 0.06299649\n",
      "Iteration 548, loss = 0.06281078\n",
      "Iteration 549, loss = 0.06262595\n",
      "Iteration 550, loss = 0.06244198\n",
      "Iteration 551, loss = 0.06225887\n",
      "Iteration 552, loss = 0.06207662\n",
      "Iteration 553, loss = 0.06189522\n",
      "Iteration 554, loss = 0.06171466\n",
      "Iteration 555, loss = 0.06153495\n",
      "Iteration 556, loss = 0.06135606\n",
      "Iteration 557, loss = 0.06117801\n",
      "Iteration 558, loss = 0.06100077\n",
      "Iteration 559, loss = 0.06082436\n",
      "Iteration 560, loss = 0.06064875\n",
      "Iteration 561, loss = 0.06047396\n",
      "Iteration 562, loss = 0.06029996\n",
      "Iteration 563, loss = 0.06012676\n",
      "Iteration 564, loss = 0.05995435\n",
      "Iteration 565, loss = 0.05978273\n",
      "Iteration 566, loss = 0.05961188\n",
      "Iteration 567, loss = 0.05944182\n",
      "Iteration 568, loss = 0.05927252\n",
      "Iteration 569, loss = 0.05910399\n",
      "Iteration 570, loss = 0.05893621\n",
      "Iteration 571, loss = 0.05876920\n",
      "Iteration 572, loss = 0.05860293\n",
      "Iteration 573, loss = 0.05843741\n",
      "Iteration 574, loss = 0.05827264\n",
      "Iteration 575, loss = 0.05810859\n",
      "Iteration 576, loss = 0.05794528\n",
      "Iteration 577, loss = 0.05778270\n",
      "Iteration 578, loss = 0.05762084\n",
      "Iteration 579, loss = 0.05745970\n",
      "Iteration 580, loss = 0.05729927\n",
      "Iteration 581, loss = 0.05713955\n",
      "Iteration 582, loss = 0.05698053\n",
      "Iteration 583, loss = 0.05682222\n",
      "Iteration 584, loss = 0.05666459\n",
      "Iteration 585, loss = 0.05650767\n",
      "Iteration 586, loss = 0.05635142\n",
      "Iteration 587, loss = 0.05619586\n",
      "Iteration 588, loss = 0.05604098\n",
      "Iteration 589, loss = 0.05588677\n",
      "Iteration 590, loss = 0.05573324\n",
      "Iteration 591, loss = 0.05558036\n",
      "Iteration 592, loss = 0.05542815\n",
      "Iteration 593, loss = 0.05527660\n",
      "Iteration 594, loss = 0.05512570\n",
      "Iteration 595, loss = 0.05497545\n",
      "Iteration 596, loss = 0.05482585\n",
      "Iteration 597, loss = 0.05467689\n",
      "Iteration 598, loss = 0.05452856\n",
      "Iteration 599, loss = 0.05438087\n",
      "Iteration 600, loss = 0.05423381\n",
      "Iteration 601, loss = 0.05408737\n",
      "Iteration 602, loss = 0.05394156\n",
      "Iteration 603, loss = 0.05379637\n",
      "Iteration 604, loss = 0.05365179\n",
      "Iteration 605, loss = 0.05350782\n",
      "Iteration 606, loss = 0.05336446\n",
      "Iteration 607, loss = 0.05322170\n",
      "Iteration 608, loss = 0.05307954\n",
      "Iteration 609, loss = 0.05293798\n",
      "Iteration 610, loss = 0.05279701\n",
      "Iteration 611, loss = 0.05265663\n",
      "Iteration 612, loss = 0.05251684\n",
      "Iteration 613, loss = 0.05237763\n",
      "Iteration 614, loss = 0.05223900\n",
      "Iteration 615, loss = 0.05210094\n",
      "Iteration 616, loss = 0.05196346\n",
      "Iteration 617, loss = 0.05182654\n",
      "Iteration 618, loss = 0.05169019\n",
      "Iteration 619, loss = 0.05155440\n",
      "Iteration 620, loss = 0.05141917\n",
      "Iteration 621, loss = 0.05128450\n",
      "Iteration 622, loss = 0.05115038\n",
      "Iteration 623, loss = 0.05101680\n",
      "Iteration 624, loss = 0.05088378\n",
      "Iteration 625, loss = 0.05075129\n",
      "Iteration 626, loss = 0.05061935\n",
      "Iteration 627, loss = 0.05048794\n",
      "Iteration 628, loss = 0.05035706\n",
      "Iteration 629, loss = 0.05022672\n",
      "Iteration 630, loss = 0.05009690\n",
      "Iteration 631, loss = 0.04996761\n",
      "Iteration 632, loss = 0.04983883\n",
      "Iteration 633, loss = 0.04971058\n",
      "Iteration 634, loss = 0.04958284\n",
      "Iteration 635, loss = 0.04945561\n",
      "Iteration 636, loss = 0.04932889\n",
      "Iteration 637, loss = 0.04920268\n",
      "Iteration 638, loss = 0.04907697\n",
      "Iteration 639, loss = 0.04895176\n",
      "Iteration 640, loss = 0.04882705\n",
      "Iteration 641, loss = 0.04870283\n",
      "Iteration 642, loss = 0.04857911\n",
      "Iteration 643, loss = 0.04845587\n",
      "Iteration 644, loss = 0.04833313\n",
      "Iteration 645, loss = 0.04821086\n",
      "Iteration 646, loss = 0.04808908\n",
      "Iteration 647, loss = 0.04796777\n",
      "Iteration 648, loss = 0.04784695\n",
      "Iteration 649, loss = 0.04772659\n",
      "Iteration 650, loss = 0.04760670\n",
      "Iteration 651, loss = 0.04748729\n",
      "Iteration 652, loss = 0.04736834\n",
      "Iteration 653, loss = 0.04724985\n",
      "Iteration 654, loss = 0.04713182\n",
      "Iteration 655, loss = 0.04701425\n",
      "Iteration 656, loss = 0.04689713\n",
      "Iteration 657, loss = 0.04678047\n",
      "Iteration 658, loss = 0.04666425\n",
      "Iteration 659, loss = 0.04654849\n",
      "Iteration 660, loss = 0.04643316\n",
      "Iteration 661, loss = 0.04631829\n",
      "Iteration 662, loss = 0.04620385\n",
      "Iteration 663, loss = 0.04608985\n",
      "Iteration 664, loss = 0.04597628\n",
      "Iteration 665, loss = 0.04586315\n",
      "Iteration 666, loss = 0.04575045\n",
      "Iteration 667, loss = 0.04563818\n",
      "Iteration 668, loss = 0.04552633\n",
      "Iteration 669, loss = 0.04541491\n",
      "Iteration 670, loss = 0.04530391\n",
      "Iteration 671, loss = 0.04519333\n",
      "Iteration 672, loss = 0.04508317\n",
      "Iteration 673, loss = 0.04497342\n",
      "Iteration 674, loss = 0.04486408\n",
      "Iteration 675, loss = 0.04475516\n",
      "Iteration 676, loss = 0.04464664\n",
      "Iteration 677, loss = 0.04453853\n",
      "Iteration 678, loss = 0.04443082\n",
      "Iteration 679, loss = 0.04432351\n",
      "Iteration 680, loss = 0.04421661\n",
      "Iteration 681, loss = 0.04411010\n",
      "Iteration 682, loss = 0.04400399\n",
      "Iteration 683, loss = 0.04389827\n",
      "Iteration 684, loss = 0.04379294\n",
      "Iteration 685, loss = 0.04368800\n",
      "Iteration 686, loss = 0.04358345\n",
      "Iteration 687, loss = 0.04347928\n",
      "Iteration 688, loss = 0.04337550\n",
      "Iteration 689, loss = 0.04327210\n",
      "Iteration 690, loss = 0.04316907\n",
      "Iteration 691, loss = 0.04306643\n",
      "Iteration 692, loss = 0.04296416\n",
      "Iteration 693, loss = 0.04286226\n",
      "Iteration 694, loss = 0.04276074\n",
      "Iteration 695, loss = 0.04265958\n",
      "Iteration 696, loss = 0.04255879\n",
      "Iteration 697, loss = 0.04245837\n",
      "Iteration 698, loss = 0.04235831\n",
      "Iteration 699, loss = 0.04225861\n",
      "Iteration 700, loss = 0.04215927\n",
      "Iteration 701, loss = 0.04206030\n",
      "Iteration 702, loss = 0.04196167\n",
      "Iteration 703, loss = 0.04186341\n",
      "Iteration 704, loss = 0.04176549\n",
      "Iteration 705, loss = 0.04166793\n",
      "Iteration 706, loss = 0.04157071\n",
      "Iteration 707, loss = 0.04147384\n",
      "Iteration 708, loss = 0.04137732\n",
      "Iteration 709, loss = 0.04128115\n",
      "Iteration 710, loss = 0.04118531\n",
      "Iteration 711, loss = 0.04108982\n",
      "Iteration 712, loss = 0.04099466\n",
      "Iteration 713, loss = 0.04089984\n",
      "Iteration 714, loss = 0.04080536\n",
      "Iteration 715, loss = 0.04071121\n",
      "Iteration 716, loss = 0.04061739\n",
      "Iteration 717, loss = 0.04052390\n",
      "Iteration 718, loss = 0.04043074\n",
      "Iteration 719, loss = 0.04033791\n",
      "Iteration 720, loss = 0.04024541\n",
      "Iteration 721, loss = 0.04015323\n",
      "Iteration 722, loss = 0.04006137\n",
      "Iteration 723, loss = 0.03996983\n",
      "Iteration 724, loss = 0.03987861\n",
      "Iteration 725, loss = 0.03978771\n",
      "Iteration 726, loss = 0.03969712\n",
      "Iteration 727, loss = 0.03960685\n",
      "Iteration 728, loss = 0.03951689\n",
      "Iteration 729, loss = 0.03942724\n",
      "Iteration 730, loss = 0.03933790\n",
      "Iteration 731, loss = 0.03924888\n",
      "Iteration 732, loss = 0.03916015\n",
      "Iteration 733, loss = 0.03907174\n",
      "Iteration 734, loss = 0.03898362\n",
      "Iteration 735, loss = 0.03889581\n",
      "Iteration 736, loss = 0.03880830\n",
      "Iteration 737, loss = 0.03872109\n",
      "Iteration 738, loss = 0.03863418\n",
      "Iteration 739, loss = 0.03854756\n",
      "Iteration 740, loss = 0.03846124\n",
      "Iteration 741, loss = 0.03837522\n",
      "Iteration 742, loss = 0.03828948\n",
      "Iteration 743, loss = 0.03820404\n",
      "Iteration 744, loss = 0.03811889\n",
      "Iteration 745, loss = 0.03803402\n",
      "Iteration 746, loss = 0.03794944\n",
      "Iteration 747, loss = 0.03786515\n",
      "Iteration 748, loss = 0.03778114\n",
      "Iteration 749, loss = 0.03769741\n",
      "Iteration 750, loss = 0.03761397\n",
      "Iteration 751, loss = 0.03753080\n",
      "Iteration 752, loss = 0.03744792\n",
      "Iteration 753, loss = 0.03736531\n",
      "Iteration 754, loss = 0.03728297\n",
      "Iteration 755, loss = 0.03720092\n",
      "Iteration 756, loss = 0.03711913\n",
      "Iteration 757, loss = 0.03703762\n",
      "Iteration 758, loss = 0.03695638\n",
      "Iteration 759, loss = 0.03687541\n",
      "Iteration 760, loss = 0.03679470\n",
      "Iteration 761, loss = 0.03671427\n",
      "Iteration 762, loss = 0.03663410\n",
      "Iteration 763, loss = 0.03655419\n",
      "Iteration 764, loss = 0.03647455\n",
      "Iteration 765, loss = 0.03639517\n",
      "Iteration 766, loss = 0.03631605\n",
      "Iteration 767, loss = 0.03623720\n",
      "Iteration 768, loss = 0.03615860\n",
      "Iteration 769, loss = 0.03608025\n",
      "Iteration 770, loss = 0.03600217\n",
      "Iteration 771, loss = 0.03592434\n",
      "Iteration 772, loss = 0.03584676\n",
      "Iteration 773, loss = 0.03576944\n",
      "Iteration 774, loss = 0.03569236\n",
      "Iteration 775, loss = 0.03561554\n",
      "Iteration 776, loss = 0.03553897\n",
      "Iteration 777, loss = 0.03546264\n",
      "Iteration 778, loss = 0.03538656\n",
      "Iteration 779, loss = 0.03531073\n",
      "Iteration 780, loss = 0.03523515\n",
      "Iteration 781, loss = 0.03515980\n",
      "Iteration 782, loss = 0.03508470\n",
      "Iteration 783, loss = 0.03500984\n",
      "Iteration 784, loss = 0.03493522\n",
      "Iteration 785, loss = 0.03486084\n",
      "Iteration 786, loss = 0.03478670\n",
      "Iteration 787, loss = 0.03471280\n",
      "Iteration 788, loss = 0.03463913\n",
      "Iteration 789, loss = 0.03456570\n",
      "Iteration 790, loss = 0.03449250\n",
      "Iteration 791, loss = 0.03441953\n",
      "Iteration 792, loss = 0.03434680\n",
      "Iteration 793, loss = 0.03427430\n",
      "Iteration 794, loss = 0.03420202\n",
      "Iteration 795, loss = 0.03412998\n",
      "Iteration 796, loss = 0.03405816\n",
      "Iteration 797, loss = 0.03398657\n",
      "Iteration 798, loss = 0.03391521\n",
      "Iteration 799, loss = 0.03384407\n",
      "Iteration 800, loss = 0.03377316\n",
      "Iteration 801, loss = 0.03370246\n",
      "Iteration 802, loss = 0.03363199\n",
      "Iteration 803, loss = 0.03356174\n",
      "Iteration 804, loss = 0.03349171\n",
      "Iteration 805, loss = 0.03342190\n",
      "Iteration 806, loss = 0.03335231\n",
      "Iteration 807, loss = 0.03328293\n",
      "Iteration 808, loss = 0.03321378\n",
      "Iteration 809, loss = 0.03314483\n",
      "Iteration 810, loss = 0.03307610\n",
      "Iteration 811, loss = 0.03300758\n",
      "Iteration 812, loss = 0.03293928\n",
      "Iteration 813, loss = 0.03287119\n",
      "Iteration 814, loss = 0.03280330\n",
      "Iteration 815, loss = 0.03273563\n",
      "Iteration 816, loss = 0.03266817\n",
      "Iteration 817, loss = 0.03260091\n",
      "Iteration 818, loss = 0.03253386\n",
      "Iteration 819, loss = 0.03246702\n",
      "Iteration 820, loss = 0.03240038\n",
      "Iteration 821, loss = 0.03233395\n",
      "Iteration 822, loss = 0.03226771\n",
      "Iteration 823, loss = 0.03220169\n",
      "Iteration 824, loss = 0.03213586\n",
      "Iteration 825, loss = 0.03207023\n",
      "Iteration 826, loss = 0.03200481\n",
      "Iteration 827, loss = 0.03193958\n",
      "Iteration 828, loss = 0.03187455\n",
      "Iteration 829, loss = 0.03180972\n",
      "Iteration 830, loss = 0.03174509\n",
      "Iteration 831, loss = 0.03168065\n",
      "Iteration 832, loss = 0.03161641\n",
      "Iteration 833, loss = 0.03155236\n",
      "Iteration 834, loss = 0.03148850\n",
      "Iteration 835, loss = 0.03142484\n",
      "Iteration 836, loss = 0.03136136\n",
      "Iteration 837, loss = 0.03129808\n",
      "Iteration 838, loss = 0.03123499\n",
      "Iteration 839, loss = 0.03117209\n",
      "Iteration 840, loss = 0.03110937\n",
      "Iteration 841, loss = 0.03104685\n",
      "Iteration 842, loss = 0.03098451\n",
      "Iteration 843, loss = 0.03092236\n",
      "Iteration 844, loss = 0.03086039\n",
      "Iteration 845, loss = 0.03079860\n",
      "Iteration 846, loss = 0.03073700\n",
      "Iteration 847, loss = 0.03067559\n",
      "Iteration 848, loss = 0.03061435\n",
      "Iteration 849, loss = 0.03055330\n",
      "Iteration 850, loss = 0.03049243\n",
      "Iteration 851, loss = 0.03043174\n",
      "Iteration 852, loss = 0.03037123\n",
      "Iteration 853, loss = 0.03031089\n",
      "Iteration 854, loss = 0.03025074\n",
      "Iteration 855, loss = 0.03019076\n",
      "Iteration 856, loss = 0.03013095\n",
      "Iteration 857, loss = 0.03007133\n",
      "Iteration 858, loss = 0.03001187\n",
      "Iteration 859, loss = 0.02995260\n",
      "Iteration 860, loss = 0.02989349\n",
      "Iteration 861, loss = 0.02983456\n",
      "Iteration 862, loss = 0.02977580\n",
      "Iteration 863, loss = 0.02971721\n",
      "Iteration 864, loss = 0.02965879\n",
      "Iteration 865, loss = 0.02960055\n",
      "Iteration 866, loss = 0.02954247\n",
      "Iteration 867, loss = 0.02948456\n",
      "Iteration 868, loss = 0.02942682\n",
      "Iteration 869, loss = 0.02936924\n",
      "Iteration 870, loss = 0.02931183\n",
      "Iteration 871, loss = 0.02925459\n",
      "Iteration 872, loss = 0.02919751\n",
      "Iteration 873, loss = 0.02914060\n",
      "Iteration 874, loss = 0.02908385\n",
      "Iteration 875, loss = 0.02902727\n",
      "Iteration 876, loss = 0.02897085\n",
      "Iteration 877, loss = 0.02891459\n",
      "Iteration 878, loss = 0.02885849\n",
      "Iteration 879, loss = 0.02880255\n",
      "Iteration 880, loss = 0.02874677\n",
      "Iteration 881, loss = 0.02869115\n",
      "Iteration 882, loss = 0.02863569\n",
      "Iteration 883, loss = 0.02858039\n",
      "Iteration 884, loss = 0.02852524\n",
      "Iteration 885, loss = 0.02847026\n",
      "Iteration 886, loss = 0.02841542\n",
      "Iteration 887, loss = 0.02836075\n",
      "Iteration 888, loss = 0.02830623\n",
      "Iteration 889, loss = 0.02825186\n",
      "Iteration 890, loss = 0.02819765\n",
      "Iteration 891, loss = 0.02814359\n",
      "Iteration 892, loss = 0.02808968\n",
      "Iteration 893, loss = 0.02803593\n",
      "Iteration 894, loss = 0.02798233\n",
      "Iteration 895, loss = 0.02792887\n",
      "Iteration 896, loss = 0.02787557\n",
      "Iteration 897, loss = 0.02782242\n",
      "Iteration 898, loss = 0.02776942\n",
      "Iteration 899, loss = 0.02771656\n",
      "Iteration 900, loss = 0.02766385\n",
      "Iteration 901, loss = 0.02761129\n",
      "Iteration 902, loss = 0.02755888\n",
      "Iteration 903, loss = 0.02750661\n",
      "Iteration 904, loss = 0.02745449\n",
      "Iteration 905, loss = 0.02740252\n",
      "Iteration 906, loss = 0.02735069\n",
      "Iteration 907, loss = 0.02729900\n",
      "Iteration 908, loss = 0.02724745\n",
      "Iteration 909, loss = 0.02719605\n",
      "Iteration 910, loss = 0.02714479\n",
      "Iteration 911, loss = 0.02709368\n",
      "Iteration 912, loss = 0.02704270\n",
      "Iteration 913, loss = 0.02699187\n",
      "Iteration 914, loss = 0.02694117\n",
      "Iteration 915, loss = 0.02689061\n",
      "Iteration 916, loss = 0.02684020\n",
      "Iteration 917, loss = 0.02678992\n",
      "Iteration 918, loss = 0.02673978\n",
      "Iteration 919, loss = 0.02668978\n",
      "Iteration 920, loss = 0.02663991\n",
      "Iteration 921, loss = 0.02659018\n",
      "Iteration 922, loss = 0.02654059\n",
      "Iteration 923, loss = 0.02649113\n",
      "Iteration 924, loss = 0.02644181\n",
      "Iteration 925, loss = 0.02639262\n",
      "Iteration 926, loss = 0.02634357\n",
      "Iteration 927, loss = 0.02629465\n",
      "Iteration 928, loss = 0.02624586\n",
      "Iteration 929, loss = 0.02619720\n",
      "Iteration 930, loss = 0.02614868\n",
      "Iteration 931, loss = 0.02610028\n",
      "Iteration 932, loss = 0.02605202\n",
      "Iteration 933, loss = 0.02600389\n",
      "Iteration 934, loss = 0.02595589\n",
      "Iteration 935, loss = 0.02590802\n",
      "Iteration 936, loss = 0.02586027\n",
      "Iteration 937, loss = 0.02581266\n",
      "Iteration 938, loss = 0.02576517\n",
      "Iteration 939, loss = 0.02571781\n",
      "Iteration 940, loss = 0.02567058\n",
      "Iteration 941, loss = 0.02562348\n",
      "Iteration 942, loss = 0.02557650\n",
      "Iteration 943, loss = 0.02552964\n",
      "Iteration 944, loss = 0.02548292\n",
      "Iteration 945, loss = 0.02543631\n",
      "Iteration 946, loss = 0.02538983\n",
      "Iteration 947, loss = 0.02534348\n",
      "Iteration 948, loss = 0.02529724\n",
      "Iteration 949, loss = 0.02525113\n",
      "Iteration 950, loss = 0.02520515\n",
      "Iteration 951, loss = 0.02515928\n",
      "Iteration 952, loss = 0.02511354\n",
      "Iteration 953, loss = 0.02506791\n",
      "Iteration 954, loss = 0.02502241\n",
      "Iteration 955, loss = 0.02497703\n",
      "Iteration 956, loss = 0.02493177\n",
      "Iteration 957, loss = 0.02488663\n",
      "Iteration 958, loss = 0.02484160\n",
      "Iteration 959, loss = 0.02479670\n",
      "Iteration 960, loss = 0.02475191\n",
      "Iteration 961, loss = 0.02470724\n",
      "Iteration 962, loss = 0.02466269\n",
      "Iteration 963, loss = 0.02461825\n",
      "Iteration 964, loss = 0.02457393\n",
      "Iteration 965, loss = 0.02452973\n",
      "Iteration 966, loss = 0.02448564\n",
      "Iteration 967, loss = 0.02444167\n",
      "Iteration 968, loss = 0.02439781\n",
      "Iteration 969, loss = 0.02435407\n",
      "Iteration 970, loss = 0.02431044\n",
      "Iteration 971, loss = 0.02426692\n",
      "Iteration 972, loss = 0.02422352\n",
      "Iteration 973, loss = 0.02418022\n",
      "Iteration 974, loss = 0.02413705\n",
      "Iteration 975, loss = 0.02409398\n",
      "Iteration 976, loss = 0.02405102\n",
      "Iteration 977, loss = 0.02400818\n",
      "Iteration 978, loss = 0.02396544\n",
      "Iteration 979, loss = 0.02392282\n",
      "Iteration 980, loss = 0.02388031\n",
      "Iteration 981, loss = 0.02383790\n",
      "Iteration 982, loss = 0.02379561\n",
      "Iteration 983, loss = 0.02375342\n",
      "Iteration 984, loss = 0.02371134\n",
      "Iteration 985, loss = 0.02366937\n",
      "Iteration 986, loss = 0.02362751\n",
      "Iteration 987, loss = 0.02358575\n",
      "Iteration 988, loss = 0.02354410\n",
      "Iteration 989, loss = 0.02350256\n",
      "Iteration 990, loss = 0.02346112\n",
      "Iteration 991, loss = 0.02341979\n",
      "Iteration 992, loss = 0.02337857\n",
      "Iteration 993, loss = 0.02333745\n",
      "Iteration 994, loss = 0.02329643\n",
      "Iteration 995, loss = 0.02325552\n",
      "Iteration 996, loss = 0.02321471\n",
      "Iteration 997, loss = 0.02317400\n",
      "Iteration 998, loss = 0.02313340\n",
      "Iteration 999, loss = 0.02309290\n",
      "Iteration 1000, loss = 0.02305250\n",
      "Iteration 1001, loss = 0.02301221\n",
      "Iteration 1002, loss = 0.02297202\n",
      "Iteration 1003, loss = 0.02293192\n",
      "Iteration 1004, loss = 0.02289193\n",
      "Iteration 1005, loss = 0.02285204\n",
      "Iteration 1006, loss = 0.02281225\n",
      "Iteration 1007, loss = 0.02277256\n",
      "Iteration 1008, loss = 0.02273297\n",
      "Iteration 1009, loss = 0.02269348\n",
      "Iteration 1010, loss = 0.02265409\n",
      "Iteration 1011, loss = 0.02261479\n",
      "Iteration 1012, loss = 0.02257560\n",
      "Iteration 1013, loss = 0.02253650\n",
      "Iteration 1014, loss = 0.02249750\n",
      "Iteration 1015, loss = 0.02245860\n",
      "Iteration 1016, loss = 0.02241979\n",
      "Iteration 1017, loss = 0.02238108\n",
      "Iteration 1018, loss = 0.02234247\n",
      "Iteration 1019, loss = 0.02230395\n",
      "Iteration 1020, loss = 0.02226553\n",
      "Iteration 1021, loss = 0.02222720\n",
      "Iteration 1022, loss = 0.02218897\n",
      "Iteration 1023, loss = 0.02215083\n",
      "Iteration 1024, loss = 0.02211279\n",
      "Iteration 1025, loss = 0.02207484\n",
      "Iteration 1026, loss = 0.02203698\n",
      "Iteration 1027, loss = 0.02199922\n",
      "Iteration 1028, loss = 0.02196155\n",
      "Iteration 1029, loss = 0.02192397\n",
      "Iteration 1030, loss = 0.02188649\n",
      "Iteration 1031, loss = 0.02184909\n",
      "Iteration 1032, loss = 0.02181179\n",
      "Iteration 1033, loss = 0.02177458\n",
      "Iteration 1034, loss = 0.02173746\n",
      "Iteration 1035, loss = 0.02170044\n",
      "Iteration 1036, loss = 0.02166350\n",
      "Iteration 1037, loss = 0.02162665\n",
      "Iteration 1038, loss = 0.02158989\n",
      "Iteration 1039, loss = 0.02155323\n",
      "Iteration 1040, loss = 0.02151665\n",
      "Iteration 1041, loss = 0.02148016\n",
      "Iteration 1042, loss = 0.02144376\n",
      "Iteration 1043, loss = 0.02140745\n",
      "Iteration 1044, loss = 0.02137122\n",
      "Iteration 1045, loss = 0.02133509\n",
      "Iteration 1046, loss = 0.02129904\n",
      "Iteration 1047, loss = 0.02126308\n",
      "Iteration 1048, loss = 0.02122720\n",
      "Iteration 1049, loss = 0.02119141\n",
      "Iteration 1050, loss = 0.02115571\n",
      "Iteration 1051, loss = 0.02112010\n",
      "Iteration 1052, loss = 0.02108457\n",
      "Iteration 1053, loss = 0.02104912\n",
      "Iteration 1054, loss = 0.02101376\n",
      "Iteration 1055, loss = 0.02097849\n",
      "Iteration 1056, loss = 0.02094330\n",
      "Iteration 1057, loss = 0.02090819\n",
      "Iteration 1058, loss = 0.02087317\n",
      "Iteration 1059, loss = 0.02083823\n",
      "Iteration 1060, loss = 0.02080338\n",
      "Iteration 1061, loss = 0.02076861\n",
      "Iteration 1062, loss = 0.02073392\n",
      "Iteration 1063, loss = 0.02069932\n",
      "Iteration 1064, loss = 0.02066480\n",
      "Iteration 1065, loss = 0.02063036\n",
      "Iteration 1066, loss = 0.02059600\n",
      "Iteration 1067, loss = 0.02056172\n",
      "Iteration 1068, loss = 0.02052753\n",
      "Iteration 1069, loss = 0.02049341\n",
      "Iteration 1070, loss = 0.02045938\n",
      "Iteration 1071, loss = 0.02042543\n",
      "Iteration 1072, loss = 0.02039155\n",
      "Iteration 1073, loss = 0.02035776\n",
      "Iteration 1074, loss = 0.02032405\n",
      "Iteration 1075, loss = 0.02029042\n",
      "Iteration 1076, loss = 0.02025686\n",
      "Iteration 1077, loss = 0.02022339\n",
      "Iteration 1078, loss = 0.02018999\n",
      "Iteration 1079, loss = 0.02015668\n",
      "Iteration 1080, loss = 0.02012344\n",
      "Iteration 1081, loss = 0.02009028\n",
      "Iteration 1082, loss = 0.02005719\n",
      "Iteration 1083, loss = 0.02002419\n",
      "Iteration 1084, loss = 0.01999126\n",
      "Iteration 1085, loss = 0.01995841\n",
      "Iteration 1086, loss = 0.01992564\n",
      "Iteration 1087, loss = 0.01989294\n",
      "Iteration 1088, loss = 0.01986032\n",
      "Iteration 1089, loss = 0.01982777\n",
      "Iteration 1090, loss = 0.01979530\n",
      "Iteration 1091, loss = 0.01976291\n",
      "Iteration 1092, loss = 0.01973059\n",
      "Iteration 1093, loss = 0.01969835\n",
      "Iteration 1094, loss = 0.01966618\n",
      "Iteration 1095, loss = 0.01963408\n",
      "Iteration 1096, loss = 0.01960206\n",
      "Iteration 1097, loss = 0.01957012\n",
      "Iteration 1098, loss = 0.01953825\n",
      "Iteration 1099, loss = 0.01950645\n",
      "Iteration 1100, loss = 0.01947472\n",
      "Iteration 1101, loss = 0.01944307\n",
      "Iteration 1102, loss = 0.01941149\n",
      "Iteration 1103, loss = 0.01937999\n",
      "Iteration 1104, loss = 0.01934855\n",
      "Iteration 1105, loss = 0.01931719\n",
      "Iteration 1106, loss = 0.01928590\n",
      "Iteration 1107, loss = 0.01925469\n",
      "Iteration 1108, loss = 0.01922354\n",
      "Iteration 1109, loss = 0.01919247\n",
      "Iteration 1110, loss = 0.01916147\n",
      "Iteration 1111, loss = 0.01913053\n",
      "Iteration 1112, loss = 0.01909967\n",
      "Iteration 1113, loss = 0.01906888\n",
      "Iteration 1114, loss = 0.01903816\n",
      "Iteration 1115, loss = 0.01900751\n",
      "Iteration 1116, loss = 0.01897693\n",
      "Iteration 1117, loss = 0.01894642\n",
      "Iteration 1118, loss = 0.01891598\n",
      "Iteration 1119, loss = 0.01888561\n",
      "Iteration 1120, loss = 0.01885530\n",
      "Iteration 1121, loss = 0.01882507\n",
      "Iteration 1122, loss = 0.01879490\n",
      "Iteration 1123, loss = 0.01876480\n",
      "Iteration 1124, loss = 0.01873477\n",
      "Iteration 1125, loss = 0.01870481\n",
      "Iteration 1126, loss = 0.01867492\n",
      "Iteration 1127, loss = 0.01864509\n",
      "Iteration 1128, loss = 0.01861533\n",
      "Iteration 1129, loss = 0.01858564\n",
      "Iteration 1130, loss = 0.01855602\n",
      "Iteration 1131, loss = 0.01852646\n",
      "Iteration 1132, loss = 0.01849697\n",
      "Iteration 1133, loss = 0.01846754\n",
      "Iteration 1134, loss = 0.01843818\n",
      "Iteration 1135, loss = 0.01840889\n",
      "Iteration 1136, loss = 0.01837966\n",
      "Iteration 1137, loss = 0.01835050\n",
      "Iteration 1138, loss = 0.01832140\n",
      "Iteration 1139, loss = 0.01829237\n",
      "Iteration 1140, loss = 0.01826340\n",
      "Iteration 1141, loss = 0.01823450\n",
      "Iteration 1142, loss = 0.01820566\n",
      "Iteration 1143, loss = 0.01817688\n",
      "Iteration 1144, loss = 0.01814817\n",
      "Iteration 1145, loss = 0.01811953\n",
      "Iteration 1146, loss = 0.01809094\n",
      "Iteration 1147, loss = 0.01806242\n",
      "Iteration 1148, loss = 0.01803397\n",
      "Iteration 1149, loss = 0.01800558\n",
      "Iteration 1150, loss = 0.01797725\n",
      "Iteration 1151, loss = 0.01794898\n",
      "Iteration 1152, loss = 0.01792077\n",
      "Iteration 1153, loss = 0.01789263\n",
      "Iteration 1154, loss = 0.01786455\n",
      "Iteration 1155, loss = 0.01783653\n",
      "Iteration 1156, loss = 0.01780858\n",
      "Iteration 1157, loss = 0.01778068\n",
      "Iteration 1158, loss = 0.01775285\n",
      "Iteration 1159, loss = 0.01772508\n",
      "Iteration 1160, loss = 0.01769737\n",
      "Iteration 1161, loss = 0.01766972\n",
      "Iteration 1162, loss = 0.01764213\n",
      "Iteration 1163, loss = 0.01761460\n",
      "Iteration 1164, loss = 0.01758713\n",
      "Iteration 1165, loss = 0.01755972\n",
      "Iteration 1166, loss = 0.01753237\n",
      "Iteration 1167, loss = 0.01750508\n",
      "Iteration 1168, loss = 0.01747786\n",
      "Iteration 1169, loss = 0.01745069\n",
      "Iteration 1170, loss = 0.01742358\n",
      "Iteration 1171, loss = 0.01739652\n",
      "Iteration 1172, loss = 0.01736953\n",
      "Iteration 1173, loss = 0.01734260\n",
      "Iteration 1174, loss = 0.01731572\n",
      "Iteration 1175, loss = 0.01728891\n",
      "Iteration 1176, loss = 0.01726215\n",
      "Iteration 1177, loss = 0.01723545\n",
      "Iteration 1178, loss = 0.01720880\n",
      "Iteration 1179, loss = 0.01718222\n",
      "Iteration 1180, loss = 0.01715569\n",
      "Iteration 1181, loss = 0.01712922\n",
      "Iteration 1182, loss = 0.01710281\n",
      "Iteration 1183, loss = 0.01707645\n",
      "Iteration 1184, loss = 0.01705015\n",
      "Iteration 1185, loss = 0.01702391\n",
      "Iteration 1186, loss = 0.01699773\n",
      "Iteration 1187, loss = 0.01697160\n",
      "Iteration 1188, loss = 0.01694552\n",
      "Iteration 1189, loss = 0.01691951\n",
      "Iteration 1190, loss = 0.01689354\n",
      "Iteration 1191, loss = 0.01686764\n",
      "Iteration 1192, loss = 0.01684179\n",
      "Iteration 1193, loss = 0.01681599\n",
      "Iteration 1194, loss = 0.01679025\n",
      "Iteration 1195, loss = 0.01676457\n",
      "Iteration 1196, loss = 0.01673894\n",
      "Iteration 1197, loss = 0.01671337\n",
      "Iteration 1198, loss = 0.01668784\n",
      "Iteration 1199, loss = 0.01666238\n",
      "Iteration 1200, loss = 0.01663697\n",
      "Iteration 1201, loss = 0.01661161\n",
      "Iteration 1202, loss = 0.01658631\n",
      "Iteration 1203, loss = 0.01656106\n",
      "Iteration 1204, loss = 0.01653586\n",
      "Iteration 1205, loss = 0.01651072\n",
      "Iteration 1206, loss = 0.01648563\n",
      "Iteration 1207, loss = 0.01646059\n",
      "Iteration 1208, loss = 0.01643561\n",
      "Iteration 1209, loss = 0.01641068\n",
      "Iteration 1210, loss = 0.01638580\n",
      "Iteration 1211, loss = 0.01636098\n",
      "Iteration 1212, loss = 0.01633620\n",
      "Iteration 1213, loss = 0.01631148\n",
      "Iteration 1214, loss = 0.01628681\n",
      "Iteration 1215, loss = 0.01626220\n",
      "Iteration 1216, loss = 0.01623763\n",
      "Iteration 1217, loss = 0.01621312\n",
      "Iteration 1218, loss = 0.01618866\n",
      "Iteration 1219, loss = 0.01616425\n",
      "Iteration 1220, loss = 0.01613989\n",
      "Iteration 1221, loss = 0.01611558\n",
      "Iteration 1222, loss = 0.01609133\n",
      "Iteration 1223, loss = 0.01606712\n",
      "Iteration 1224, loss = 0.01604297\n",
      "Iteration 1225, loss = 0.01601886\n",
      "Iteration 1226, loss = 0.01599481\n",
      "Iteration 1227, loss = 0.01597080\n",
      "Iteration 1228, loss = 0.01594685\n",
      "Iteration 1229, loss = 0.01592294\n",
      "Iteration 1230, loss = 0.01589909\n",
      "Iteration 1231, loss = 0.01587529\n",
      "Iteration 1232, loss = 0.01585153\n",
      "Iteration 1233, loss = 0.01582782\n",
      "Iteration 1234, loss = 0.01580417\n",
      "Iteration 1235, loss = 0.01578056\n",
      "Iteration 1236, loss = 0.01575700\n",
      "Iteration 1237, loss = 0.01573349\n",
      "Iteration 1238, loss = 0.01571003\n",
      "Iteration 1239, loss = 0.01568662\n",
      "Iteration 1240, loss = 0.01566325\n",
      "Iteration 1241, loss = 0.01563994\n",
      "Iteration 1242, loss = 0.01561667\n",
      "Iteration 1243, loss = 0.01559345\n",
      "Iteration 1244, loss = 0.01557028\n",
      "Iteration 1245, loss = 0.01554716\n",
      "Iteration 1246, loss = 0.01552408\n",
      "Iteration 1247, loss = 0.01550105\n",
      "Iteration 1248, loss = 0.01547807\n",
      "Iteration 1249, loss = 0.01545513\n",
      "Iteration 1250, loss = 0.01543225\n",
      "Iteration 1251, loss = 0.01540941\n",
      "Iteration 1252, loss = 0.01538661\n",
      "Iteration 1253, loss = 0.01536387\n",
      "Iteration 1254, loss = 0.01534117\n",
      "Iteration 1255, loss = 0.01531851\n",
      "Iteration 1256, loss = 0.01529590\n",
      "Iteration 1257, loss = 0.01527334\n",
      "Iteration 1258, loss = 0.01525083\n",
      "Iteration 1259, loss = 0.01522836\n",
      "Iteration 1260, loss = 0.01520594\n",
      "Iteration 1261, loss = 0.01518356\n",
      "Iteration 1262, loss = 0.01516122\n",
      "Iteration 1263, loss = 0.01513894\n",
      "Iteration 1264, loss = 0.01511670\n",
      "Iteration 1265, loss = 0.01509450\n",
      "Iteration 1266, loss = 0.01507235\n",
      "Iteration 1267, loss = 0.01505024\n",
      "Iteration 1268, loss = 0.01502818\n",
      "Iteration 1269, loss = 0.01500616\n",
      "Iteration 1270, loss = 0.01498419\n",
      "Iteration 1271, loss = 0.01496226\n",
      "Iteration 1272, loss = 0.01494038\n",
      "Iteration 1273, loss = 0.01491854\n",
      "Iteration 1274, loss = 0.01489674\n",
      "Iteration 1275, loss = 0.01487499\n",
      "Iteration 1276, loss = 0.01485328\n",
      "Iteration 1277, loss = 0.01483162\n",
      "Iteration 1278, loss = 0.01481000\n",
      "Iteration 1279, loss = 0.01478842\n",
      "Iteration 1280, loss = 0.01476689\n",
      "Iteration 1281, loss = 0.01474540\n",
      "Iteration 1282, loss = 0.01472395\n",
      "Iteration 1283, loss = 0.01470255\n",
      "Iteration 1284, loss = 0.01468119\n",
      "Iteration 1285, loss = 0.01465987\n",
      "Iteration 1286, loss = 0.01463859\n",
      "Iteration 1287, loss = 0.01461736\n",
      "Iteration 1288, loss = 0.01459617\n",
      "Iteration 1289, loss = 0.01457502\n",
      "Iteration 1290, loss = 0.01455391\n",
      "Iteration 1291, loss = 0.01453285\n",
      "Iteration 1292, loss = 0.01451182\n",
      "Iteration 1293, loss = 0.01449084\n",
      "Iteration 1294, loss = 0.01446990\n",
      "Iteration 1295, loss = 0.01444901\n",
      "Iteration 1296, loss = 0.01442815\n",
      "Iteration 1297, loss = 0.01440734\n",
      "Iteration 1298, loss = 0.01438656\n",
      "Iteration 1299, loss = 0.01436583\n",
      "Iteration 1300, loss = 0.01434514\n",
      "Iteration 1301, loss = 0.01432449\n",
      "Iteration 1302, loss = 0.01430388\n",
      "Iteration 1303, loss = 0.01428331\n",
      "Iteration 1304, loss = 0.01426279\n",
      "Iteration 1305, loss = 0.01424230\n",
      "Iteration 1306, loss = 0.01422185\n",
      "Iteration 1307, loss = 0.01420145\n",
      "Iteration 1308, loss = 0.01418108\n",
      "Iteration 1309, loss = 0.01416076\n",
      "Iteration 1310, loss = 0.01414047\n",
      "Iteration 1311, loss = 0.01412022\n",
      "Iteration 1312, loss = 0.01410002\n",
      "Iteration 1313, loss = 0.01407985\n",
      "Iteration 1314, loss = 0.01405972\n",
      "Iteration 1315, loss = 0.01403964\n",
      "Iteration 1316, loss = 0.01401959\n",
      "Iteration 1317, loss = 0.01399958\n",
      "Iteration 1318, loss = 0.01397961\n",
      "Iteration 1319, loss = 0.01395968\n",
      "Iteration 1320, loss = 0.01393979\n",
      "Iteration 1321, loss = 0.01391993\n",
      "Iteration 1322, loss = 0.01390012\n",
      "Iteration 1323, loss = 0.01388034\n",
      "Iteration 1324, loss = 0.01386061\n",
      "Iteration 1325, loss = 0.01384091\n",
      "Iteration 1326, loss = 0.01382125\n",
      "Iteration 1327, loss = 0.01380163\n",
      "Iteration 1328, loss = 0.01378204\n",
      "Iteration 1329, loss = 0.01376250\n",
      "Iteration 1330, loss = 0.01374299\n",
      "Iteration 1331, loss = 0.01372352\n",
      "Iteration 1332, loss = 0.01370409\n",
      "Iteration 1333, loss = 0.01368469\n",
      "Iteration 1334, loss = 0.01366534\n",
      "Iteration 1335, loss = 0.01364602\n",
      "Iteration 1336, loss = 0.01362674\n",
      "Iteration 1337, loss = 0.01360749\n",
      "Iteration 1338, loss = 0.01358828\n",
      "Iteration 1339, loss = 0.01356911\n",
      "Iteration 1340, loss = 0.01354998\n",
      "Iteration 1341, loss = 0.01353088\n",
      "Iteration 1342, loss = 0.01351182\n",
      "Iteration 1343, loss = 0.01349280\n",
      "Iteration 1344, loss = 0.01347381\n",
      "Iteration 1345, loss = 0.01345486\n",
      "Iteration 1346, loss = 0.01343595\n",
      "Iteration 1347, loss = 0.01341707\n",
      "Iteration 1348, loss = 0.01339823\n",
      "Iteration 1349, loss = 0.01337942\n",
      "Iteration 1350, loss = 0.01336065\n",
      "Iteration 1351, loss = 0.01334192\n",
      "Iteration 1352, loss = 0.01332322\n",
      "Iteration 1353, loss = 0.01330456\n",
      "Iteration 1354, loss = 0.01328594\n",
      "Iteration 1355, loss = 0.01326735\n",
      "Iteration 1356, loss = 0.01324879\n",
      "Iteration 1357, loss = 0.01323027\n",
      "Iteration 1358, loss = 0.01321179\n",
      "Iteration 1359, loss = 0.01319334\n",
      "Iteration 1360, loss = 0.01317492\n",
      "Iteration 1361, loss = 0.01315655\n",
      "Iteration 1362, loss = 0.01313820\n",
      "Iteration 1363, loss = 0.01311989\n",
      "Iteration 1364, loss = 0.01310162\n",
      "Iteration 1365, loss = 0.01308338\n",
      "Iteration 1366, loss = 0.01306517\n",
      "Iteration 1367, loss = 0.01304700\n",
      "Iteration 1368, loss = 0.01302887\n",
      "Iteration 1369, loss = 0.01301077\n",
      "Iteration 1370, loss = 0.01299270\n",
      "Iteration 1371, loss = 0.01297466\n",
      "Iteration 1372, loss = 0.01295666\n",
      "Iteration 1373, loss = 0.01293870\n",
      "Iteration 1374, loss = 0.01292077\n",
      "Iteration 1375, loss = 0.01290287\n",
      "Iteration 1376, loss = 0.01288501\n",
      "Iteration 1377, loss = 0.01286718\n",
      "Iteration 1378, loss = 0.01284938\n",
      "Iteration 1379, loss = 0.01283162\n",
      "Iteration 1380, loss = 0.01281389\n",
      "Iteration 1381, loss = 0.01279619\n",
      "Iteration 1382, loss = 0.01277852\n",
      "Iteration 1383, loss = 0.01276089\n",
      "Iteration 1384, loss = 0.01274330\n",
      "Iteration 1385, loss = 0.01272573\n",
      "Iteration 1386, loss = 0.01270820\n",
      "Iteration 1387, loss = 0.01269070\n",
      "Iteration 1388, loss = 0.01267324\n",
      "Iteration 1389, loss = 0.01265580\n",
      "Iteration 1390, loss = 0.01263840\n",
      "Iteration 1391, loss = 0.01262103\n",
      "Iteration 1392, loss = 0.01260370\n",
      "Iteration 1393, loss = 0.01258639\n",
      "Iteration 1394, loss = 0.01256912\n",
      "Iteration 1395, loss = 0.01255188\n",
      "Iteration 1396, loss = 0.01253468\n",
      "Iteration 1397, loss = 0.01251750\n",
      "Iteration 1398, loss = 0.01250036\n",
      "Iteration 1399, loss = 0.01248325\n",
      "Iteration 1400, loss = 0.01246617\n",
      "Iteration 1401, loss = 0.01244912\n",
      "Iteration 1402, loss = 0.01243210\n",
      "Iteration 1403, loss = 0.01241512\n",
      "Iteration 1404, loss = 0.01239816\n",
      "Iteration 1405, loss = 0.01238124\n",
      "Iteration 1406, loss = 0.01236435\n",
      "Iteration 1407, loss = 0.01234749\n",
      "Iteration 1408, loss = 0.01233066\n",
      "Iteration 1409, loss = 0.01231386\n",
      "Iteration 1410, loss = 0.01229710\n",
      "Iteration 1411, loss = 0.01228036\n",
      "Iteration 1412, loss = 0.01226366\n",
      "Iteration 1413, loss = 0.01224698\n",
      "Iteration 1414, loss = 0.01223034\n",
      "Iteration 1415, loss = 0.01221373\n",
      "Iteration 1416, loss = 0.01219714\n",
      "Iteration 1417, loss = 0.01218059\n",
      "Iteration 1418, loss = 0.01216407\n",
      "Iteration 1419, loss = 0.01214758\n",
      "Iteration 1420, loss = 0.01213112\n",
      "Iteration 1421, loss = 0.01211469\n",
      "Iteration 1422, loss = 0.01209829\n",
      "Iteration 1423, loss = 0.01208192\n",
      "Iteration 1424, loss = 0.01206558\n",
      "Iteration 1425, loss = 0.01204926\n",
      "Iteration 1426, loss = 0.01203298\n",
      "Iteration 1427, loss = 0.01201673\n",
      "Iteration 1428, loss = 0.01200051\n",
      "Iteration 1429, loss = 0.01198432\n",
      "Iteration 1430, loss = 0.01196816\n",
      "Iteration 1431, loss = 0.01195202\n",
      "Iteration 1432, loss = 0.01193592\n",
      "Iteration 1433, loss = 0.01191984\n",
      "Iteration 1434, loss = 0.01190380\n",
      "Iteration 1435, loss = 0.01188778\n",
      "Iteration 1436, loss = 0.01187179\n",
      "Iteration 1437, loss = 0.01185584\n",
      "Iteration 1438, loss = 0.01183991\n",
      "Iteration 1439, loss = 0.01182401\n",
      "Iteration 1440, loss = 0.01180813\n",
      "Iteration 1441, loss = 0.01179229\n",
      "Iteration 1442, loss = 0.01177648\n",
      "Iteration 1443, loss = 0.01176069\n",
      "Iteration 1444, loss = 0.01174493\n",
      "Iteration 1445, loss = 0.01172920\n",
      "Iteration 1446, loss = 0.01171350\n",
      "Iteration 1447, loss = 0.01169783\n",
      "Iteration 1448, loss = 0.01168219\n",
      "Iteration 1449, loss = 0.01166657\n",
      "Iteration 1450, loss = 0.01165098\n",
      "Iteration 1451, loss = 0.01163542\n",
      "Iteration 1452, loss = 0.01161989\n",
      "Iteration 1453, loss = 0.01160439\n",
      "Iteration 1454, loss = 0.01158891\n",
      "Iteration 1455, loss = 0.01157346\n",
      "Iteration 1456, loss = 0.01155804\n",
      "Iteration 1457, loss = 0.01154265\n",
      "Iteration 1458, loss = 0.01152728\n",
      "Iteration 1459, loss = 0.01151195\n",
      "Iteration 1460, loss = 0.01149664\n",
      "Iteration 1461, loss = 0.01148135\n",
      "Iteration 1462, loss = 0.01146610\n",
      "Iteration 1463, loss = 0.01145087\n",
      "Iteration 1464, loss = 0.01143567\n",
      "Iteration 1465, loss = 0.01142049\n",
      "Iteration 1466, loss = 0.01140535\n",
      "Iteration 1467, loss = 0.01139023\n",
      "Iteration 1468, loss = 0.01137513\n",
      "Iteration 1469, loss = 0.01136007\n",
      "Iteration 1470, loss = 0.01134503\n",
      "Iteration 1471, loss = 0.01133001\n",
      "Iteration 1472, loss = 0.01131503\n",
      "Iteration 1473, loss = 0.01130007\n",
      "Iteration 1474, loss = 0.01128514\n",
      "Iteration 1475, loss = 0.01127023\n",
      "Iteration 1476, loss = 0.01125535\n",
      "Iteration 1477, loss = 0.01124049\n",
      "Iteration 1478, loss = 0.01122567\n",
      "Iteration 1479, loss = 0.01121087\n",
      "Iteration 1480, loss = 0.01119609\n",
      "Iteration 1481, loss = 0.01118134\n",
      "Iteration 1482, loss = 0.01116662\n",
      "Iteration 1483, loss = 0.01115192\n",
      "Iteration 1484, loss = 0.01113725\n",
      "Iteration 1485, loss = 0.01112261\n",
      "Iteration 1486, loss = 0.01110799\n",
      "Iteration 1487, loss = 0.01109339\n",
      "Iteration 1488, loss = 0.01107882\n",
      "Iteration 1489, loss = 0.01106428\n",
      "Iteration 1490, loss = 0.01104977\n",
      "Iteration 1491, loss = 0.01103527\n",
      "Iteration 1492, loss = 0.01102081\n",
      "Iteration 1493, loss = 0.01100637\n",
      "Iteration 1494, loss = 0.01099195\n",
      "Iteration 1495, loss = 0.01097756\n",
      "Iteration 1496, loss = 0.01096320\n",
      "Iteration 1497, loss = 0.01094886\n",
      "Iteration 1498, loss = 0.01093454\n",
      "Iteration 1499, loss = 0.01092025\n",
      "Iteration 1500, loss = 0.01090599\n",
      "Iteration 1501, loss = 0.01089175\n",
      "Iteration 1502, loss = 0.01087754\n",
      "Iteration 1503, loss = 0.01086335\n",
      "Iteration 1504, loss = 0.01084918\n",
      "Iteration 1505, loss = 0.01083504\n",
      "Iteration 1506, loss = 0.01082092\n",
      "Iteration 1507, loss = 0.01080683\n",
      "Iteration 1508, loss = 0.01079277\n",
      "Iteration 1509, loss = 0.01077872\n",
      "Iteration 1510, loss = 0.01076471\n",
      "Iteration 1511, loss = 0.01075071\n",
      "Iteration 1512, loss = 0.01073674\n",
      "Iteration 1513, loss = 0.01072280\n",
      "Iteration 1514, loss = 0.01070888\n",
      "Iteration 1515, loss = 0.01069498\n",
      "Iteration 1516, loss = 0.01068111\n",
      "Iteration 1517, loss = 0.01066726\n",
      "Iteration 1518, loss = 0.01065343\n",
      "Iteration 1519, loss = 0.01063963\n",
      "Iteration 1520, loss = 0.01062586\n",
      "Iteration 1521, loss = 0.01061210\n",
      "Iteration 1522, loss = 0.01059837\n",
      "Iteration 1523, loss = 0.01058467\n",
      "Iteration 1524, loss = 0.01057099\n",
      "Iteration 1525, loss = 0.01055733\n",
      "Iteration 1526, loss = 0.01054369\n",
      "Iteration 1527, loss = 0.01053008\n",
      "Iteration 1528, loss = 0.01051649\n",
      "Iteration 1529, loss = 0.01050293\n",
      "Iteration 1530, loss = 0.01048939\n",
      "Iteration 1531, loss = 0.01047587\n",
      "Iteration 1532, loss = 0.01046237\n",
      "Iteration 1533, loss = 0.01044890\n",
      "Iteration 1534, loss = 0.01043545\n",
      "Iteration 1535, loss = 0.01042203\n",
      "Iteration 1536, loss = 0.01040863\n",
      "Iteration 1537, loss = 0.01039525\n",
      "Iteration 1538, loss = 0.01038189\n",
      "Iteration 1539, loss = 0.01036855\n",
      "Iteration 1540, loss = 0.01035524\n",
      "Iteration 1541, loss = 0.01034195\n",
      "Iteration 1542, loss = 0.01032869\n",
      "Iteration 1543, loss = 0.01031545\n",
      "Iteration 1544, loss = 0.01030222\n",
      "Iteration 1545, loss = 0.01028903\n",
      "Iteration 1546, loss = 0.01027585\n",
      "Iteration 1547, loss = 0.01026270\n",
      "Iteration 1548, loss = 0.01024957\n",
      "Iteration 1549, loss = 0.01023646\n",
      "Iteration 1550, loss = 0.01022337\n",
      "Iteration 1551, loss = 0.01021031\n",
      "Iteration 1552, loss = 0.01019727\n",
      "Iteration 1553, loss = 0.01018425\n",
      "Iteration 1554, loss = 0.01017125\n",
      "Iteration 1555, loss = 0.01015827\n",
      "Iteration 1556, loss = 0.01014532\n",
      "Iteration 1557, loss = 0.01013239\n",
      "Iteration 1558, loss = 0.01011948\n",
      "Iteration 1559, loss = 0.01010659\n",
      "Iteration 1560, loss = 0.01009372\n",
      "Iteration 1561, loss = 0.01008088\n",
      "Iteration 1562, loss = 0.01006806\n",
      "Iteration 1563, loss = 0.01005526\n",
      "Iteration 1564, loss = 0.01004248\n",
      "Iteration 1565, loss = 0.01002972\n",
      "Iteration 1566, loss = 0.01001698\n",
      "Iteration 1567, loss = 0.01000427\n",
      "Iteration 1568, loss = 0.00999157\n",
      "Iteration 1569, loss = 0.00997890\n",
      "Iteration 1570, loss = 0.00996625\n",
      "Iteration 1571, loss = 0.00995362\n",
      "Iteration 1572, loss = 0.00994101\n",
      "Iteration 1573, loss = 0.00992842\n",
      "Iteration 1574, loss = 0.00991586\n",
      "Iteration 1575, loss = 0.00990331\n",
      "Iteration 1576, loss = 0.00989079\n",
      "Iteration 1577, loss = 0.00987829\n",
      "Iteration 1578, loss = 0.00986580\n",
      "Iteration 1579, loss = 0.00985334\n",
      "Iteration 1580, loss = 0.00984090\n",
      "Iteration 1581, loss = 0.00982848\n",
      "Iteration 1582, loss = 0.00981608\n",
      "Iteration 1583, loss = 0.00980370\n",
      "Iteration 1584, loss = 0.00979135\n",
      "Iteration 1585, loss = 0.00977901\n",
      "Iteration 1586, loss = 0.00976669\n",
      "Iteration 1587, loss = 0.00975440\n",
      "Iteration 1588, loss = 0.00974212\n",
      "Iteration 1589, loss = 0.00972987\n",
      "Iteration 1590, loss = 0.00971763\n",
      "Iteration 1591, loss = 0.00970542\n",
      "Iteration 1592, loss = 0.00969322\n",
      "Iteration 1593, loss = 0.00968105\n",
      "Iteration 1594, loss = 0.00966890\n",
      "Iteration 1595, loss = 0.00965676\n",
      "Iteration 1596, loss = 0.00964465\n",
      "Iteration 1597, loss = 0.00963256\n",
      "Iteration 1598, loss = 0.00962048\n",
      "Iteration 1599, loss = 0.00960843\n",
      "Iteration 1600, loss = 0.00959640\n",
      "Iteration 1601, loss = 0.00958438\n",
      "Iteration 1602, loss = 0.00957239\n",
      "Iteration 1603, loss = 0.00956041\n",
      "Iteration 1604, loss = 0.00954846\n",
      "Iteration 1605, loss = 0.00953653\n",
      "Iteration 1606, loss = 0.00952461\n",
      "Iteration 1607, loss = 0.00951272\n",
      "Iteration 1608, loss = 0.00950084\n",
      "Iteration 1609, loss = 0.00948899\n",
      "Iteration 1610, loss = 0.00947715\n",
      "Iteration 1611, loss = 0.00946533\n",
      "Iteration 1612, loss = 0.00945353\n",
      "Iteration 1613, loss = 0.00944176\n",
      "Iteration 1614, loss = 0.00943000\n",
      "Iteration 1615, loss = 0.00941826\n",
      "Iteration 1616, loss = 0.00940654\n",
      "Iteration 1617, loss = 0.00939484\n",
      "Iteration 1618, loss = 0.00938316\n",
      "Iteration 1619, loss = 0.00937149\n",
      "Iteration 1620, loss = 0.00935985\n",
      "Iteration 1621, loss = 0.00934823\n",
      "Iteration 1622, loss = 0.00933662\n",
      "Iteration 1623, loss = 0.00932503\n",
      "Iteration 1624, loss = 0.00931347\n",
      "Iteration 1625, loss = 0.00930192\n",
      "Iteration 1626, loss = 0.00929039\n",
      "Iteration 1627, loss = 0.00927888\n",
      "Iteration 1628, loss = 0.00926738\n",
      "Iteration 1629, loss = 0.00925591\n",
      "Iteration 1630, loss = 0.00924446\n",
      "Iteration 1631, loss = 0.00923302\n",
      "Iteration 1632, loss = 0.00922160\n",
      "Iteration 1633, loss = 0.00921020\n",
      "Iteration 1634, loss = 0.00919882\n",
      "Iteration 1635, loss = 0.00918746\n",
      "Iteration 1636, loss = 0.00917612\n",
      "Iteration 1637, loss = 0.00916479\n",
      "Iteration 1638, loss = 0.00915349\n",
      "Iteration 1639, loss = 0.00914220\n",
      "Iteration 1640, loss = 0.00913093\n",
      "Iteration 1641, loss = 0.00911968\n",
      "Iteration 1642, loss = 0.00910844\n",
      "Iteration 1643, loss = 0.00909723\n",
      "Iteration 1644, loss = 0.00908603\n",
      "Iteration 1645, loss = 0.00907485\n",
      "Iteration 1646, loss = 0.00906369\n",
      "Iteration 1647, loss = 0.00905255\n",
      "Iteration 1648, loss = 0.00904142\n",
      "Iteration 1649, loss = 0.00903031\n",
      "Iteration 1650, loss = 0.00901923\n",
      "Iteration 1651, loss = 0.00900815\n",
      "Iteration 1652, loss = 0.00899710\n",
      "Iteration 1653, loss = 0.00898606\n",
      "Iteration 1654, loss = 0.00897505\n",
      "Iteration 1655, loss = 0.00896405\n",
      "Iteration 1656, loss = 0.00895306\n",
      "Iteration 1657, loss = 0.00894210\n",
      "Iteration 1658, loss = 0.00893115\n",
      "Iteration 1659, loss = 0.00892022\n",
      "Iteration 1660, loss = 0.00890931\n",
      "Iteration 1661, loss = 0.00889842\n",
      "Iteration 1662, loss = 0.00888754\n",
      "Iteration 1663, loss = 0.00887668\n",
      "Iteration 1664, loss = 0.00886584\n",
      "Iteration 1665, loss = 0.00885501\n",
      "Iteration 1666, loss = 0.00884420\n",
      "Iteration 1667, loss = 0.00883341\n",
      "Iteration 1668, loss = 0.00882264\n",
      "Iteration 1669, loss = 0.00881188\n",
      "Iteration 1670, loss = 0.00880114\n",
      "Iteration 1671, loss = 0.00879042\n",
      "Iteration 1672, loss = 0.00877972\n",
      "Iteration 1673, loss = 0.00876903\n",
      "Iteration 1674, loss = 0.00875836\n",
      "Iteration 1675, loss = 0.00874771\n",
      "Iteration 1676, loss = 0.00873707\n",
      "Iteration 1677, loss = 0.00872645\n",
      "Iteration 1678, loss = 0.00871585\n",
      "Iteration 1679, loss = 0.00870526\n",
      "Iteration 1680, loss = 0.00869469\n",
      "Iteration 1681, loss = 0.00868414\n",
      "Iteration 1682, loss = 0.00867360\n",
      "Iteration 1683, loss = 0.00866309\n",
      "Iteration 1684, loss = 0.00865258\n",
      "Iteration 1685, loss = 0.00864210\n",
      "Iteration 1686, loss = 0.00863163\n",
      "Iteration 1687, loss = 0.00862118\n",
      "Iteration 1688, loss = 0.00861074\n",
      "Iteration 1689, loss = 0.00860032\n",
      "Iteration 1690, loss = 0.00858992\n",
      "Iteration 1691, loss = 0.00857953\n",
      "Iteration 1692, loss = 0.00856916\n",
      "Iteration 1693, loss = 0.00855881\n",
      "Iteration 1694, loss = 0.00854847\n",
      "Iteration 1695, loss = 0.00853815\n",
      "Iteration 1696, loss = 0.00852785\n",
      "Iteration 1697, loss = 0.00851756\n",
      "Iteration 1698, loss = 0.00850729\n",
      "Iteration 1699, loss = 0.00849703\n",
      "Iteration 1700, loss = 0.00848679\n",
      "Iteration 1701, loss = 0.00847657\n",
      "Iteration 1702, loss = 0.00846636\n",
      "Iteration 1703, loss = 0.00845617\n",
      "Iteration 1704, loss = 0.00844599\n",
      "Iteration 1705, loss = 0.00843583\n",
      "Iteration 1706, loss = 0.00842569\n",
      "Iteration 1707, loss = 0.00841556\n",
      "Iteration 1708, loss = 0.00840545\n",
      "Iteration 1709, loss = 0.00839535\n",
      "Iteration 1710, loss = 0.00838527\n",
      "Iteration 1711, loss = 0.00837521\n",
      "Iteration 1712, loss = 0.00836516\n",
      "Iteration 1713, loss = 0.00835512\n",
      "Iteration 1714, loss = 0.00834511\n",
      "Iteration 1715, loss = 0.00833510\n",
      "Iteration 1716, loss = 0.00832512\n",
      "Iteration 1717, loss = 0.00831515\n",
      "Iteration 1718, loss = 0.00830519\n",
      "Iteration 1719, loss = 0.00829525\n",
      "Iteration 1720, loss = 0.00828533\n",
      "Iteration 1721, loss = 0.00827542\n",
      "Iteration 1722, loss = 0.00826553\n",
      "Iteration 1723, loss = 0.00825565\n",
      "Iteration 1724, loss = 0.00824579\n",
      "Iteration 1725, loss = 0.00823594\n",
      "Iteration 1726, loss = 0.00822611\n",
      "Iteration 1727, loss = 0.00821629\n",
      "Iteration 1728, loss = 0.00820649\n",
      "Iteration 1729, loss = 0.00819670\n",
      "Iteration 1730, loss = 0.00818693\n",
      "Iteration 1731, loss = 0.00817718\n",
      "Iteration 1732, loss = 0.00816744\n",
      "Iteration 1733, loss = 0.00815771\n",
      "Iteration 1734, loss = 0.00814800\n",
      "Iteration 1735, loss = 0.00813830\n",
      "Iteration 1736, loss = 0.00812862\n",
      "Iteration 1737, loss = 0.00811896\n",
      "Iteration 1738, loss = 0.00810931\n",
      "Iteration 1739, loss = 0.00809967\n",
      "Iteration 1740, loss = 0.00809005\n",
      "Iteration 1741, loss = 0.00808045\n",
      "Iteration 1742, loss = 0.00807085\n",
      "Iteration 1743, loss = 0.00806128\n",
      "Iteration 1744, loss = 0.00805172\n",
      "Iteration 1745, loss = 0.00804217\n",
      "Iteration 1746, loss = 0.00803264\n",
      "Iteration 1747, loss = 0.00802312\n",
      "Iteration 1748, loss = 0.00801362\n",
      "Iteration 1749, loss = 0.00800413\n",
      "Iteration 1750, loss = 0.00799465\n",
      "Iteration 1751, loss = 0.00798520\n",
      "Iteration 1752, loss = 0.00797575\n",
      "Iteration 1753, loss = 0.00796632\n",
      "Iteration 1754, loss = 0.00795691\n",
      "Iteration 1755, loss = 0.00794750\n",
      "Iteration 1756, loss = 0.00793812\n",
      "Iteration 1757, loss = 0.00792874\n",
      "Iteration 1758, loss = 0.00791939\n",
      "Iteration 1759, loss = 0.00791004\n",
      "Iteration 1760, loss = 0.00790071\n",
      "Iteration 1761, loss = 0.00789140\n",
      "Iteration 1762, loss = 0.00788210\n",
      "Iteration 1763, loss = 0.00787281\n",
      "Iteration 1764, loss = 0.00786354\n",
      "Iteration 1765, loss = 0.00785428\n",
      "Iteration 1766, loss = 0.00784504\n",
      "Iteration 1767, loss = 0.00783581\n",
      "Iteration 1768, loss = 0.00782659\n",
      "Iteration 1769, loss = 0.00781739\n",
      "Iteration 1770, loss = 0.00780820\n",
      "Iteration 1771, loss = 0.00779902\n",
      "Iteration 1772, loss = 0.00778986\n",
      "Iteration 1773, loss = 0.00778072\n",
      "Iteration 1774, loss = 0.00777159\n",
      "Iteration 1775, loss = 0.00776247\n",
      "Iteration 1776, loss = 0.00775336\n",
      "Iteration 1777, loss = 0.00774427\n",
      "Iteration 1778, loss = 0.00773520\n",
      "Iteration 1779, loss = 0.00772613\n",
      "Iteration 1780, loss = 0.00771708\n",
      "Iteration 1781, loss = 0.00770805\n",
      "Iteration 1782, loss = 0.00769902\n",
      "Iteration 1783, loss = 0.00769002\n",
      "Iteration 1784, loss = 0.00768102\n",
      "Iteration 1785, loss = 0.00767204\n",
      "Iteration 1786, loss = 0.00766307\n",
      "Iteration 1787, loss = 0.00765412\n",
      "Iteration 1788, loss = 0.00764518\n",
      "Iteration 1789, loss = 0.00763625\n",
      "Iteration 1790, loss = 0.00762734\n",
      "Iteration 1791, loss = 0.00761844\n",
      "Iteration 1792, loss = 0.00760955\n",
      "Iteration 1793, loss = 0.00760068\n",
      "Iteration 1794, loss = 0.00759182\n",
      "Iteration 1795, loss = 0.00758297\n",
      "Iteration 1796, loss = 0.00757414\n",
      "Iteration 1797, loss = 0.00756532\n",
      "Iteration 1798, loss = 0.00755651\n",
      "Iteration 1799, loss = 0.00754772\n",
      "Iteration 1800, loss = 0.00753894\n",
      "Iteration 1801, loss = 0.00753017\n",
      "Iteration 1802, loss = 0.00752142\n",
      "Iteration 1803, loss = 0.00751267\n",
      "Iteration 1804, loss = 0.00750395\n",
      "Iteration 1805, loss = 0.00749523\n",
      "Iteration 1806, loss = 0.00748653\n",
      "Iteration 1807, loss = 0.00747784\n",
      "Iteration 1808, loss = 0.00746917\n",
      "Iteration 1809, loss = 0.00746050\n",
      "Iteration 1810, loss = 0.00745185\n",
      "Iteration 1811, loss = 0.00744322\n",
      "Iteration 1812, loss = 0.00743459\n",
      "Iteration 1813, loss = 0.00742598\n",
      "Iteration 1814, loss = 0.00741739\n",
      "Iteration 1815, loss = 0.00740880\n",
      "Iteration 1816, loss = 0.00740023\n",
      "Iteration 1817, loss = 0.00739167\n",
      "Iteration 1818, loss = 0.00738312\n",
      "Iteration 1819, loss = 0.00737459\n",
      "Iteration 1820, loss = 0.00736607\n",
      "Iteration 1821, loss = 0.00735756\n",
      "Iteration 1822, loss = 0.00734906\n",
      "Iteration 1823, loss = 0.00734058\n",
      "Iteration 1824, loss = 0.00733211\n",
      "Iteration 1825, loss = 0.00732365\n",
      "Iteration 1826, loss = 0.00731520\n",
      "Iteration 1827, loss = 0.00730677\n",
      "Iteration 1828, loss = 0.00729835\n",
      "Iteration 1829, loss = 0.00728994\n",
      "Iteration 1830, loss = 0.00728155\n",
      "Iteration 1831, loss = 0.00727316\n",
      "Iteration 1832, loss = 0.00726479\n",
      "Iteration 1833, loss = 0.00725643\n",
      "Iteration 1834, loss = 0.00724809\n",
      "Iteration 1835, loss = 0.00723975\n",
      "Iteration 1836, loss = 0.00723143\n",
      "Iteration 1837, loss = 0.00722312\n",
      "Iteration 1838, loss = 0.00721483\n",
      "Iteration 1839, loss = 0.00720654\n",
      "Iteration 1840, loss = 0.00719827\n",
      "Iteration 1841, loss = 0.00719001\n",
      "Iteration 1842, loss = 0.00718176\n",
      "Iteration 1843, loss = 0.00717353\n",
      "Iteration 1844, loss = 0.00716530\n",
      "Iteration 1845, loss = 0.00715709\n",
      "Iteration 1846, loss = 0.00714889\n",
      "Iteration 1847, loss = 0.00714071\n",
      "Iteration 1848, loss = 0.00713253\n",
      "Iteration 1849, loss = 0.00712437\n",
      "Iteration 1850, loss = 0.00711622\n",
      "Iteration 1851, loss = 0.00710808\n",
      "Iteration 1852, loss = 0.00709995\n",
      "Iteration 1853, loss = 0.00709184\n",
      "Iteration 1854, loss = 0.00708373\n",
      "Iteration 1855, loss = 0.00707564\n",
      "Iteration 1856, loss = 0.00706756\n",
      "Iteration 1857, loss = 0.00705949\n",
      "Iteration 1858, loss = 0.00705144\n",
      "Iteration 1859, loss = 0.00704339\n",
      "Iteration 1860, loss = 0.00703536\n",
      "Iteration 1861, loss = 0.00702734\n",
      "Iteration 1862, loss = 0.00701933\n",
      "Iteration 1863, loss = 0.00701133\n",
      "Iteration 1864, loss = 0.00700335\n",
      "Iteration 1865, loss = 0.00699538\n",
      "Iteration 1866, loss = 0.00698741\n",
      "Iteration 1867, loss = 0.00697946\n",
      "Iteration 1868, loss = 0.00697152\n",
      "Iteration 1869, loss = 0.00696360\n",
      "Iteration 1870, loss = 0.00695568\n",
      "Iteration 1871, loss = 0.00694778\n",
      "Iteration 1872, loss = 0.00693988\n",
      "Iteration 1873, loss = 0.00693200\n",
      "Iteration 1874, loss = 0.00692413\n",
      "Iteration 1875, loss = 0.00691627\n",
      "Iteration 1876, loss = 0.00690843\n",
      "Iteration 1877, loss = 0.00690059\n",
      "Iteration 1878, loss = 0.00689277\n",
      "Iteration 1879, loss = 0.00688496\n",
      "Iteration 1880, loss = 0.00687715\n",
      "Iteration 1881, loss = 0.00686936\n",
      "Iteration 1882, loss = 0.00686159\n",
      "Iteration 1883, loss = 0.00685382\n",
      "Iteration 1884, loss = 0.00684606\n",
      "Iteration 1885, loss = 0.00683832\n",
      "Iteration 1886, loss = 0.00683058\n",
      "Iteration 1887, loss = 0.00682286\n",
      "Iteration 1888, loss = 0.00681515\n",
      "Iteration 1889, loss = 0.00680745\n",
      "Iteration 1890, loss = 0.00679976\n",
      "Iteration 1891, loss = 0.00679208\n",
      "Iteration 1892, loss = 0.00678442\n",
      "Iteration 1893, loss = 0.00677676\n",
      "Iteration 1894, loss = 0.00676912\n",
      "Iteration 1895, loss = 0.00676148\n",
      "Iteration 1896, loss = 0.00675386\n",
      "Iteration 1897, loss = 0.00674625\n",
      "Iteration 1898, loss = 0.00673865\n",
      "Iteration 1899, loss = 0.00673106\n",
      "Iteration 1900, loss = 0.00672348\n",
      "Iteration 1901, loss = 0.00671591\n",
      "Iteration 1902, loss = 0.00670836\n",
      "Iteration 1903, loss = 0.00670081\n",
      "Iteration 1904, loss = 0.00669327\n",
      "Iteration 1905, loss = 0.00668575\n",
      "Iteration 1906, loss = 0.00667824\n",
      "Iteration 1907, loss = 0.00667073\n",
      "Iteration 1908, loss = 0.00666324\n",
      "Iteration 1909, loss = 0.00665576\n",
      "Iteration 1910, loss = 0.00664829\n",
      "Iteration 1911, loss = 0.00664083\n",
      "Iteration 1912, loss = 0.00663338\n",
      "Iteration 1913, loss = 0.00662594\n",
      "Iteration 1914, loss = 0.00661852\n",
      "Iteration 1915, loss = 0.00661110\n",
      "Iteration 1916, loss = 0.00660369\n",
      "Iteration 1917, loss = 0.00659630\n",
      "Iteration 1918, loss = 0.00658891\n",
      "Iteration 1919, loss = 0.00658154\n",
      "Iteration 1920, loss = 0.00657417\n",
      "Iteration 1921, loss = 0.00656682\n",
      "Iteration 1922, loss = 0.00655948\n",
      "Iteration 1923, loss = 0.00655214\n",
      "Iteration 1924, loss = 0.00654482\n",
      "Iteration 1925, loss = 0.00653751\n",
      "Iteration 1926, loss = 0.00653021\n",
      "Iteration 1927, loss = 0.00652292\n",
      "Iteration 1928, loss = 0.00651564\n",
      "Iteration 1929, loss = 0.00650837\n",
      "Iteration 1930, loss = 0.00650111\n",
      "Iteration 1931, loss = 0.00649386\n",
      "Iteration 1932, loss = 0.00648662\n",
      "Iteration 1933, loss = 0.00647939\n",
      "Iteration 1934, loss = 0.00647217\n",
      "Iteration 1935, loss = 0.00646496\n",
      "Iteration 1936, loss = 0.00645776\n",
      "Iteration 1937, loss = 0.00645058\n",
      "Iteration 1938, loss = 0.00644340\n",
      "Iteration 1939, loss = 0.00643623\n",
      "Iteration 1940, loss = 0.00642907\n",
      "Iteration 1941, loss = 0.00642193\n",
      "Iteration 1942, loss = 0.00641479\n",
      "Iteration 1943, loss = 0.00640766\n",
      "Iteration 1944, loss = 0.00640055\n",
      "Iteration 1945, loss = 0.00639344\n",
      "Iteration 1946, loss = 0.00638634\n",
      "Iteration 1947, loss = 0.00637926\n",
      "Iteration 1948, loss = 0.00637218\n",
      "Iteration 1949, loss = 0.00636511\n",
      "Iteration 1950, loss = 0.00635806\n",
      "Iteration 1951, loss = 0.00635101\n",
      "Iteration 1952, loss = 0.00634397\n",
      "Iteration 1953, loss = 0.00633695\n",
      "Iteration 1954, loss = 0.00632993\n",
      "Iteration 1955, loss = 0.00632292\n",
      "Iteration 1956, loss = 0.00631593\n",
      "Iteration 1957, loss = 0.00630894\n",
      "Iteration 1958, loss = 0.00630196\n",
      "Iteration 1959, loss = 0.00629500\n",
      "Iteration 1960, loss = 0.00628804\n",
      "Iteration 1961, loss = 0.00628109\n",
      "Iteration 1962, loss = 0.00627415\n",
      "Iteration 1963, loss = 0.00626722\n",
      "Iteration 1964, loss = 0.00626031\n",
      "Iteration 1965, loss = 0.00625340\n",
      "Iteration 1966, loss = 0.00624650\n",
      "Iteration 1967, loss = 0.00623961\n",
      "Iteration 1968, loss = 0.00623273\n",
      "Iteration 1969, loss = 0.00622586\n",
      "Iteration 1970, loss = 0.00621900\n",
      "Iteration 1971, loss = 0.00621215\n",
      "Iteration 1972, loss = 0.00620531\n",
      "Iteration 1973, loss = 0.00619847\n",
      "Iteration 1974, loss = 0.00619165\n",
      "Iteration 1975, loss = 0.00618484\n",
      "Iteration 1976, loss = 0.00617804\n",
      "Iteration 1977, loss = 0.00617124\n",
      "Iteration 1978, loss = 0.00616446\n",
      "Iteration 1979, loss = 0.00615768\n",
      "Iteration 1980, loss = 0.00615092\n",
      "Iteration 1981, loss = 0.00614416\n",
      "Iteration 1982, loss = 0.00613742\n",
      "Iteration 1983, loss = 0.00613068\n",
      "Iteration 1984, loss = 0.00612395\n",
      "Iteration 1985, loss = 0.00611724\n",
      "Iteration 1986, loss = 0.00611053\n",
      "Iteration 1987, loss = 0.00610383\n",
      "Iteration 1988, loss = 0.00609714\n",
      "Iteration 1989, loss = 0.00609046\n",
      "Iteration 1990, loss = 0.00608379\n",
      "Iteration 1991, loss = 0.00607712\n",
      "Iteration 1992, loss = 0.00607047\n",
      "Iteration 1993, loss = 0.00606383\n",
      "Iteration 1994, loss = 0.00605719\n",
      "Iteration 1995, loss = 0.00605057\n",
      "Iteration 1996, loss = 0.00604395\n",
      "Iteration 1997, loss = 0.00603735\n",
      "Iteration 1998, loss = 0.00603075\n",
      "Iteration 1999, loss = 0.00602416\n",
      "Iteration 2000, loss = 0.00601758\n",
      "Iteration 2001, loss = 0.00601101\n",
      "Iteration 2002, loss = 0.00600445\n",
      "Iteration 2003, loss = 0.00599790\n",
      "Iteration 2004, loss = 0.00599136\n",
      "Iteration 2005, loss = 0.00598482\n",
      "Iteration 2006, loss = 0.00597830\n",
      "Iteration 2007, loss = 0.00597178\n",
      "Iteration 2008, loss = 0.00596528\n",
      "Iteration 2009, loss = 0.00595878\n",
      "Iteration 2010, loss = 0.00595229\n",
      "Iteration 2011, loss = 0.00594581\n",
      "Iteration 2012, loss = 0.00593934\n",
      "Iteration 2013, loss = 0.00593288\n",
      "Iteration 2014, loss = 0.00592643\n",
      "Iteration 2015, loss = 0.00591998\n",
      "Iteration 2016, loss = 0.00591355\n",
      "Iteration 2017, loss = 0.00590712\n",
      "Iteration 2018, loss = 0.00590070\n",
      "Iteration 2019, loss = 0.00589430\n",
      "Iteration 2020, loss = 0.00588790\n",
      "Iteration 2021, loss = 0.00588151\n",
      "Iteration 2022, loss = 0.00587512\n",
      "Iteration 2023, loss = 0.00586875\n",
      "Iteration 2024, loss = 0.00586239\n",
      "Iteration 2025, loss = 0.00585603\n",
      "Iteration 2026, loss = 0.00584968\n",
      "Iteration 2027, loss = 0.00584335\n",
      "Iteration 2028, loss = 0.00583702\n",
      "Iteration 2029, loss = 0.00583070\n",
      "Iteration 2030, loss = 0.00582438\n",
      "Iteration 2031, loss = 0.00581808\n",
      "Iteration 2032, loss = 0.00581179\n",
      "Iteration 2033, loss = 0.00580550\n",
      "Iteration 2034, loss = 0.00579922\n",
      "Iteration 2035, loss = 0.00579295\n",
      "Iteration 2036, loss = 0.00578669\n",
      "Iteration 2037, loss = 0.00578044\n",
      "Iteration 2038, loss = 0.00577420\n",
      "Iteration 2039, loss = 0.00576796\n",
      "Iteration 2040, loss = 0.00576174\n",
      "Iteration 2041, loss = 0.00575552\n",
      "Iteration 2042, loss = 0.00574931\n",
      "Iteration 2043, loss = 0.00574311\n",
      "Iteration 2044, loss = 0.00573692\n",
      "Iteration 2045, loss = 0.00573074\n",
      "Iteration 2046, loss = 0.00572456\n",
      "Iteration 2047, loss = 0.00571839\n",
      "Iteration 2048, loss = 0.00571224\n",
      "Iteration 2049, loss = 0.00570609\n",
      "Iteration 2050, loss = 0.00569994\n",
      "Iteration 2051, loss = 0.00569381\n",
      "Iteration 2052, loss = 0.00568769\n",
      "Iteration 2053, loss = 0.00568157\n",
      "Iteration 2054, loss = 0.00567546\n",
      "Iteration 2055, loss = 0.00566936\n",
      "Iteration 2056, loss = 0.00566327\n",
      "Iteration 2057, loss = 0.00565719\n",
      "Iteration 2058, loss = 0.00565111\n",
      "Iteration 2059, loss = 0.00564505\n",
      "Iteration 2060, loss = 0.00563899\n",
      "Iteration 2061, loss = 0.00563294\n",
      "Iteration 2062, loss = 0.00562690\n",
      "Iteration 2063, loss = 0.00562086\n",
      "Iteration 2064, loss = 0.00561484\n",
      "Iteration 2065, loss = 0.00560882\n",
      "Iteration 2066, loss = 0.00560281\n",
      "Iteration 2067, loss = 0.00559681\n",
      "Iteration 2068, loss = 0.00559082\n",
      "Iteration 2069, loss = 0.00558483\n",
      "Iteration 2070, loss = 0.00557885\n",
      "Iteration 2071, loss = 0.00557289\n",
      "Iteration 2072, loss = 0.00556693\n",
      "Iteration 2073, loss = 0.00556097\n",
      "Iteration 2074, loss = 0.00555503\n",
      "Iteration 2075, loss = 0.00554909\n",
      "Iteration 2076, loss = 0.00554316\n",
      "Iteration 2077, loss = 0.00553724\n",
      "Iteration 2078, loss = 0.00553133\n",
      "Iteration 2079, loss = 0.00552543\n",
      "Iteration 2080, loss = 0.00551953\n",
      "Iteration 2081, loss = 0.00551364\n",
      "Iteration 2082, loss = 0.00550776\n",
      "Iteration 2083, loss = 0.00550189\n",
      "Iteration 2084, loss = 0.00549603\n",
      "Iteration 2085, loss = 0.00549017\n",
      "Iteration 2086, loss = 0.00548432\n",
      "Iteration 2087, loss = 0.00547848\n",
      "Iteration 2088, loss = 0.00547265\n",
      "Iteration 2089, loss = 0.00546682\n",
      "Iteration 2090, loss = 0.00546101\n",
      "Iteration 2091, loss = 0.00545520\n",
      "Iteration 2092, loss = 0.00544939\n",
      "Iteration 2093, loss = 0.00544360\n",
      "Iteration 2094, loss = 0.00543781\n",
      "Iteration 2095, loss = 0.00543204\n",
      "Iteration 2096, loss = 0.00542627\n",
      "Iteration 2097, loss = 0.00542050\n",
      "Iteration 2098, loss = 0.00541475\n",
      "Iteration 2099, loss = 0.00540900\n",
      "Iteration 2100, loss = 0.00540326\n",
      "Iteration 2101, loss = 0.00539753\n",
      "Iteration 2102, loss = 0.00539181\n",
      "Iteration 2103, loss = 0.00538609\n",
      "Iteration 2104, loss = 0.00538038\n",
      "Iteration 2105, loss = 0.00537468\n",
      "Iteration 2106, loss = 0.00536899\n",
      "Iteration 2107, loss = 0.00536330\n",
      "Iteration 2108, loss = 0.00535762\n",
      "Iteration 2109, loss = 0.00535195\n",
      "Iteration 2110, loss = 0.00534629\n",
      "Iteration 2111, loss = 0.00534063\n",
      "Iteration 2112, loss = 0.00533498\n",
      "Iteration 2113, loss = 0.00532934\n",
      "Iteration 2114, loss = 0.00532371\n",
      "Iteration 2115, loss = 0.00531809\n",
      "Iteration 2116, loss = 0.00531247\n",
      "Iteration 2117, loss = 0.00530686\n",
      "Iteration 2118, loss = 0.00530126\n",
      "Iteration 2119, loss = 0.00529566\n",
      "Iteration 2120, loss = 0.00529007\n",
      "Iteration 2121, loss = 0.00528449\n",
      "Iteration 2122, loss = 0.00527892\n",
      "Iteration 2123, loss = 0.00527335\n",
      "Iteration 2124, loss = 0.00526780\n",
      "Iteration 2125, loss = 0.00526224\n",
      "Iteration 2126, loss = 0.00525670\n",
      "Iteration 2127, loss = 0.00525117\n",
      "Iteration 2128, loss = 0.00524564\n",
      "Iteration 2129, loss = 0.00524012\n",
      "Iteration 2130, loss = 0.00523460\n",
      "Iteration 2131, loss = 0.00522910\n",
      "Iteration 2132, loss = 0.00522360\n",
      "Iteration 2133, loss = 0.00521810\n",
      "Iteration 2134, loss = 0.00521262\n",
      "Iteration 2135, loss = 0.00520714\n",
      "Iteration 2136, loss = 0.00520167\n",
      "Iteration 2137, loss = 0.00519621\n",
      "Iteration 2138, loss = 0.00519075\n",
      "Iteration 2139, loss = 0.00518531\n",
      "Iteration 2140, loss = 0.00517986\n",
      "Iteration 2141, loss = 0.00517443\n",
      "Iteration 2142, loss = 0.00516900\n",
      "Iteration 2143, loss = 0.00516358\n",
      "Iteration 2144, loss = 0.00515817\n",
      "Iteration 2145, loss = 0.00515277\n",
      "Iteration 2146, loss = 0.00514737\n",
      "Iteration 2147, loss = 0.00514198\n",
      "Iteration 2148, loss = 0.00513659\n",
      "Iteration 2149, loss = 0.00513122\n",
      "Iteration 2150, loss = 0.00512585\n",
      "Iteration 2151, loss = 0.00512049\n",
      "Iteration 2152, loss = 0.00511513\n",
      "Iteration 2153, loss = 0.00510978\n",
      "Iteration 2154, loss = 0.00510444\n",
      "Iteration 2155, loss = 0.00509911\n",
      "Iteration 2156, loss = 0.00509378\n",
      "Iteration 2157, loss = 0.00508846\n",
      "Iteration 2158, loss = 0.00508315\n",
      "Iteration 2159, loss = 0.00507784\n",
      "Iteration 2160, loss = 0.00507254\n",
      "Iteration 2161, loss = 0.00506725\n",
      "Iteration 2162, loss = 0.00506197\n",
      "Iteration 2163, loss = 0.00505669\n",
      "Iteration 2164, loss = 0.00505142\n",
      "Iteration 2165, loss = 0.00504615\n",
      "Iteration 2166, loss = 0.00504089\n",
      "Iteration 2167, loss = 0.00503564\n",
      "Iteration 2168, loss = 0.00503040\n",
      "Iteration 2169, loss = 0.00502516\n",
      "Iteration 2170, loss = 0.00501993\n",
      "Iteration 2171, loss = 0.00501471\n",
      "Iteration 2172, loss = 0.00500950\n",
      "Iteration 2173, loss = 0.00500429\n",
      "Iteration 2174, loss = 0.00499908\n",
      "Iteration 2175, loss = 0.00499389\n",
      "Iteration 2176, loss = 0.00498870\n",
      "Iteration 2177, loss = 0.00498352\n",
      "Iteration 2178, loss = 0.00497834\n",
      "Iteration 2179, loss = 0.00497318\n",
      "Iteration 2180, loss = 0.00496801\n",
      "Iteration 2181, loss = 0.00496286\n",
      "Iteration 2182, loss = 0.00495771\n",
      "Iteration 2183, loss = 0.00495257\n",
      "Iteration 2184, loss = 0.00494744\n",
      "Iteration 2185, loss = 0.00494231\n",
      "Iteration 2186, loss = 0.00493719\n",
      "Iteration 2187, loss = 0.00493207\n",
      "Iteration 2188, loss = 0.00492696\n",
      "Iteration 2189, loss = 0.00492186\n",
      "Iteration 2190, loss = 0.00491677\n",
      "Iteration 2191, loss = 0.00491168\n",
      "Iteration 2192, loss = 0.00490660\n",
      "Iteration 2193, loss = 0.00490153\n",
      "Iteration 2194, loss = 0.00489646\n",
      "Iteration 2195, loss = 0.00489140\n",
      "Iteration 2196, loss = 0.00488634\n",
      "Iteration 2197, loss = 0.00488130\n",
      "Iteration 2198, loss = 0.00487625\n",
      "Iteration 2199, loss = 0.00487122\n",
      "Iteration 2200, loss = 0.00486619\n",
      "Iteration 2201, loss = 0.00486117\n",
      "Iteration 2202, loss = 0.00485615\n",
      "Iteration 2203, loss = 0.00485115\n",
      "Iteration 2204, loss = 0.00484614\n",
      "Iteration 2205, loss = 0.00484115\n",
      "Iteration 2206, loss = 0.00483616\n",
      "Iteration 2207, loss = 0.00483118\n",
      "Iteration 2208, loss = 0.00482620\n",
      "Iteration 2209, loss = 0.00482123\n",
      "Iteration 2210, loss = 0.00481627\n",
      "Iteration 2211, loss = 0.00481131\n",
      "Iteration 2212, loss = 0.00480636\n",
      "Iteration 2213, loss = 0.00480142\n",
      "Iteration 2214, loss = 0.00479648\n",
      "Iteration 2215, loss = 0.00479155\n",
      "Iteration 2216, loss = 0.00478662\n",
      "Iteration 2217, loss = 0.00478170\n",
      "Iteration 2218, loss = 0.00477679\n",
      "Iteration 2219, loss = 0.00477189\n",
      "Iteration 2220, loss = 0.00476699\n",
      "Iteration 2221, loss = 0.00476210\n",
      "Iteration 2222, loss = 0.00475721\n",
      "Iteration 2223, loss = 0.00475233\n",
      "Iteration 2224, loss = 0.00474745\n",
      "Iteration 2225, loss = 0.00474259\n",
      "Iteration 2226, loss = 0.00473773\n",
      "Iteration 2227, loss = 0.00473287\n",
      "Iteration 2228, loss = 0.00472802\n",
      "Iteration 2229, loss = 0.00472318\n",
      "Iteration 2230, loss = 0.00471834\n",
      "Iteration 2231, loss = 0.00471351\n",
      "Iteration 2232, loss = 0.00470869\n",
      "Iteration 2233, loss = 0.00470387\n",
      "Iteration 2234, loss = 0.00469906\n",
      "Iteration 2235, loss = 0.00469426\n",
      "Iteration 2236, loss = 0.00468946\n",
      "Iteration 2237, loss = 0.00468467\n",
      "Iteration 2238, loss = 0.00467988\n",
      "Iteration 2239, loss = 0.00467510\n",
      "Iteration 2240, loss = 0.00467032\n",
      "Iteration 2241, loss = 0.00466556\n",
      "Iteration 2242, loss = 0.00466079\n",
      "Iteration 2243, loss = 0.00465604\n",
      "Iteration 2244, loss = 0.00465129\n",
      "Iteration 2245, loss = 0.00464655\n",
      "Iteration 2246, loss = 0.00464181\n",
      "Iteration 2247, loss = 0.00463708\n",
      "Iteration 2248, loss = 0.00463235\n",
      "Iteration 2249, loss = 0.00462763\n",
      "Iteration 2250, loss = 0.00462292\n",
      "Iteration 2251, loss = 0.00461821\n",
      "Iteration 2252, loss = 0.00461351\n",
      "Iteration 2253, loss = 0.00460882\n",
      "Iteration 2254, loss = 0.00460413\n",
      "Iteration 2255, loss = 0.00459945\n",
      "Iteration 2256, loss = 0.00459477\n",
      "Iteration 2257, loss = 0.00459010\n",
      "Iteration 2258, loss = 0.00458543\n",
      "Iteration 2259, loss = 0.00458077\n",
      "Iteration 2260, loss = 0.00457612\n",
      "Iteration 2261, loss = 0.00457147\n",
      "Iteration 2262, loss = 0.00456683\n",
      "Iteration 2263, loss = 0.00456220\n",
      "Iteration 2264, loss = 0.00455757\n",
      "Iteration 2265, loss = 0.00455295\n",
      "Iteration 2266, loss = 0.00454833\n",
      "Iteration 2267, loss = 0.00454372\n",
      "Iteration 2268, loss = 0.00453911\n",
      "Iteration 2269, loss = 0.00453451\n",
      "Iteration 2270, loss = 0.00452992\n",
      "Iteration 2271, loss = 0.00452533\n",
      "Iteration 2272, loss = 0.00452075\n",
      "Iteration 2273, loss = 0.00451617\n",
      "Iteration 2274, loss = 0.00451160\n",
      "Iteration 2275, loss = 0.00450704\n",
      "Iteration 2276, loss = 0.00450248\n",
      "Iteration 2277, loss = 0.00449793\n",
      "Iteration 2278, loss = 0.00449338\n",
      "Iteration 2279, loss = 0.00448884\n",
      "Iteration 2280, loss = 0.00448430\n",
      "Iteration 2281, loss = 0.00447977\n",
      "Iteration 2282, loss = 0.00447525\n",
      "Iteration 2283, loss = 0.00447073\n",
      "Iteration 2284, loss = 0.00446622\n",
      "Iteration 2285, loss = 0.00446171\n",
      "Iteration 2286, loss = 0.00445721\n",
      "Iteration 2287, loss = 0.00445272\n",
      "Iteration 2288, loss = 0.00444823\n",
      "Iteration 2289, loss = 0.00444375\n",
      "Iteration 2290, loss = 0.00443927\n",
      "Iteration 2291, loss = 0.00443480\n",
      "Iteration 2292, loss = 0.00443033\n",
      "Iteration 2293, loss = 0.00442587\n",
      "Iteration 2294, loss = 0.00442141\n",
      "Iteration 2295, loss = 0.00441696\n",
      "Iteration 2296, loss = 0.00441252\n",
      "Iteration 2297, loss = 0.00440808\n",
      "Iteration 2298, loss = 0.00440365\n",
      "Iteration 2299, loss = 0.00439922\n",
      "Iteration 2300, loss = 0.00439480\n",
      "Iteration 2301, loss = 0.00439038\n",
      "Iteration 2302, loss = 0.00438597\n",
      "Iteration 2303, loss = 0.00438157\n",
      "Iteration 2304, loss = 0.00437717\n",
      "Iteration 2305, loss = 0.00437278\n",
      "Iteration 2306, loss = 0.00436839\n",
      "Iteration 2307, loss = 0.00436401\n",
      "Iteration 2308, loss = 0.00435963\n",
      "Iteration 2309, loss = 0.00435526\n",
      "Iteration 2310, loss = 0.00435089\n",
      "Iteration 2311, loss = 0.00434653\n",
      "Iteration 2312, loss = 0.00434218\n",
      "Iteration 2313, loss = 0.00433783\n",
      "Iteration 2314, loss = 0.00433349\n",
      "Iteration 2315, loss = 0.00432915\n",
      "Iteration 2316, loss = 0.00432482\n",
      "Iteration 2317, loss = 0.00432049\n",
      "Iteration 2318, loss = 0.00431617\n",
      "Iteration 2319, loss = 0.00431185\n",
      "Iteration 2320, loss = 0.00430754\n",
      "Iteration 2321, loss = 0.00430323\n",
      "Iteration 2322, loss = 0.00429893\n",
      "Iteration 2323, loss = 0.00429464\n",
      "Iteration 2324, loss = 0.00429035\n",
      "Iteration 2325, loss = 0.00428607\n",
      "Iteration 2326, loss = 0.00428179\n",
      "Iteration 2327, loss = 0.00427752\n",
      "Iteration 2328, loss = 0.00427325\n",
      "Iteration 2329, loss = 0.00426899\n",
      "Iteration 2330, loss = 0.00426473\n",
      "Iteration 2331, loss = 0.00426048\n",
      "Iteration 2332, loss = 0.00425623\n",
      "Iteration 2333, loss = 0.00425199\n",
      "Iteration 2334, loss = 0.00424776\n",
      "Iteration 2335, loss = 0.00424353\n",
      "Iteration 2336, loss = 0.00423930\n",
      "Iteration 2337, loss = 0.00423508\n",
      "Iteration 2338, loss = 0.00423087\n",
      "Iteration 2339, loss = 0.00422666\n",
      "Iteration 2340, loss = 0.00422245\n",
      "Iteration 2341, loss = 0.00421826\n",
      "Iteration 2342, loss = 0.00421406\n",
      "Iteration 2343, loss = 0.00420988\n",
      "Iteration 2344, loss = 0.00420569\n",
      "Iteration 2345, loss = 0.00420152\n",
      "Iteration 2346, loss = 0.00419734\n",
      "Iteration 2347, loss = 0.00419318\n",
      "Iteration 2348, loss = 0.00418902\n",
      "Iteration 2349, loss = 0.00418486\n",
      "Iteration 2350, loss = 0.00418071\n",
      "Iteration 2351, loss = 0.00417656\n",
      "Iteration 2352, loss = 0.00417242\n",
      "Iteration 2353, loss = 0.00416829\n",
      "Iteration 2354, loss = 0.00416416\n",
      "Iteration 2355, loss = 0.00416003\n",
      "Iteration 2356, loss = 0.00415591\n",
      "Iteration 2357, loss = 0.00415180\n",
      "Iteration 2358, loss = 0.00414769\n",
      "Iteration 2359, loss = 0.00414358\n",
      "Iteration 2360, loss = 0.00413948\n",
      "Iteration 2361, loss = 0.00413539\n",
      "Iteration 2362, loss = 0.00413130\n",
      "Iteration 2363, loss = 0.00412721\n",
      "Iteration 2364, loss = 0.00412313\n",
      "Iteration 2365, loss = 0.00411906\n",
      "Iteration 2366, loss = 0.00411499\n",
      "Iteration 2367, loss = 0.00411093\n",
      "Iteration 2368, loss = 0.00410687\n",
      "Iteration 2369, loss = 0.00410282\n",
      "Iteration 2370, loss = 0.00409877\n",
      "Iteration 2371, loss = 0.00409472\n",
      "Iteration 2372, loss = 0.00409068\n",
      "Iteration 2373, loss = 0.00408665\n",
      "Iteration 2374, loss = 0.00408262\n",
      "Iteration 2375, loss = 0.00407860\n",
      "Iteration 2376, loss = 0.00407458\n",
      "Iteration 2377, loss = 0.00407057\n",
      "Iteration 2378, loss = 0.00406656\n",
      "Iteration 2379, loss = 0.00406256\n",
      "Iteration 2380, loss = 0.00405856\n",
      "Iteration 2381, loss = 0.00405456\n",
      "Iteration 2382, loss = 0.00405057\n",
      "Iteration 2383, loss = 0.00404659\n",
      "Iteration 2384, loss = 0.00404261\n",
      "Iteration 2385, loss = 0.00403864\n",
      "Iteration 2386, loss = 0.00403467\n",
      "Iteration 2387, loss = 0.00403071\n",
      "Iteration 2388, loss = 0.00402675\n",
      "Iteration 2389, loss = 0.00402279\n",
      "Iteration 2390, loss = 0.00401884\n",
      "Iteration 2391, loss = 0.00401490\n",
      "Iteration 2392, loss = 0.00401096\n",
      "Iteration 2393, loss = 0.00400703\n",
      "Iteration 2394, loss = 0.00400310\n",
      "Iteration 2395, loss = 0.00399917\n",
      "Iteration 2396, loss = 0.00399525\n",
      "Iteration 2397, loss = 0.00399134\n",
      "Iteration 2398, loss = 0.00398743\n",
      "Iteration 2399, loss = 0.00398352\n",
      "Iteration 2400, loss = 0.00397962\n",
      "Iteration 2401, loss = 0.00397572\n",
      "Iteration 2402, loss = 0.00397183\n",
      "Iteration 2403, loss = 0.00396795\n",
      "Iteration 2404, loss = 0.00396407\n",
      "Iteration 2405, loss = 0.00396019\n",
      "Iteration 2406, loss = 0.00395632\n",
      "Iteration 2407, loss = 0.00395245\n",
      "Iteration 2408, loss = 0.00394859\n",
      "Iteration 2409, loss = 0.00394473\n",
      "Iteration 2410, loss = 0.00394088\n",
      "Iteration 2411, loss = 0.00393703\n",
      "Iteration 2412, loss = 0.00393319\n",
      "Iteration 2413, loss = 0.00392935\n",
      "Iteration 2414, loss = 0.00392552\n",
      "Iteration 2415, loss = 0.00392169\n",
      "Iteration 2416, loss = 0.00391786\n",
      "Iteration 2417, loss = 0.00391404\n",
      "Iteration 2418, loss = 0.00391023\n",
      "Iteration 2419, loss = 0.00390642\n",
      "Iteration 2420, loss = 0.00390261\n",
      "Iteration 2421, loss = 0.00389881\n",
      "Iteration 2422, loss = 0.00389502\n",
      "Iteration 2423, loss = 0.00389123\n",
      "Iteration 2424, loss = 0.00388744\n",
      "Iteration 2425, loss = 0.00388366\n",
      "Iteration 2426, loss = 0.00387988\n",
      "Iteration 2427, loss = 0.00387611\n",
      "Iteration 2428, loss = 0.00387234\n",
      "Iteration 2429, loss = 0.00386858\n",
      "Iteration 2430, loss = 0.00386482\n",
      "Iteration 2431, loss = 0.00386106\n",
      "Iteration 2432, loss = 0.00385731\n",
      "Iteration 2433, loss = 0.00385357\n",
      "Iteration 2434, loss = 0.00384983\n",
      "Iteration 2435, loss = 0.00384609\n",
      "Iteration 2436, loss = 0.00384236\n",
      "Iteration 2437, loss = 0.00383863\n",
      "Iteration 2438, loss = 0.00383491\n",
      "Iteration 2439, loss = 0.00383119\n",
      "Iteration 2440, loss = 0.00382748\n",
      "Iteration 2441, loss = 0.00382377\n",
      "Iteration 2442, loss = 0.00382007\n",
      "Iteration 2443, loss = 0.00381637\n",
      "Iteration 2444, loss = 0.00381268\n",
      "Iteration 2445, loss = 0.00380899\n",
      "Iteration 2446, loss = 0.00380530\n",
      "Iteration 2447, loss = 0.00380162\n",
      "Iteration 2448, loss = 0.00379794\n",
      "Iteration 2449, loss = 0.00379427\n",
      "Iteration 2450, loss = 0.00379060\n",
      "Iteration 2451, loss = 0.00378694\n",
      "Iteration 2452, loss = 0.00378328\n",
      "Iteration 2453, loss = 0.00377963\n",
      "Iteration 2454, loss = 0.00377598\n",
      "Iteration 2455, loss = 0.00377233\n",
      "Iteration 2456, loss = 0.00376869\n",
      "Iteration 2457, loss = 0.00376505\n",
      "Iteration 2458, loss = 0.00376142\n",
      "Iteration 2459, loss = 0.00375779\n",
      "Iteration 2460, loss = 0.00375417\n",
      "Iteration 2461, loss = 0.00375055\n",
      "Iteration 2462, loss = 0.00374694\n",
      "Iteration 2463, loss = 0.00374333\n",
      "Iteration 2464, loss = 0.00373972\n",
      "Iteration 2465, loss = 0.00373612\n",
      "Iteration 2466, loss = 0.00373252\n",
      "Iteration 2467, loss = 0.00372893\n",
      "Iteration 2468, loss = 0.00372534\n",
      "Iteration 2469, loss = 0.00372176\n",
      "Iteration 2470, loss = 0.00371818\n",
      "Iteration 2471, loss = 0.00371460\n",
      "Iteration 2472, loss = 0.00371103\n",
      "Iteration 2473, loss = 0.00370747\n",
      "Iteration 2474, loss = 0.00370391\n",
      "Iteration 2475, loss = 0.00370035\n",
      "Iteration 2476, loss = 0.00369679\n",
      "Iteration 2477, loss = 0.00369325\n",
      "Iteration 2478, loss = 0.00368970\n",
      "Iteration 2479, loss = 0.00368616\n",
      "Iteration 2480, loss = 0.00368262\n",
      "Iteration 2481, loss = 0.00367909\n",
      "Iteration 2482, loss = 0.00367556\n",
      "Iteration 2483, loss = 0.00367204\n",
      "Iteration 2484, loss = 0.00366852\n",
      "Iteration 2485, loss = 0.00366501\n",
      "Iteration 2486, loss = 0.00366150\n",
      "Iteration 2487, loss = 0.00365799\n",
      "Iteration 2488, loss = 0.00365449\n",
      "Iteration 2489, loss = 0.00365099\n",
      "Iteration 2490, loss = 0.00364750\n",
      "Iteration 2491, loss = 0.00364401\n",
      "Iteration 2492, loss = 0.00364052\n",
      "Iteration 2493, loss = 0.00363704\n",
      "Iteration 2494, loss = 0.00363357\n",
      "Iteration 2495, loss = 0.00363009\n",
      "Iteration 2496, loss = 0.00362663\n",
      "Iteration 2497, loss = 0.00362316\n",
      "Iteration 2498, loss = 0.00361970\n",
      "Iteration 2499, loss = 0.00361625\n",
      "Iteration 2500, loss = 0.00361280\n",
      "Iteration 2501, loss = 0.00360935\n",
      "Iteration 2502, loss = 0.00360591\n",
      "Iteration 2503, loss = 0.00360247\n",
      "Iteration 2504, loss = 0.00359903\n",
      "Iteration 2505, loss = 0.00359560\n",
      "Iteration 2506, loss = 0.00359217\n",
      "Iteration 2507, loss = 0.00358875\n",
      "Iteration 2508, loss = 0.00358533\n",
      "Iteration 2509, loss = 0.00358192\n",
      "Iteration 2510, loss = 0.00357851\n",
      "Iteration 2511, loss = 0.00357510\n",
      "Iteration 2512, loss = 0.00357170\n",
      "Iteration 2513, loss = 0.00356830\n",
      "Iteration 2514, loss = 0.00356491\n",
      "Iteration 2515, loss = 0.00356152\n",
      "Iteration 2516, loss = 0.00355814\n",
      "Iteration 2517, loss = 0.00355475\n",
      "Iteration 2518, loss = 0.00355138\n",
      "Iteration 2519, loss = 0.00354800\n",
      "Iteration 2520, loss = 0.00354463\n",
      "Iteration 2521, loss = 0.00354127\n",
      "Iteration 2522, loss = 0.00353791\n",
      "Iteration 2523, loss = 0.00353455\n",
      "Iteration 2524, loss = 0.00353120\n",
      "Iteration 2525, loss = 0.00352785\n",
      "Iteration 2526, loss = 0.00352450\n",
      "Iteration 2527, loss = 0.00352116\n",
      "Iteration 2528, loss = 0.00351783\n",
      "Iteration 2529, loss = 0.00351449\n",
      "Iteration 2530, loss = 0.00351116\n",
      "Iteration 2531, loss = 0.00350784\n",
      "Iteration 2532, loss = 0.00350452\n",
      "Iteration 2533, loss = 0.00350120\n",
      "Iteration 2534, loss = 0.00349789\n",
      "Iteration 2535, loss = 0.00349458\n",
      "Iteration 2536, loss = 0.00349127\n",
      "Iteration 2537, loss = 0.00348797\n",
      "Iteration 2538, loss = 0.00348468\n",
      "Iteration 2539, loss = 0.00348138\n",
      "Iteration 2540, loss = 0.00347809\n",
      "Iteration 2541, loss = 0.00347481\n",
      "Iteration 2542, loss = 0.00347153\n",
      "Iteration 2543, loss = 0.00346825\n",
      "Iteration 2544, loss = 0.00346498\n",
      "Iteration 2545, loss = 0.00346171\n",
      "Iteration 2546, loss = 0.00345844\n",
      "Iteration 2547, loss = 0.00345518\n",
      "Iteration 2548, loss = 0.00345192\n",
      "Iteration 2549, loss = 0.00344867\n",
      "Iteration 2550, loss = 0.00344542\n",
      "Iteration 2551, loss = 0.00344217\n",
      "Iteration 2552, loss = 0.00343893\n",
      "Iteration 2553, loss = 0.00343569\n",
      "Iteration 2554, loss = 0.00343245\n",
      "Iteration 2555, loss = 0.00342922\n",
      "Iteration 2556, loss = 0.00342600\n",
      "Iteration 2557, loss = 0.00342277\n",
      "Iteration 2558, loss = 0.00341955\n",
      "Iteration 2559, loss = 0.00341634\n",
      "Iteration 2560, loss = 0.00341313\n",
      "Iteration 2561, loss = 0.00340992\n",
      "Iteration 2562, loss = 0.00340671\n",
      "Iteration 2563, loss = 0.00340351\n",
      "Iteration 2564, loss = 0.00340032\n",
      "Iteration 2565, loss = 0.00339713\n",
      "Iteration 2566, loss = 0.00339394\n",
      "Iteration 2567, loss = 0.00339075\n",
      "Iteration 2568, loss = 0.00338757\n",
      "Iteration 2569, loss = 0.00338439\n",
      "Iteration 2570, loss = 0.00338122\n",
      "Iteration 2571, loss = 0.00337805\n",
      "Iteration 2572, loss = 0.00337488\n",
      "Iteration 2573, loss = 0.00337172\n",
      "Iteration 2574, loss = 0.00336856\n",
      "Iteration 2575, loss = 0.00336541\n",
      "Iteration 2576, loss = 0.00336226\n",
      "Iteration 2577, loss = 0.00335911\n",
      "Iteration 2578, loss = 0.00335597\n",
      "Iteration 2579, loss = 0.00335283\n",
      "Iteration 2580, loss = 0.00334969\n",
      "Iteration 2581, loss = 0.00334656\n",
      "Iteration 2582, loss = 0.00334343\n",
      "Iteration 2583, loss = 0.00334030\n",
      "Iteration 2584, loss = 0.00333718\n",
      "Iteration 2585, loss = 0.00333406\n",
      "Iteration 2586, loss = 0.00333095\n",
      "Iteration 2587, loss = 0.00332784\n",
      "Iteration 2588, loss = 0.00332473\n",
      "Iteration 2589, loss = 0.00332163\n",
      "Iteration 2590, loss = 0.00331853\n",
      "Iteration 2591, loss = 0.00331544\n",
      "Iteration 2592, loss = 0.00331234\n",
      "Iteration 2593, loss = 0.00330926\n",
      "Iteration 2594, loss = 0.00330617\n",
      "Iteration 2595, loss = 0.00330309\n",
      "Iteration 2596, loss = 0.00330001\n",
      "Iteration 2597, loss = 0.00329694\n",
      "Iteration 2598, loss = 0.00329387\n",
      "Iteration 2599, loss = 0.00329080\n",
      "Iteration 2600, loss = 0.00328774\n",
      "Iteration 2601, loss = 0.00328468\n",
      "Iteration 2602, loss = 0.00328162\n",
      "Iteration 2603, loss = 0.00327857\n",
      "Iteration 2604, loss = 0.00327552\n",
      "Iteration 2605, loss = 0.00327248\n",
      "Iteration 2606, loss = 0.00326944\n",
      "Iteration 2607, loss = 0.00326640\n",
      "Iteration 2608, loss = 0.00326337\n",
      "Iteration 2609, loss = 0.00326034\n",
      "Iteration 2610, loss = 0.00325731\n",
      "Iteration 2611, loss = 0.00325429\n",
      "Iteration 2612, loss = 0.00325127\n",
      "Iteration 2613, loss = 0.00324825\n",
      "Iteration 2614, loss = 0.00324524\n",
      "Iteration 2615, loss = 0.00324223\n",
      "Iteration 2616, loss = 0.00323922\n",
      "Iteration 2617, loss = 0.00323622\n",
      "Iteration 2618, loss = 0.00323322\n",
      "Iteration 2619, loss = 0.00323023\n",
      "Iteration 2620, loss = 0.00322723\n",
      "Iteration 2621, loss = 0.00322425\n",
      "Iteration 2622, loss = 0.00322126\n",
      "Iteration 2623, loss = 0.00321828\n",
      "Iteration 2624, loss = 0.00321530\n",
      "Iteration 2625, loss = 0.00321233\n",
      "Iteration 2626, loss = 0.00320936\n",
      "Iteration 2627, loss = 0.00320639\n",
      "Iteration 2628, loss = 0.00320343\n",
      "Iteration 2629, loss = 0.00320047\n",
      "Iteration 2630, loss = 0.00319751\n",
      "Iteration 2631, loss = 0.00319456\n",
      "Iteration 2632, loss = 0.00319161\n",
      "Iteration 2633, loss = 0.00318866\n",
      "Iteration 2634, loss = 0.00318572\n",
      "Iteration 2635, loss = 0.00318278\n",
      "Iteration 2636, loss = 0.00317984\n",
      "Iteration 2637, loss = 0.00317691\n",
      "Iteration 2638, loss = 0.00317398\n",
      "Iteration 2639, loss = 0.00317106\n",
      "Iteration 2640, loss = 0.00316814\n",
      "Iteration 2641, loss = 0.00316522\n",
      "Iteration 2642, loss = 0.00316230\n",
      "Iteration 2643, loss = 0.00315939\n",
      "Iteration 2644, loss = 0.00315648\n",
      "Iteration 2645, loss = 0.00315358\n",
      "Iteration 2646, loss = 0.00315067\n",
      "Iteration 2647, loss = 0.00314778\n",
      "Iteration 2648, loss = 0.00314488\n",
      "Iteration 2649, loss = 0.00314199\n",
      "Iteration 2650, loss = 0.00313910\n",
      "Iteration 2651, loss = 0.00313622\n",
      "Iteration 2652, loss = 0.00313333\n",
      "Iteration 2653, loss = 0.00313046\n",
      "Iteration 2654, loss = 0.00312758\n",
      "Iteration 2655, loss = 0.00312471\n",
      "Iteration 2656, loss = 0.00312184\n",
      "Iteration 2657, loss = 0.00311898\n",
      "Iteration 2658, loss = 0.00311612\n",
      "Iteration 2659, loss = 0.00311326\n",
      "Iteration 2660, loss = 0.00311040\n",
      "Iteration 2661, loss = 0.00310755\n",
      "Iteration 2662, loss = 0.00310470\n",
      "Iteration 2663, loss = 0.00310186\n",
      "Iteration 2664, loss = 0.00309902\n",
      "Iteration 2665, loss = 0.00309618\n",
      "Iteration 2666, loss = 0.00309334\n",
      "Iteration 2667, loss = 0.00309051\n",
      "Iteration 2668, loss = 0.00308768\n",
      "Iteration 2669, loss = 0.00308486\n",
      "Iteration 2670, loss = 0.00308204\n",
      "Iteration 2671, loss = 0.00307922\n",
      "Iteration 2672, loss = 0.00307640\n",
      "Iteration 2673, loss = 0.00307359\n",
      "Iteration 2674, loss = 0.00307078\n",
      "Iteration 2675, loss = 0.00306798\n",
      "Iteration 2676, loss = 0.00306517\n",
      "Iteration 2677, loss = 0.00306237\n",
      "Iteration 2678, loss = 0.00305958\n",
      "Iteration 2679, loss = 0.00305679\n",
      "Iteration 2680, loss = 0.00305400\n",
      "Iteration 2681, loss = 0.00305121\n",
      "Iteration 2682, loss = 0.00304843\n",
      "Iteration 2683, loss = 0.00304565\n",
      "Iteration 2684, loss = 0.00304287\n",
      "Iteration 2685, loss = 0.00304010\n",
      "Iteration 2686, loss = 0.00303733\n",
      "Iteration 2687, loss = 0.00303456\n",
      "Iteration 2688, loss = 0.00303180\n",
      "Iteration 2689, loss = 0.00302904\n",
      "Iteration 2690, loss = 0.00302628\n",
      "Iteration 2691, loss = 0.00302353\n",
      "Iteration 2692, loss = 0.00302078\n",
      "Iteration 2693, loss = 0.00301803\n",
      "Iteration 2694, loss = 0.00301528\n",
      "Iteration 2695, loss = 0.00301254\n",
      "Iteration 2696, loss = 0.00300981\n",
      "Iteration 2697, loss = 0.00300707\n",
      "Iteration 2698, loss = 0.00300434\n",
      "Iteration 2699, loss = 0.00300161\n",
      "Iteration 2700, loss = 0.00299888\n",
      "Iteration 2701, loss = 0.00299616\n",
      "Iteration 2702, loss = 0.00299344\n",
      "Iteration 2703, loss = 0.00299073\n",
      "Iteration 2704, loss = 0.00298801\n",
      "Iteration 2705, loss = 0.00298530\n",
      "Iteration 2706, loss = 0.00298260\n",
      "Iteration 2707, loss = 0.00297989\n",
      "Iteration 2708, loss = 0.00297719\n",
      "Iteration 2709, loss = 0.00297450\n",
      "Iteration 2710, loss = 0.00297180\n",
      "Iteration 2711, loss = 0.00296911\n",
      "Iteration 2712, loss = 0.00296642\n",
      "Iteration 2713, loss = 0.00296374\n",
      "Iteration 2714, loss = 0.00296106\n",
      "Iteration 2715, loss = 0.00295838\n",
      "Iteration 2716, loss = 0.00295570\n",
      "Iteration 2717, loss = 0.00295303\n",
      "Iteration 2718, loss = 0.00295036\n",
      "Iteration 2719, loss = 0.00294769\n",
      "Iteration 2720, loss = 0.00294503\n",
      "Iteration 2721, loss = 0.00294237\n",
      "Iteration 2722, loss = 0.00293971\n",
      "Iteration 2723, loss = 0.00293706\n",
      "Iteration 2724, loss = 0.00293440\n",
      "Iteration 2725, loss = 0.00293176\n",
      "Iteration 2726, loss = 0.00292911\n",
      "Iteration 2727, loss = 0.00292647\n",
      "Iteration 2728, loss = 0.00292383\n",
      "Iteration 2729, loss = 0.00292119\n",
      "Iteration 2730, loss = 0.00291856\n",
      "Iteration 2731, loss = 0.00291593\n",
      "Iteration 2732, loss = 0.00291330\n",
      "Iteration 2733, loss = 0.00291068\n",
      "Iteration 2734, loss = 0.00290806\n",
      "Iteration 2735, loss = 0.00290544\n",
      "Iteration 2736, loss = 0.00290282\n",
      "Iteration 2737, loss = 0.00290021\n",
      "Iteration 2738, loss = 0.00289760\n",
      "Iteration 2739, loss = 0.00289500\n",
      "Iteration 2740, loss = 0.00289239\n",
      "Iteration 2741, loss = 0.00288979\n",
      "Iteration 2742, loss = 0.00288720\n",
      "Iteration 2743, loss = 0.00288460\n",
      "Iteration 2744, loss = 0.00288201\n",
      "Iteration 2745, loss = 0.00287942\n",
      "Iteration 2746, loss = 0.00287684\n",
      "Iteration 2747, loss = 0.00287425\n",
      "Iteration 2748, loss = 0.00287167\n",
      "Iteration 2749, loss = 0.00286910\n",
      "Iteration 2750, loss = 0.00286652\n",
      "Iteration 2751, loss = 0.00286395\n",
      "Iteration 2752, loss = 0.00286139\n",
      "Iteration 2753, loss = 0.00285882\n",
      "Iteration 2754, loss = 0.00285626\n",
      "Iteration 2755, loss = 0.00285370\n",
      "Iteration 2756, loss = 0.00285114\n",
      "Iteration 2757, loss = 0.00284859\n",
      "Iteration 2758, loss = 0.00284604\n",
      "Iteration 2759, loss = 0.00284349\n",
      "Iteration 2760, loss = 0.00284095\n",
      "Iteration 2761, loss = 0.00283841\n",
      "Iteration 2762, loss = 0.00283587\n",
      "Iteration 2763, loss = 0.00283333\n",
      "Iteration 2764, loss = 0.00283080\n",
      "Iteration 2765, loss = 0.00282827\n",
      "Iteration 2766, loss = 0.00282574\n",
      "Iteration 2767, loss = 0.00282322\n",
      "Iteration 2768, loss = 0.00282070\n",
      "Iteration 2769, loss = 0.00281818\n",
      "Iteration 2770, loss = 0.00281566\n",
      "Iteration 2771, loss = 0.00281315\n",
      "Iteration 2772, loss = 0.00281064\n",
      "Iteration 2773, loss = 0.00280813\n",
      "Iteration 2774, loss = 0.00280563\n",
      "Iteration 2775, loss = 0.00280313\n",
      "Iteration 2776, loss = 0.00280063\n",
      "Iteration 2777, loss = 0.00279813\n",
      "Iteration 2778, loss = 0.00279564\n",
      "Iteration 2779, loss = 0.00279315\n",
      "Iteration 2780, loss = 0.00279066\n",
      "Iteration 2781, loss = 0.00278818\n",
      "Iteration 2782, loss = 0.00278569\n",
      "Iteration 2783, loss = 0.00278321\n",
      "Iteration 2784, loss = 0.00278074\n",
      "Iteration 2785, loss = 0.00277826\n",
      "Iteration 2786, loss = 0.00277579\n",
      "Iteration 2787, loss = 0.00277333\n",
      "Iteration 2788, loss = 0.00277086\n",
      "Iteration 2789, loss = 0.00276840\n",
      "Iteration 2790, loss = 0.00276594\n",
      "Iteration 2791, loss = 0.00276348\n",
      "Iteration 2792, loss = 0.00276103\n",
      "Iteration 2793, loss = 0.00275858\n",
      "Iteration 2794, loss = 0.00275613\n",
      "Iteration 2795, loss = 0.00275368\n",
      "Iteration 2796, loss = 0.00275124\n",
      "Iteration 2797, loss = 0.00274880\n",
      "Iteration 2798, loss = 0.00274636\n",
      "Iteration 2799, loss = 0.00274393\n",
      "Iteration 2800, loss = 0.00274150\n",
      "Iteration 2801, loss = 0.00273907\n",
      "Iteration 2802, loss = 0.00273664\n",
      "Iteration 2803, loss = 0.00273422\n",
      "Iteration 2804, loss = 0.00273180\n",
      "Iteration 2805, loss = 0.00272938\n",
      "Iteration 2806, loss = 0.00272696\n",
      "Iteration 2807, loss = 0.00272455\n",
      "Iteration 2808, loss = 0.00272214\n",
      "Iteration 2809, loss = 0.00271973\n",
      "Iteration 2810, loss = 0.00271733\n",
      "Iteration 2811, loss = 0.00271493\n",
      "Iteration 2812, loss = 0.00271253\n",
      "Iteration 2813, loss = 0.00271013\n",
      "Iteration 2814, loss = 0.00270774\n",
      "Iteration 2815, loss = 0.00270535\n",
      "Iteration 2816, loss = 0.00270296\n",
      "Iteration 2817, loss = 0.00270057\n",
      "Iteration 2818, loss = 0.00269819\n",
      "Iteration 2819, loss = 0.00269581\n",
      "Iteration 2820, loss = 0.00269343\n",
      "Iteration 2821, loss = 0.00269105\n",
      "Iteration 2822, loss = 0.00268868\n",
      "Iteration 2823, loss = 0.00268631\n",
      "Iteration 2824, loss = 0.00268394\n",
      "Iteration 2825, loss = 0.00268158\n",
      "Iteration 2826, loss = 0.00267922\n",
      "Iteration 2827, loss = 0.00267686\n",
      "Iteration 2828, loss = 0.00267450\n",
      "Iteration 2829, loss = 0.00267215\n",
      "Iteration 2830, loss = 0.00266980\n",
      "Iteration 2831, loss = 0.00266745\n",
      "Iteration 2832, loss = 0.00266510\n",
      "Iteration 2833, loss = 0.00266276\n",
      "Iteration 2834, loss = 0.00266042\n",
      "Iteration 2835, loss = 0.00265808\n",
      "Iteration 2836, loss = 0.00265574\n",
      "Iteration 2837, loss = 0.00265341\n",
      "Iteration 2838, loss = 0.00265108\n",
      "Iteration 2839, loss = 0.00264875\n",
      "Iteration 2840, loss = 0.00264643\n",
      "Iteration 2841, loss = 0.00264410\n",
      "Iteration 2842, loss = 0.00264178\n",
      "Iteration 2843, loss = 0.00263947\n",
      "Iteration 2844, loss = 0.00263715\n",
      "Iteration 2845, loss = 0.00263484\n",
      "Iteration 2846, loss = 0.00263253\n",
      "Iteration 2847, loss = 0.00263022\n",
      "Iteration 2848, loss = 0.00262792\n",
      "Iteration 2849, loss = 0.00262562\n",
      "Iteration 2850, loss = 0.00262332\n",
      "Iteration 2851, loss = 0.00262102\n",
      "Iteration 2852, loss = 0.00261873\n",
      "Iteration 2853, loss = 0.00261643\n",
      "Iteration 2854, loss = 0.00261415\n",
      "Iteration 2855, loss = 0.00261186\n",
      "Iteration 2856, loss = 0.00260957\n",
      "Iteration 2857, loss = 0.00260729\n",
      "Iteration 2858, loss = 0.00260501\n",
      "Iteration 2859, loss = 0.00260274\n",
      "Iteration 2860, loss = 0.00260046\n",
      "Iteration 2861, loss = 0.00259819\n",
      "Iteration 2862, loss = 0.00259592\n",
      "Iteration 2863, loss = 0.00259366\n",
      "Iteration 2864, loss = 0.00259139\n",
      "Iteration 2865, loss = 0.00258913\n",
      "Iteration 2866, loss = 0.00258687\n",
      "Iteration 2867, loss = 0.00258462\n",
      "Iteration 2868, loss = 0.00258236\n",
      "Iteration 2869, loss = 0.00258011\n",
      "Iteration 2870, loss = 0.00257786\n",
      "Iteration 2871, loss = 0.00257562\n",
      "Iteration 2872, loss = 0.00257337\n",
      "Iteration 2873, loss = 0.00257113\n",
      "Iteration 2874, loss = 0.00256889\n",
      "Iteration 2875, loss = 0.00256665\n",
      "Iteration 2876, loss = 0.00256442\n",
      "Iteration 2877, loss = 0.00256219\n",
      "Iteration 2878, loss = 0.00255996\n",
      "Iteration 2879, loss = 0.00255773\n",
      "Iteration 2880, loss = 0.00255551\n",
      "Iteration 2881, loss = 0.00255329\n",
      "Iteration 2882, loss = 0.00255107\n",
      "Iteration 2883, loss = 0.00254885\n",
      "Iteration 2884, loss = 0.00254664\n",
      "Iteration 2885, loss = 0.00254443\n",
      "Iteration 2886, loss = 0.00254222\n",
      "Iteration 2887, loss = 0.00254001\n",
      "Iteration 2888, loss = 0.00253780\n",
      "Iteration 2889, loss = 0.00253560\n",
      "Iteration 2890, loss = 0.00253340\n",
      "Iteration 2891, loss = 0.00253121\n",
      "Iteration 2892, loss = 0.00252901\n",
      "Iteration 2893, loss = 0.00252682\n",
      "Iteration 2894, loss = 0.00252463\n",
      "Iteration 2895, loss = 0.00252244\n",
      "Iteration 2896, loss = 0.00252026\n",
      "Iteration 2897, loss = 0.00251807\n",
      "Iteration 2898, loss = 0.00251589\n",
      "Iteration 2899, loss = 0.00251371\n",
      "Iteration 2900, loss = 0.00251154\n",
      "Iteration 2901, loss = 0.00250937\n",
      "Iteration 2902, loss = 0.00250719\n",
      "Iteration 2903, loss = 0.00250503\n",
      "Iteration 2904, loss = 0.00250286\n",
      "Iteration 2905, loss = 0.00250070\n",
      "Iteration 2906, loss = 0.00249853\n",
      "Iteration 2907, loss = 0.00249638\n",
      "Iteration 2908, loss = 0.00249422\n",
      "Iteration 2909, loss = 0.00249207\n",
      "Iteration 2910, loss = 0.00248991\n",
      "Iteration 2911, loss = 0.00248776\n",
      "Iteration 2912, loss = 0.00248562\n",
      "Iteration 2913, loss = 0.00248347\n",
      "Iteration 2914, loss = 0.00248133\n",
      "Iteration 2915, loss = 0.00247919\n",
      "Iteration 2916, loss = 0.00247705\n",
      "Iteration 2917, loss = 0.00247492\n",
      "Iteration 2918, loss = 0.00247278\n",
      "Iteration 2919, loss = 0.00247065\n",
      "Iteration 2920, loss = 0.00246852\n",
      "Iteration 2921, loss = 0.00246640\n",
      "Iteration 2922, loss = 0.00246427\n",
      "Iteration 2923, loss = 0.00246215\n",
      "Iteration 2924, loss = 0.00246003\n",
      "Iteration 2925, loss = 0.00245792\n",
      "Iteration 2926, loss = 0.00245580\n",
      "Iteration 2927, loss = 0.00245369\n",
      "Iteration 2928, loss = 0.00245158\n",
      "Iteration 2929, loss = 0.00244947\n",
      "Iteration 2930, loss = 0.00244737\n",
      "Iteration 2931, loss = 0.00244527\n",
      "Iteration 2932, loss = 0.00244317\n",
      "Iteration 2933, loss = 0.00244107\n",
      "Iteration 2934, loss = 0.00243897\n",
      "Iteration 2935, loss = 0.00243688\n",
      "Iteration 2936, loss = 0.00243479\n",
      "Iteration 2937, loss = 0.00243270\n",
      "Iteration 2938, loss = 0.00243061\n",
      "Iteration 2939, loss = 0.00242853\n",
      "Iteration 2940, loss = 0.00242644\n",
      "Iteration 2941, loss = 0.00242436\n",
      "Iteration 2942, loss = 0.00242229\n",
      "Iteration 2943, loss = 0.00242021\n",
      "Iteration 2944, loss = 0.00241814\n",
      "Iteration 2945, loss = 0.00241607\n",
      "Iteration 2946, loss = 0.00241400\n",
      "Iteration 2947, loss = 0.00241193\n",
      "Iteration 2948, loss = 0.00240987\n",
      "Iteration 2949, loss = 0.00240780\n",
      "Iteration 2950, loss = 0.00240574\n",
      "Iteration 2951, loss = 0.00240369\n",
      "Iteration 2952, loss = 0.00240163\n",
      "Iteration 2953, loss = 0.00239958\n",
      "Iteration 2954, loss = 0.00239753\n",
      "Iteration 2955, loss = 0.00239548\n",
      "Iteration 2956, loss = 0.00239343\n",
      "Iteration 2957, loss = 0.00239139\n",
      "Iteration 2958, loss = 0.00238935\n",
      "Iteration 2959, loss = 0.00238731\n",
      "Iteration 2960, loss = 0.00238527\n",
      "Iteration 2961, loss = 0.00238323\n",
      "Iteration 2962, loss = 0.00238120\n",
      "Iteration 2963, loss = 0.00237917\n",
      "Iteration 2964, loss = 0.00237714\n",
      "Iteration 2965, loss = 0.00237512\n",
      "Iteration 2966, loss = 0.00237309\n",
      "Iteration 2967, loss = 0.00237107\n",
      "Iteration 2968, loss = 0.00236905\n",
      "Iteration 2969, loss = 0.00236703\n",
      "Iteration 2970, loss = 0.00236502\n",
      "Iteration 2971, loss = 0.00236300\n",
      "Iteration 2972, loss = 0.00236099\n",
      "Iteration 2973, loss = 0.00235898\n",
      "Iteration 2974, loss = 0.00235698\n",
      "Iteration 2975, loss = 0.00235497\n",
      "Iteration 2976, loss = 0.00235297\n",
      "Iteration 2977, loss = 0.00235097\n",
      "Iteration 2978, loss = 0.00234897\n",
      "Iteration 2979, loss = 0.00234697\n",
      "Iteration 2980, loss = 0.00234498\n",
      "Iteration 2981, loss = 0.00234299\n",
      "Iteration 2982, loss = 0.00234100\n",
      "Iteration 2983, loss = 0.00233901\n",
      "Iteration 2984, loss = 0.00233703\n",
      "Iteration 2985, loss = 0.00233504\n",
      "Iteration 2986, loss = 0.00233306\n",
      "Iteration 2987, loss = 0.00233108\n",
      "Iteration 2988, loss = 0.00232911\n",
      "Iteration 2989, loss = 0.00232713\n",
      "Iteration 2990, loss = 0.00232516\n",
      "Iteration 2991, loss = 0.00232319\n",
      "Iteration 2992, loss = 0.00232122\n",
      "Iteration 2993, loss = 0.00231926\n",
      "Iteration 2994, loss = 0.00231729\n",
      "Iteration 2995, loss = 0.00231533\n",
      "Iteration 2996, loss = 0.00231337\n",
      "Iteration 2997, loss = 0.00231141\n",
      "Iteration 2998, loss = 0.00230946\n",
      "Iteration 2999, loss = 0.00230750\n",
      "Iteration 3000, loss = 0.00230555\n",
      "Iteration 3001, loss = 0.00230360\n",
      "Iteration 3002, loss = 0.00230166\n",
      "Iteration 3003, loss = 0.00229971\n",
      "Iteration 3004, loss = 0.00229777\n",
      "Iteration 3005, loss = 0.00229583\n",
      "Iteration 3006, loss = 0.00229389\n",
      "Iteration 3007, loss = 0.00229195\n",
      "Iteration 3008, loss = 0.00229002\n",
      "Iteration 3009, loss = 0.00228808\n",
      "Iteration 3010, loss = 0.00228615\n",
      "Iteration 3011, loss = 0.00228423\n",
      "Iteration 3012, loss = 0.00228230\n",
      "Iteration 3013, loss = 0.00228038\n",
      "Iteration 3014, loss = 0.00227845\n",
      "Iteration 3015, loss = 0.00227653\n",
      "Iteration 3016, loss = 0.00227461\n",
      "Iteration 3017, loss = 0.00227270\n",
      "Iteration 3018, loss = 0.00227078\n",
      "Iteration 3019, loss = 0.00226887\n",
      "Iteration 3020, loss = 0.00226696\n",
      "Iteration 3021, loss = 0.00226506\n",
      "Iteration 3022, loss = 0.00226315\n",
      "Iteration 3023, loss = 0.00226125\n",
      "Iteration 3024, loss = 0.00225934\n",
      "Iteration 3025, loss = 0.00225744\n",
      "Iteration 3026, loss = 0.00225555\n",
      "Iteration 3027, loss = 0.00225365\n",
      "Iteration 3028, loss = 0.00225176\n",
      "Iteration 3029, loss = 0.00224987\n",
      "Iteration 3030, loss = 0.00224798\n",
      "Iteration 3031, loss = 0.00224609\n",
      "Iteration 3032, loss = 0.00224420\n",
      "Iteration 3033, loss = 0.00224232\n",
      "Iteration 3034, loss = 0.00224044\n",
      "Iteration 3035, loss = 0.00223856\n",
      "Iteration 3036, loss = 0.00223668\n",
      "Iteration 3037, loss = 0.00223481\n",
      "Iteration 3038, loss = 0.00223293\n",
      "Iteration 3039, loss = 0.00223106\n",
      "Iteration 3040, loss = 0.00222919\n",
      "Iteration 3041, loss = 0.00222732\n",
      "Iteration 3042, loss = 0.00222546\n",
      "Iteration 3043, loss = 0.00222359\n",
      "Iteration 3044, loss = 0.00222173\n",
      "Iteration 3045, loss = 0.00221987\n",
      "Iteration 3046, loss = 0.00221802\n",
      "Iteration 3047, loss = 0.00221616\n",
      "Iteration 3048, loss = 0.00221431\n",
      "Iteration 3049, loss = 0.00221246\n",
      "Iteration 3050, loss = 0.00221061\n",
      "Iteration 3051, loss = 0.00220876\n",
      "Iteration 3052, loss = 0.00220691\n",
      "Iteration 3053, loss = 0.00220507\n",
      "Iteration 3054, loss = 0.00220323\n",
      "Iteration 3055, loss = 0.00220139\n",
      "Iteration 3056, loss = 0.00219955\n",
      "Iteration 3057, loss = 0.00219771\n",
      "Iteration 3058, loss = 0.00219588\n",
      "Iteration 3059, loss = 0.00219405\n",
      "Iteration 3060, loss = 0.00219222\n",
      "Iteration 3061, loss = 0.00219039\n",
      "Iteration 3062, loss = 0.00218856\n",
      "Iteration 3063, loss = 0.00218674\n",
      "Iteration 3064, loss = 0.00218491\n",
      "Iteration 3065, loss = 0.00218309\n",
      "Iteration 3066, loss = 0.00218128\n",
      "Iteration 3067, loss = 0.00217946\n",
      "Iteration 3068, loss = 0.00217764\n",
      "Iteration 3069, loss = 0.00217583\n",
      "Iteration 3070, loss = 0.00217402\n",
      "Iteration 3071, loss = 0.00217221\n",
      "Iteration 3072, loss = 0.00217040\n",
      "Iteration 3073, loss = 0.00216860\n",
      "Iteration 3074, loss = 0.00216680\n",
      "Iteration 3075, loss = 0.00216500\n",
      "Iteration 3076, loss = 0.00216320\n",
      "Iteration 3077, loss = 0.00216140\n",
      "Iteration 3078, loss = 0.00215960\n",
      "Iteration 3079, loss = 0.00215781\n",
      "Iteration 3080, loss = 0.00215602\n",
      "Iteration 3081, loss = 0.00215423\n",
      "Iteration 3082, loss = 0.00215244\n",
      "Iteration 3083, loss = 0.00215065\n",
      "Iteration 3084, loss = 0.00214887\n",
      "Iteration 3085, loss = 0.00214709\n",
      "Iteration 3086, loss = 0.00214531\n",
      "Iteration 3087, loss = 0.00214353\n",
      "Iteration 3088, loss = 0.00214175\n",
      "Iteration 3089, loss = 0.00213998\n",
      "Iteration 3090, loss = 0.00213820\n",
      "Iteration 3091, loss = 0.00213643\n",
      "Iteration 3092, loss = 0.00213466\n",
      "Iteration 3093, loss = 0.00213289\n",
      "Iteration 3094, loss = 0.00213113\n",
      "Iteration 3095, loss = 0.00212937\n",
      "Iteration 3096, loss = 0.00212760\n",
      "Iteration 3097, loss = 0.00212584\n",
      "Iteration 3098, loss = 0.00212409\n",
      "Iteration 3099, loss = 0.00212233\n",
      "Iteration 3100, loss = 0.00212057\n",
      "Iteration 3101, loss = 0.00211882\n",
      "Iteration 3102, loss = 0.00211707\n",
      "Iteration 3103, loss = 0.00211532\n",
      "Iteration 3104, loss = 0.00211357\n",
      "Iteration 3105, loss = 0.00211183\n",
      "Iteration 3106, loss = 0.00211009\n",
      "Iteration 3107, loss = 0.00210834\n",
      "Iteration 3108, loss = 0.00210660\n",
      "Iteration 3109, loss = 0.00210487\n",
      "Iteration 3110, loss = 0.00210313\n",
      "Iteration 3111, loss = 0.00210140\n",
      "Iteration 3112, loss = 0.00209966\n",
      "Iteration 3113, loss = 0.00209793\n",
      "Iteration 3114, loss = 0.00209620\n",
      "Iteration 3115, loss = 0.00209448\n",
      "Iteration 3116, loss = 0.00209275\n",
      "Iteration 3117, loss = 0.00209103\n",
      "Iteration 3118, loss = 0.00208931\n",
      "Iteration 3119, loss = 0.00208759\n",
      "Iteration 3120, loss = 0.00208587\n",
      "Iteration 3121, loss = 0.00208415\n",
      "Iteration 3122, loss = 0.00208244\n",
      "Iteration 3123, loss = 0.00208073\n",
      "Iteration 3124, loss = 0.00207901\n",
      "Iteration 3125, loss = 0.00207731\n",
      "Iteration 3126, loss = 0.00207560\n",
      "Iteration 3127, loss = 0.00207389\n",
      "Iteration 3128, loss = 0.00207219\n",
      "Iteration 3129, loss = 0.00207049\n",
      "Iteration 3130, loss = 0.00206879\n",
      "Iteration 3131, loss = 0.00206709\n",
      "Iteration 3132, loss = 0.00206539\n",
      "Iteration 3133, loss = 0.00206370\n",
      "Iteration 3134, loss = 0.00206200\n",
      "Iteration 3135, loss = 0.00206031\n",
      "Iteration 3136, loss = 0.00205862\n",
      "Iteration 3137, loss = 0.00205694\n",
      "Iteration 3138, loss = 0.00205525\n",
      "Iteration 3139, loss = 0.00205357\n",
      "Iteration 3140, loss = 0.00205188\n",
      "Iteration 3141, loss = 0.00205020\n",
      "Iteration 3142, loss = 0.00204852\n",
      "Iteration 3143, loss = 0.00204685\n",
      "Iteration 3144, loss = 0.00204517\n",
      "Iteration 3145, loss = 0.00204350\n",
      "Iteration 3146, loss = 0.00204182\n",
      "Iteration 3147, loss = 0.00204015\n",
      "Iteration 3148, loss = 0.00203849\n",
      "Iteration 3149, loss = 0.00203682\n",
      "Iteration 3150, loss = 0.00203515\n",
      "Iteration 3151, loss = 0.00203349\n",
      "Iteration 3152, loss = 0.00203183\n",
      "Iteration 3153, loss = 0.00203017\n",
      "Iteration 3154, loss = 0.00202851\n",
      "Iteration 3155, loss = 0.00202685\n",
      "Iteration 3156, loss = 0.00202520\n",
      "Iteration 3157, loss = 0.00202355\n",
      "Iteration 3158, loss = 0.00202189\n",
      "Iteration 3159, loss = 0.00202024\n",
      "Iteration 3160, loss = 0.00201860\n",
      "Iteration 3161, loss = 0.00201695\n",
      "Iteration 3162, loss = 0.00201531\n",
      "Iteration 3163, loss = 0.00201366\n",
      "Iteration 3164, loss = 0.00201202\n",
      "Iteration 3165, loss = 0.00201038\n",
      "Iteration 3166, loss = 0.00200874\n",
      "Iteration 3167, loss = 0.00200711\n",
      "Iteration 3168, loss = 0.00200547\n",
      "Iteration 3169, loss = 0.00200384\n",
      "Iteration 3170, loss = 0.00200221\n",
      "Iteration 3171, loss = 0.00200058\n",
      "Iteration 3172, loss = 0.00199895\n",
      "Iteration 3173, loss = 0.00199733\n",
      "Iteration 3174, loss = 0.00199570\n",
      "Iteration 3175, loss = 0.00199408\n",
      "Iteration 3176, loss = 0.00199246\n",
      "Iteration 3177, loss = 0.00199084\n",
      "Iteration 3178, loss = 0.00198922\n",
      "Iteration 3179, loss = 0.00198761\n",
      "Iteration 3180, loss = 0.00198599\n",
      "Iteration 3181, loss = 0.00198438\n",
      "Iteration 3182, loss = 0.00198277\n",
      "Iteration 3183, loss = 0.00198116\n",
      "Iteration 3184, loss = 0.00197955\n",
      "Iteration 3185, loss = 0.00197795\n",
      "Iteration 3186, loss = 0.00197634\n",
      "Iteration 3187, loss = 0.00197474\n",
      "Iteration 3188, loss = 0.00197314\n",
      "Iteration 3189, loss = 0.00197154\n",
      "Iteration 3190, loss = 0.00196994\n",
      "Iteration 3191, loss = 0.00196835\n",
      "Iteration 3192, loss = 0.00196675\n",
      "Iteration 3193, loss = 0.00196516\n",
      "Iteration 3194, loss = 0.00196357\n",
      "Iteration 3195, loss = 0.00196198\n",
      "Iteration 3196, loss = 0.00196039\n",
      "Iteration 3197, loss = 0.00195881\n",
      "Iteration 3198, loss = 0.00195722\n",
      "Iteration 3199, loss = 0.00195564\n",
      "Iteration 3200, loss = 0.00195406\n",
      "Iteration 3201, loss = 0.00195248\n",
      "Iteration 3202, loss = 0.00195090\n",
      "Iteration 3203, loss = 0.00194932\n",
      "Iteration 3204, loss = 0.00194775\n",
      "Iteration 3205, loss = 0.00194618\n",
      "Iteration 3206, loss = 0.00194460\n",
      "Iteration 3207, loss = 0.00194303\n",
      "Iteration 3208, loss = 0.00194147\n",
      "Iteration 3209, loss = 0.00193990\n",
      "Iteration 3210, loss = 0.00193833\n",
      "Iteration 3211, loss = 0.00193677\n",
      "Iteration 3212, loss = 0.00193521\n",
      "Iteration 3213, loss = 0.00193365\n",
      "Iteration 3214, loss = 0.00193209\n",
      "Iteration 3215, loss = 0.00193053\n",
      "Iteration 3216, loss = 0.00192898\n",
      "Iteration 3217, loss = 0.00192742\n",
      "Iteration 3218, loss = 0.00192587\n",
      "Iteration 3219, loss = 0.00192432\n",
      "Iteration 3220, loss = 0.00192277\n",
      "Iteration 3221, loss = 0.00192122\n",
      "Iteration 3222, loss = 0.00191968\n",
      "Iteration 3223, loss = 0.00191813\n",
      "Iteration 3224, loss = 0.00191659\n",
      "Iteration 3225, loss = 0.00191505\n",
      "Iteration 3226, loss = 0.00191351\n",
      "Iteration 3227, loss = 0.00191197\n",
      "Iteration 3228, loss = 0.00191043\n",
      "Iteration 3229, loss = 0.00190890\n",
      "Iteration 3230, loss = 0.00190737\n",
      "Iteration 3231, loss = 0.00190583\n",
      "Iteration 3232, loss = 0.00190430\n",
      "Iteration 3233, loss = 0.00190278\n",
      "Iteration 3234, loss = 0.00190125\n",
      "Iteration 3235, loss = 0.00189972\n",
      "Iteration 3236, loss = 0.00189820\n",
      "Iteration 3237, loss = 0.00189668\n",
      "Iteration 3238, loss = 0.00189516\n",
      "Iteration 3239, loss = 0.00189364\n",
      "Iteration 3240, loss = 0.00189212\n",
      "Iteration 3241, loss = 0.00189060\n",
      "Iteration 3242, loss = 0.00188909\n",
      "Iteration 3243, loss = 0.00188757\n",
      "Iteration 3244, loss = 0.00188606\n",
      "Iteration 3245, loss = 0.00188455\n",
      "Iteration 3246, loss = 0.00188304\n",
      "Iteration 3247, loss = 0.00188154\n",
      "Iteration 3248, loss = 0.00188003\n",
      "Iteration 3249, loss = 0.00187853\n",
      "Iteration 3250, loss = 0.00187702\n",
      "Iteration 3251, loss = 0.00187552\n",
      "Iteration 3252, loss = 0.00187402\n",
      "Iteration 3253, loss = 0.00187253\n",
      "Iteration 3254, loss = 0.00187103\n",
      "Iteration 3255, loss = 0.00186953\n",
      "Iteration 3256, loss = 0.00186804\n",
      "Iteration 3257, loss = 0.00186655\n",
      "Iteration 3258, loss = 0.00186506\n",
      "Iteration 3259, loss = 0.00186357\n",
      "Iteration 3260, loss = 0.00186208\n",
      "Iteration 3261, loss = 0.00186060\n",
      "Iteration 3262, loss = 0.00185911\n",
      "Iteration 3263, loss = 0.00185763\n",
      "Iteration 3264, loss = 0.00185615\n",
      "Iteration 3265, loss = 0.00185467\n",
      "Iteration 3266, loss = 0.00185319\n",
      "Iteration 3267, loss = 0.00185171\n",
      "Iteration 3268, loss = 0.00185024\n",
      "Iteration 3269, loss = 0.00184876\n",
      "Iteration 3270, loss = 0.00184729\n",
      "Iteration 3271, loss = 0.00184582\n",
      "Iteration 3272, loss = 0.00184435\n",
      "Iteration 3273, loss = 0.00184288\n",
      "Iteration 3274, loss = 0.00184142\n",
      "Iteration 3275, loss = 0.00183995\n",
      "Iteration 3276, loss = 0.00183849\n",
      "Iteration 3277, loss = 0.00183703\n",
      "Iteration 3278, loss = 0.00183557\n",
      "Iteration 3279, loss = 0.00183411\n",
      "Iteration 3280, loss = 0.00183265\n",
      "Iteration 3281, loss = 0.00183119\n",
      "Iteration 3282, loss = 0.00182974\n",
      "Iteration 3283, loss = 0.00182829\n",
      "Iteration 3284, loss = 0.00182684\n",
      "Iteration 3285, loss = 0.00182539\n",
      "Iteration 3286, loss = 0.00182394\n",
      "Iteration 3287, loss = 0.00182249\n",
      "Iteration 3288, loss = 0.00182104\n",
      "Iteration 3289, loss = 0.00181960\n",
      "Iteration 3290, loss = 0.00181816\n",
      "Iteration 3291, loss = 0.00181672\n",
      "Iteration 3292, loss = 0.00181528\n",
      "Iteration 3293, loss = 0.00181384\n",
      "Iteration 3294, loss = 0.00181240\n",
      "Iteration 3295, loss = 0.00181096\n",
      "Iteration 3296, loss = 0.00180953\n",
      "Iteration 3297, loss = 0.00180810\n",
      "Iteration 3298, loss = 0.00180667\n",
      "Iteration 3299, loss = 0.00180524\n",
      "Iteration 3300, loss = 0.00180381\n",
      "Iteration 3301, loss = 0.00180238\n",
      "Iteration 3302, loss = 0.00180095\n",
      "Iteration 3303, loss = 0.00179953\n",
      "Iteration 3304, loss = 0.00179811\n",
      "Iteration 3305, loss = 0.00179669\n",
      "Iteration 3306, loss = 0.00179527\n",
      "Iteration 3307, loss = 0.00179385\n",
      "Iteration 3308, loss = 0.00179243\n",
      "Iteration 3309, loss = 0.00179102\n",
      "Iteration 3310, loss = 0.00178960\n",
      "Iteration 3311, loss = 0.00178819\n",
      "Iteration 3312, loss = 0.00178678\n",
      "Iteration 3313, loss = 0.00178537\n",
      "Iteration 3314, loss = 0.00178396\n",
      "Iteration 3315, loss = 0.00178255\n",
      "Iteration 3316, loss = 0.00178115\n",
      "Iteration 3317, loss = 0.00177974\n",
      "Iteration 3318, loss = 0.00177834\n",
      "Iteration 3319, loss = 0.00177694\n",
      "Iteration 3320, loss = 0.00177554\n",
      "Iteration 3321, loss = 0.00177414\n",
      "Iteration 3322, loss = 0.00177274\n",
      "Iteration 3323, loss = 0.00177135\n",
      "Iteration 3324, loss = 0.00176995\n",
      "Iteration 3325, loss = 0.00176856\n",
      "Iteration 3326, loss = 0.00176717\n",
      "Iteration 3327, loss = 0.00176578\n",
      "Iteration 3328, loss = 0.00176439\n",
      "Iteration 3329, loss = 0.00176300\n",
      "Iteration 3330, loss = 0.00176161\n",
      "Iteration 3331, loss = 0.00176023\n",
      "Iteration 3332, loss = 0.00175885\n",
      "Iteration 3333, loss = 0.00175746\n",
      "Iteration 3334, loss = 0.00175608\n",
      "Iteration 3335, loss = 0.00175470\n",
      "Iteration 3336, loss = 0.00175333\n",
      "Iteration 3337, loss = 0.00175195\n",
      "Iteration 3338, loss = 0.00175057\n",
      "Iteration 3339, loss = 0.00174920\n",
      "Iteration 3340, loss = 0.00174783\n",
      "Iteration 3341, loss = 0.00174646\n",
      "Iteration 3342, loss = 0.00174509\n",
      "Iteration 3343, loss = 0.00174372\n",
      "Iteration 3344, loss = 0.00174235\n",
      "Iteration 3345, loss = 0.00174099\n",
      "Iteration 3346, loss = 0.00173962\n",
      "Iteration 3347, loss = 0.00173826\n",
      "Iteration 3348, loss = 0.00173690\n",
      "Iteration 3349, loss = 0.00173554\n",
      "Iteration 3350, loss = 0.00173418\n",
      "Iteration 3351, loss = 0.00173282\n",
      "Iteration 3352, loss = 0.00173147\n",
      "Iteration 3353, loss = 0.00173011\n",
      "Iteration 3354, loss = 0.00172876\n",
      "Iteration 3355, loss = 0.00172741\n",
      "Iteration 3356, loss = 0.00172606\n",
      "Iteration 3357, loss = 0.00172471\n",
      "Iteration 3358, loss = 0.00172336\n",
      "Iteration 3359, loss = 0.00172201\n",
      "Iteration 3360, loss = 0.00172067\n",
      "Iteration 3361, loss = 0.00171932\n",
      "Iteration 3362, loss = 0.00171798\n",
      "Iteration 3363, loss = 0.00171664\n",
      "Iteration 3364, loss = 0.00171530\n",
      "Iteration 3365, loss = 0.00171396\n",
      "Iteration 3366, loss = 0.00171262\n",
      "Iteration 3367, loss = 0.00171129\n",
      "Iteration 3368, loss = 0.00170995\n",
      "Iteration 3369, loss = 0.00170862\n",
      "Iteration 3370, loss = 0.00170729\n",
      "Iteration 3371, loss = 0.00170596\n",
      "Iteration 3372, loss = 0.00170463\n",
      "Iteration 3373, loss = 0.00170330\n",
      "Iteration 3374, loss = 0.00170197\n",
      "Iteration 3375, loss = 0.00170065\n",
      "Iteration 3376, loss = 0.00169932\n",
      "Iteration 3377, loss = 0.00169800\n",
      "Iteration 3378, loss = 0.00169668\n",
      "Iteration 3379, loss = 0.00169536\n",
      "Iteration 3380, loss = 0.00169404\n",
      "Iteration 3381, loss = 0.00169272\n",
      "Iteration 3382, loss = 0.00169140\n",
      "Iteration 3383, loss = 0.00169009\n",
      "Iteration 3384, loss = 0.00168877\n",
      "Iteration 3385, loss = 0.00168746\n",
      "Iteration 3386, loss = 0.00168615\n",
      "Iteration 3387, loss = 0.00168484\n",
      "Iteration 3388, loss = 0.00168353\n",
      "Iteration 3389, loss = 0.00168223\n",
      "Iteration 3390, loss = 0.00168092\n",
      "Iteration 3391, loss = 0.00167961\n",
      "Iteration 3392, loss = 0.00167831\n",
      "Iteration 3393, loss = 0.00167701\n",
      "Iteration 3394, loss = 0.00167571\n",
      "Iteration 3395, loss = 0.00167441\n",
      "Iteration 3396, loss = 0.00167311\n",
      "Iteration 3397, loss = 0.00167181\n",
      "Iteration 3398, loss = 0.00167052\n",
      "Iteration 3399, loss = 0.00166922\n",
      "Iteration 3400, loss = 0.00166793\n",
      "Iteration 3401, loss = 0.00166664\n",
      "Iteration 3402, loss = 0.00166535\n",
      "Iteration 3403, loss = 0.00166406\n",
      "Iteration 3404, loss = 0.00166277\n",
      "Iteration 3405, loss = 0.00166148\n",
      "Iteration 3406, loss = 0.00166020\n",
      "Iteration 3407, loss = 0.00165891\n",
      "Iteration 3408, loss = 0.00165763\n",
      "Iteration 3409, loss = 0.00165635\n",
      "Iteration 3410, loss = 0.00165507\n",
      "Iteration 3411, loss = 0.00165379\n",
      "Iteration 3412, loss = 0.00165251\n",
      "Iteration 3413, loss = 0.00165123\n",
      "Iteration 3414, loss = 0.00164996\n",
      "Iteration 3415, loss = 0.00164868\n",
      "Iteration 3416, loss = 0.00164741\n",
      "Iteration 3417, loss = 0.00164614\n",
      "Iteration 3418, loss = 0.00164487\n",
      "Iteration 3419, loss = 0.00164360\n",
      "Iteration 3420, loss = 0.00164233\n",
      "Iteration 3421, loss = 0.00164106\n",
      "Iteration 3422, loss = 0.00163980\n",
      "Iteration 3423, loss = 0.00163853\n",
      "Iteration 3424, loss = 0.00163727\n",
      "Iteration 3425, loss = 0.00163601\n",
      "Iteration 3426, loss = 0.00163475\n",
      "Iteration 3427, loss = 0.00163349\n",
      "Iteration 3428, loss = 0.00163223\n",
      "Iteration 3429, loss = 0.00163097\n",
      "Iteration 3430, loss = 0.00162972\n",
      "Iteration 3431, loss = 0.00162846\n",
      "Iteration 3432, loss = 0.00162721\n",
      "Iteration 3433, loss = 0.00162596\n",
      "Iteration 3434, loss = 0.00162471\n",
      "Iteration 3435, loss = 0.00162346\n",
      "Iteration 3436, loss = 0.00162221\n",
      "Iteration 3437, loss = 0.00162096\n",
      "Iteration 3438, loss = 0.00161972\n",
      "Iteration 3439, loss = 0.00161847\n",
      "Iteration 3440, loss = 0.00161723\n",
      "Iteration 3441, loss = 0.00161599\n",
      "Iteration 3442, loss = 0.00161474\n",
      "Iteration 3443, loss = 0.00161350\n",
      "Iteration 3444, loss = 0.00161227\n",
      "Iteration 3445, loss = 0.00161103\n",
      "Iteration 3446, loss = 0.00160979\n",
      "Iteration 3447, loss = 0.00160856\n",
      "Iteration 3448, loss = 0.00160732\n",
      "Iteration 3449, loss = 0.00160609\n",
      "Iteration 3450, loss = 0.00160486\n",
      "Iteration 3451, loss = 0.00160363\n",
      "Iteration 3452, loss = 0.00160240\n",
      "Iteration 3453, loss = 0.00160117\n",
      "Iteration 3454, loss = 0.00159995\n",
      "Iteration 3455, loss = 0.00159872\n",
      "Iteration 3456, loss = 0.00159750\n",
      "Iteration 3457, loss = 0.00159627\n",
      "Iteration 3458, loss = 0.00159505\n",
      "Iteration 3459, loss = 0.00159383\n",
      "Iteration 3460, loss = 0.00159261\n",
      "Iteration 3461, loss = 0.00159139\n",
      "Iteration 3462, loss = 0.00159017\n",
      "Iteration 3463, loss = 0.00158896\n",
      "Iteration 3464, loss = 0.00158774\n",
      "Iteration 3465, loss = 0.00158653\n",
      "Iteration 3466, loss = 0.00158532\n",
      "Iteration 3467, loss = 0.00158411\n",
      "Iteration 3468, loss = 0.00158290\n",
      "Iteration 3469, loss = 0.00158169\n",
      "Iteration 3470, loss = 0.00158048\n",
      "Iteration 3471, loss = 0.00157927\n",
      "Iteration 3472, loss = 0.00157807\n",
      "Iteration 3473, loss = 0.00157686\n",
      "Iteration 3474, loss = 0.00157566\n",
      "Iteration 3475, loss = 0.00157446\n",
      "Iteration 3476, loss = 0.00157326\n",
      "Iteration 3477, loss = 0.00157206\n",
      "Iteration 3478, loss = 0.00157086\n",
      "Iteration 3479, loss = 0.00156966\n",
      "Iteration 3480, loss = 0.00156847\n",
      "Iteration 3481, loss = 0.00156727\n",
      "Iteration 3482, loss = 0.00156608\n",
      "Iteration 3483, loss = 0.00156488\n",
      "Iteration 3484, loss = 0.00156369\n",
      "Iteration 3485, loss = 0.00156250\n",
      "Iteration 3486, loss = 0.00156131\n",
      "Iteration 3487, loss = 0.00156013\n",
      "Iteration 3488, loss = 0.00155894\n",
      "Iteration 3489, loss = 0.00155775\n",
      "Iteration 3490, loss = 0.00155657\n",
      "Iteration 3491, loss = 0.00155538\n",
      "Iteration 3492, loss = 0.00155420\n",
      "Iteration 3493, loss = 0.00155302\n",
      "Iteration 3494, loss = 0.00155184\n",
      "Iteration 3495, loss = 0.00155066\n",
      "Iteration 3496, loss = 0.00154948\n",
      "Iteration 3497, loss = 0.00154831\n",
      "Iteration 3498, loss = 0.00154713\n",
      "Iteration 3499, loss = 0.00154596\n",
      "Iteration 3500, loss = 0.00154478\n",
      "Iteration 3501, loss = 0.00154361\n",
      "Iteration 3502, loss = 0.00154244\n",
      "Iteration 3503, loss = 0.00154127\n",
      "Iteration 3504, loss = 0.00154010\n",
      "Iteration 3505, loss = 0.00153894\n",
      "Iteration 3506, loss = 0.00153777\n",
      "Iteration 3507, loss = 0.00153660\n",
      "Iteration 3508, loss = 0.00153544\n",
      "Iteration 3509, loss = 0.00153428\n",
      "Iteration 3510, loss = 0.00153311\n",
      "Iteration 3511, loss = 0.00153195\n",
      "Iteration 3512, loss = 0.00153079\n",
      "Iteration 3513, loss = 0.00152963\n",
      "Iteration 3514, loss = 0.00152848\n",
      "Iteration 3515, loss = 0.00152732\n",
      "Iteration 3516, loss = 0.00152617\n",
      "Iteration 3517, loss = 0.00152501\n",
      "Iteration 3518, loss = 0.00152386\n",
      "Iteration 3519, loss = 0.00152271\n",
      "Iteration 3520, loss = 0.00152156\n",
      "Iteration 3521, loss = 0.00152041\n",
      "Iteration 3522, loss = 0.00151926\n",
      "Iteration 3523, loss = 0.00151811\n",
      "Iteration 3524, loss = 0.00151696\n",
      "Iteration 3525, loss = 0.00151582\n",
      "Iteration 3526, loss = 0.00151467\n",
      "Iteration 3527, loss = 0.00151353\n",
      "Iteration 3528, loss = 0.00151239\n",
      "Iteration 3529, loss = 0.00151125\n",
      "Iteration 3530, loss = 0.00151011\n",
      "Iteration 3531, loss = 0.00150897\n",
      "Iteration 3532, loss = 0.00150783\n",
      "Iteration 3533, loss = 0.00150669\n",
      "Iteration 3534, loss = 0.00150556\n",
      "Iteration 3535, loss = 0.00150442\n",
      "Iteration 3536, loss = 0.00150329\n",
      "Iteration 3537, loss = 0.00150216\n",
      "Iteration 3538, loss = 0.00150103\n",
      "Iteration 3539, loss = 0.00149990\n",
      "Iteration 3540, loss = 0.00149877\n",
      "Iteration 3541, loss = 0.00149764\n",
      "Iteration 3542, loss = 0.00149651\n",
      "Iteration 3543, loss = 0.00149539\n",
      "Iteration 3544, loss = 0.00149426\n",
      "Iteration 3545, loss = 0.00149314\n",
      "Iteration 3546, loss = 0.00149202\n",
      "Iteration 3547, loss = 0.00149089\n",
      "Iteration 3548, loss = 0.00148977\n",
      "Iteration 3549, loss = 0.00148865\n",
      "Iteration 3550, loss = 0.00148754\n",
      "Iteration 3551, loss = 0.00148642\n",
      "Iteration 3552, loss = 0.00148530\n",
      "Iteration 3553, loss = 0.00148419\n",
      "Iteration 3554, loss = 0.00148307\n",
      "Iteration 3555, loss = 0.00148196\n",
      "Iteration 3556, loss = 0.00148085\n",
      "Iteration 3557, loss = 0.00147974\n",
      "Iteration 3558, loss = 0.00147863\n",
      "Iteration 3559, loss = 0.00147752\n",
      "Iteration 3560, loss = 0.00147641\n",
      "Iteration 3561, loss = 0.00147530\n",
      "Iteration 3562, loss = 0.00147420\n",
      "Iteration 3563, loss = 0.00147309\n",
      "Iteration 3564, loss = 0.00147199\n",
      "Iteration 3565, loss = 0.00147089\n",
      "Iteration 3566, loss = 0.00146979\n",
      "Iteration 3567, loss = 0.00146869\n",
      "Iteration 3568, loss = 0.00146759\n",
      "Iteration 3569, loss = 0.00146649\n",
      "Iteration 3570, loss = 0.00146539\n",
      "Iteration 3571, loss = 0.00146429\n",
      "Iteration 3572, loss = 0.00146320\n",
      "Iteration 3573, loss = 0.00146210\n",
      "Iteration 3574, loss = 0.00146101\n",
      "Iteration 3575, loss = 0.00145992\n",
      "Iteration 3576, loss = 0.00145883\n",
      "Iteration 3577, loss = 0.00145774\n",
      "Iteration 3578, loss = 0.00145665\n",
      "Iteration 3579, loss = 0.00145556\n",
      "Iteration 3580, loss = 0.00145447\n",
      "Iteration 3581, loss = 0.00145339\n",
      "Iteration 3582, loss = 0.00145230\n",
      "Iteration 3583, loss = 0.00145122\n",
      "Iteration 3584, loss = 0.00145014\n",
      "Iteration 3585, loss = 0.00144906\n",
      "Iteration 3586, loss = 0.00144797\n",
      "Iteration 3587, loss = 0.00144690\n",
      "Iteration 3588, loss = 0.00144582\n",
      "Iteration 3589, loss = 0.00144474\n",
      "Iteration 3590, loss = 0.00144366\n",
      "Iteration 3591, loss = 0.00144259\n",
      "Iteration 3592, loss = 0.00144151\n",
      "Iteration 3593, loss = 0.00144044\n",
      "Iteration 3594, loss = 0.00143937\n",
      "Iteration 3595, loss = 0.00143829\n",
      "Iteration 3596, loss = 0.00143722\n",
      "Iteration 3597, loss = 0.00143615\n",
      "Iteration 3598, loss = 0.00143509\n",
      "Iteration 3599, loss = 0.00143402\n",
      "Iteration 3600, loss = 0.00143295\n",
      "Iteration 3601, loss = 0.00143189\n",
      "Iteration 3602, loss = 0.00143082\n",
      "Iteration 3603, loss = 0.00142976\n",
      "Iteration 3604, loss = 0.00142869\n",
      "Iteration 3605, loss = 0.00142763\n",
      "Iteration 3606, loss = 0.00142657\n",
      "Iteration 3607, loss = 0.00142551\n",
      "Iteration 3608, loss = 0.00142445\n",
      "Iteration 3609, loss = 0.00142340\n",
      "Iteration 3610, loss = 0.00142234\n",
      "Iteration 3611, loss = 0.00142129\n",
      "Iteration 3612, loss = 0.00142023\n",
      "Iteration 3613, loss = 0.00141918\n",
      "Iteration 3614, loss = 0.00141812\n",
      "Iteration 3615, loss = 0.00141707\n",
      "Iteration 3616, loss = 0.00141602\n",
      "Iteration 3617, loss = 0.00141497\n",
      "Iteration 3618, loss = 0.00141392\n",
      "Iteration 3619, loss = 0.00141288\n",
      "Iteration 3620, loss = 0.00141183\n",
      "Iteration 3621, loss = 0.00141078\n",
      "Iteration 3622, loss = 0.00140974\n",
      "Iteration 3623, loss = 0.00140870\n",
      "Iteration 3624, loss = 0.00140765\n",
      "Iteration 3625, loss = 0.00140661\n",
      "Iteration 3626, loss = 0.00140557\n",
      "Iteration 3627, loss = 0.00140453\n",
      "Iteration 3628, loss = 0.00140349\n",
      "Iteration 3629, loss = 0.00140245\n",
      "Iteration 3630, loss = 0.00140142\n",
      "Iteration 3631, loss = 0.00140038\n",
      "Iteration 3632, loss = 0.00139935\n",
      "Iteration 3633, loss = 0.00139831\n",
      "Iteration 3634, loss = 0.00139728\n",
      "Iteration 3635, loss = 0.00139625\n",
      "Iteration 3636, loss = 0.00139522\n",
      "Iteration 3637, loss = 0.00139419\n",
      "Iteration 3638, loss = 0.00139316\n",
      "Iteration 3639, loss = 0.00139213\n",
      "Iteration 3640, loss = 0.00139110\n",
      "Iteration 3641, loss = 0.00139008\n",
      "Iteration 3642, loss = 0.00138905\n",
      "Iteration 3643, loss = 0.00138803\n",
      "Iteration 3644, loss = 0.00138700\n",
      "Iteration 3645, loss = 0.00138598\n",
      "Iteration 3646, loss = 0.00138496\n",
      "Iteration 3647, loss = 0.00138394\n",
      "Iteration 3648, loss = 0.00138292\n",
      "Iteration 3649, loss = 0.00138190\n",
      "Iteration 3650, loss = 0.00138088\n",
      "Iteration 3651, loss = 0.00137987\n",
      "Iteration 3652, loss = 0.00137885\n",
      "Iteration 3653, loss = 0.00137784\n",
      "Iteration 3654, loss = 0.00137682\n",
      "Iteration 3655, loss = 0.00137581\n",
      "Iteration 3656, loss = 0.00137480\n",
      "Iteration 3657, loss = 0.00137379\n",
      "Iteration 3658, loss = 0.00137278\n",
      "Iteration 3659, loss = 0.00137177\n",
      "Iteration 3660, loss = 0.00137076\n",
      "Iteration 3661, loss = 0.00136975\n",
      "Iteration 3662, loss = 0.00136874\n",
      "Iteration 3663, loss = 0.00136774\n",
      "Iteration 3664, loss = 0.00136674\n",
      "Iteration 3665, loss = 0.00136573\n",
      "Iteration 3666, loss = 0.00136473\n",
      "Iteration 3667, loss = 0.00136373\n",
      "Iteration 3668, loss = 0.00136273\n",
      "Iteration 3669, loss = 0.00136173\n",
      "Iteration 3670, loss = 0.00136073\n",
      "Iteration 3671, loss = 0.00135973\n",
      "Iteration 3672, loss = 0.00135873\n",
      "Iteration 3673, loss = 0.00135774\n",
      "Iteration 3674, loss = 0.00135674\n",
      "Iteration 3675, loss = 0.00135575\n",
      "Iteration 3676, loss = 0.00135475\n",
      "Iteration 3677, loss = 0.00135376\n",
      "Iteration 3678, loss = 0.00135277\n",
      "Iteration 3679, loss = 0.00135178\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      0.94      0.97        16\n",
      "           3       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.97        36\n",
      "   macro avg       0.96      0.98      0.97        36\n",
      "weighted avg       0.98      0.97      0.97        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#原始資料標準化\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# hidden_layers = (512,) # one hidden layer\n",
    "# activation = ’relu’ # the default\n",
    "hidden_layers = (30,)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers, verbose = True, \\\n",
    "activation = activation, tol = 1e-6, max_iter = int(1e6))\n",
    "solver = 'adam' # default solver\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(X_train_scaled, y_train)\n",
    "predictions = clf_MLP.predict(X_test_scaled)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### **畫出測試資料的confusion matrix**\n",
    "這段程式碼使用了Scikit-learn中的ConfusionMatrixDisplay模組和Matplotlib來繪製混淆矩陣。讓我們一步步解釋：\n",
    "\n",
    "1. `from sklearn.metrics import ConfusionMatrixDisplay`: 導入ConfusionMatrixDisplay模組，該模組提供了一個便捷的方法來可視化混淆矩陣。\n",
    "\n",
    "2. `import matplotlib.pyplot as plt`: 導入Matplotlib庫，用於繪製圖表。\n",
    "\n",
    "3. `fig, ax = plt.subplots(1, 1, figsize=(12,12))`: 創建一個12x12大小的子圖，用於顯示混淆矩陣。\n",
    "\n",
    "4. `score = 100*clf_MLP.score(X_test_scaled, y_test)`: 使用MLP模型的score方法計算測試數據的準確率，並將其轉換為百分比形式。\n",
    "\n",
    "5. `title = 'Testing score ={:.2f}%'.format(score)`: 根據測試準確率創建標題，並將準確率插入標題字符串中。\n",
    "\n",
    "6. `disp = ConfusionMatrixDisplay.from_estimator`: 使用ConfusionMatrixDisplay模組的from_estimator方法來創建混淆矩陣的可視化。它需要以下參數：\n",
    "   - `clf_MLP`: 要評估的分類器（MLP模型）。\n",
    "   - `X_test_scaled`: 測試數據。\n",
    "   - `y_test`: 測試數據的真實標籤。\n",
    "   - `xticks_rotation=45`: x軸刻度標籤的旋轉角度，這裡設置為45度。\n",
    "   - `cmap=plt.cm.Blues`: 顏色映射，用於設置混淆矩陣的顏色。\n",
    "   - `normalize='true'`: 是否對混淆矩陣進行正規化，這裡設置為true，表示對每一行進行正規化。\n",
    "   - `ax=ax`: 指定子圖來繪製混淆矩陣。\n",
    "\n",
    "7. `disp.ax_.set_title(title)`: 設置子圖的標題為剛剛創建的標題字符串。\n",
    "\n",
    "8. `plt.show()`: 顯示混淆矩陣圖表。\n",
    "\n",
    "總的來說，這段程式碼用於創建並顯示測試數據的混淆矩陣，並在圖表中包含了測試準確率的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,12))\n",
    "score = 100*clf_MLP.score(X_test_scaled, y_test)\n",
    "title = 'Testing score ={:.2f}%'.format(score)\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "clf_MLP,\n",
    "X_test_scaled,\n",
    "y_test,\n",
    "xticks_rotation=45, #’vertical’,\n",
    "# display_labels=class_names,\n",
    "cmap=plt.cm.Blues,\n",
    "normalize='true',\n",
    "ax = ax\n",
    ")\n",
    "disp.ax_.set_title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.17598776\n",
      "Iteration 2, loss = 1.16833226\n",
      "Iteration 3, loss = 1.16084005\n",
      "Iteration 4, loss = 1.15351582\n",
      "Iteration 5, loss = 1.14636394\n",
      "Iteration 6, loss = 1.13938836\n",
      "Iteration 7, loss = 1.13259255\n",
      "Iteration 8, loss = 1.12597940\n",
      "Iteration 9, loss = 1.11955125\n",
      "Iteration 10, loss = 1.11330991\n",
      "Iteration 11, loss = 1.10725661\n",
      "Iteration 12, loss = 1.10139198\n",
      "Iteration 13, loss = 1.09571593\n",
      "Iteration 14, loss = 1.09022757\n",
      "Iteration 15, loss = 1.08492518\n",
      "Iteration 16, loss = 1.07980620\n",
      "Iteration 17, loss = 1.07486721\n",
      "Iteration 18, loss = 1.07010399\n",
      "Iteration 19, loss = 1.06551144\n",
      "Iteration 20, loss = 1.06108366\n",
      "Iteration 21, loss = 1.05681396\n",
      "Iteration 22, loss = 1.05269487\n",
      "Iteration 23, loss = 1.04871824\n",
      "Iteration 24, loss = 1.04487531\n",
      "Iteration 25, loss = 1.04115682\n",
      "Iteration 26, loss = 1.03755315\n",
      "Iteration 27, loss = 1.03405439\n",
      "Iteration 28, loss = 1.03065055\n",
      "Iteration 29, loss = 1.02733162\n",
      "Iteration 30, loss = 1.02408776\n",
      "Iteration 31, loss = 1.02090937\n",
      "Iteration 32, loss = 1.01778724\n",
      "Iteration 33, loss = 1.01471265\n",
      "Iteration 34, loss = 1.01167741\n",
      "Iteration 35, loss = 1.00867395\n",
      "Iteration 36, loss = 1.00569536\n",
      "Iteration 37, loss = 1.00273540\n",
      "Iteration 38, loss = 0.99978850\n",
      "Iteration 39, loss = 0.99684979\n",
      "Iteration 40, loss = 0.99391501\n",
      "Iteration 41, loss = 0.99098054\n",
      "Iteration 42, loss = 0.98804330\n",
      "Iteration 43, loss = 0.98510076\n",
      "Iteration 44, loss = 0.98215083\n",
      "Iteration 45, loss = 0.97919188\n",
      "Iteration 46, loss = 0.97622260\n",
      "Iteration 47, loss = 0.97324204\n",
      "Iteration 48, loss = 0.97024950\n",
      "Iteration 49, loss = 0.96724453\n",
      "Iteration 50, loss = 0.96422687\n",
      "Iteration 51, loss = 0.96119638\n",
      "Iteration 52, loss = 0.95815311\n",
      "Iteration 53, loss = 0.95509713\n",
      "Iteration 54, loss = 0.95202864\n",
      "Iteration 55, loss = 0.94894786\n",
      "Iteration 56, loss = 0.94585503\n",
      "Iteration 57, loss = 0.94275043\n",
      "Iteration 58, loss = 0.93963430\n",
      "Iteration 59, loss = 0.93650691\n",
      "Iteration 60, loss = 0.93336848\n",
      "Iteration 61, loss = 0.93021922\n",
      "Iteration 62, loss = 0.92705932\n",
      "Iteration 63, loss = 0.92388893\n",
      "Iteration 64, loss = 0.92070817\n",
      "Iteration 65, loss = 0.91751715\n",
      "Iteration 66, loss = 0.91431594\n",
      "Iteration 67, loss = 0.91110461\n",
      "Iteration 68, loss = 0.90788317\n",
      "Iteration 69, loss = 0.90465167\n",
      "Iteration 70, loss = 0.90141012\n",
      "Iteration 71, loss = 0.89815852\n",
      "Iteration 72, loss = 0.89489688\n",
      "Iteration 73, loss = 0.89162522\n",
      "Iteration 74, loss = 0.88834355\n",
      "Iteration 75, loss = 0.88505190\n",
      "Iteration 76, loss = 0.88175029\n",
      "Iteration 77, loss = 0.87843877\n",
      "Iteration 78, loss = 0.87511741\n",
      "Iteration 79, loss = 0.87178629\n",
      "Iteration 80, loss = 0.86844548\n",
      "Iteration 81, loss = 0.86509512\n",
      "Iteration 82, loss = 0.86173531\n",
      "Iteration 83, loss = 0.85836620\n",
      "Iteration 84, loss = 0.85498796\n",
      "Iteration 85, loss = 0.85160077\n",
      "Iteration 86, loss = 0.84820480\n",
      "Iteration 87, loss = 0.84480026\n",
      "Iteration 88, loss = 0.84138739\n",
      "Iteration 89, loss = 0.83796639\n",
      "Iteration 90, loss = 0.83453753\n",
      "Iteration 91, loss = 0.83110104\n",
      "Iteration 92, loss = 0.82765719\n",
      "Iteration 93, loss = 0.82420626\n",
      "Iteration 94, loss = 0.82074850\n",
      "Iteration 95, loss = 0.81728422\n",
      "Iteration 96, loss = 0.81381370\n",
      "Iteration 97, loss = 0.81033723\n",
      "Iteration 98, loss = 0.80685512\n",
      "Iteration 99, loss = 0.80336765\n",
      "Iteration 100, loss = 0.79987514\n",
      "Iteration 101, loss = 0.79637790\n",
      "Iteration 102, loss = 0.79287623\n",
      "Iteration 103, loss = 0.78937046\n",
      "Iteration 104, loss = 0.78586089\n",
      "Iteration 105, loss = 0.78234784\n",
      "Iteration 106, loss = 0.77883162\n",
      "Iteration 107, loss = 0.77531257\n",
      "Iteration 108, loss = 0.77179099\n",
      "Iteration 109, loss = 0.76826721\n",
      "Iteration 110, loss = 0.76474156\n",
      "Iteration 111, loss = 0.76121435\n",
      "Iteration 112, loss = 0.75768591\n",
      "Iteration 113, loss = 0.75415657\n",
      "Iteration 114, loss = 0.75062664\n",
      "Iteration 115, loss = 0.74709645\n",
      "Iteration 116, loss = 0.74356633\n",
      "Iteration 117, loss = 0.74003660\n",
      "Iteration 118, loss = 0.73650759\n",
      "Iteration 119, loss = 0.73297962\n",
      "Iteration 120, loss = 0.72945301\n",
      "Iteration 121, loss = 0.72592809\n",
      "Iteration 122, loss = 0.72240517\n",
      "Iteration 123, loss = 0.71888458\n",
      "Iteration 124, loss = 0.71536664\n",
      "Iteration 125, loss = 0.71185165\n",
      "Iteration 126, loss = 0.70833995\n",
      "Iteration 127, loss = 0.70483183\n",
      "Iteration 128, loss = 0.70132761\n",
      "Iteration 129, loss = 0.69782761\n",
      "Iteration 130, loss = 0.69433211\n",
      "Iteration 131, loss = 0.69084144\n",
      "Iteration 132, loss = 0.68735589\n",
      "Iteration 133, loss = 0.68387576\n",
      "Iteration 134, loss = 0.68040135\n",
      "Iteration 135, loss = 0.67693293\n",
      "Iteration 136, loss = 0.67347081\n",
      "Iteration 137, loss = 0.67001527\n",
      "Iteration 138, loss = 0.66656659\n",
      "Iteration 139, loss = 0.66312504\n",
      "Iteration 140, loss = 0.65969090\n",
      "Iteration 141, loss = 0.65626444\n",
      "Iteration 142, loss = 0.65284593\n",
      "Iteration 143, loss = 0.64943562\n",
      "Iteration 144, loss = 0.64603378\n",
      "Iteration 145, loss = 0.64264065\n",
      "Iteration 146, loss = 0.63925648\n",
      "Iteration 147, loss = 0.63588152\n",
      "Iteration 148, loss = 0.63251601\n",
      "Iteration 149, loss = 0.62916017\n",
      "Iteration 150, loss = 0.62581425\n",
      "Iteration 151, loss = 0.62247847\n",
      "Iteration 152, loss = 0.61915304\n",
      "Iteration 153, loss = 0.61583818\n",
      "Iteration 154, loss = 0.61253410\n",
      "Iteration 155, loss = 0.60924101\n",
      "Iteration 156, loss = 0.60595911\n",
      "Iteration 157, loss = 0.60268859\n",
      "Iteration 158, loss = 0.59942965\n",
      "Iteration 159, loss = 0.59618247\n",
      "Iteration 160, loss = 0.59294723\n",
      "Iteration 161, loss = 0.58972410\n",
      "Iteration 162, loss = 0.58651326\n",
      "Iteration 163, loss = 0.58331488\n",
      "Iteration 164, loss = 0.58012910\n",
      "Iteration 165, loss = 0.57695610\n",
      "Iteration 166, loss = 0.57379601\n",
      "Iteration 167, loss = 0.57064899\n",
      "Iteration 168, loss = 0.56751517\n",
      "Iteration 169, loss = 0.56439469\n",
      "Iteration 170, loss = 0.56128769\n",
      "Iteration 171, loss = 0.55819428\n",
      "Iteration 172, loss = 0.55511459\n",
      "Iteration 173, loss = 0.55204874\n",
      "Iteration 174, loss = 0.54899683\n",
      "Iteration 175, loss = 0.54595898\n",
      "Iteration 176, loss = 0.54293528\n",
      "Iteration 177, loss = 0.53992584\n",
      "Iteration 178, loss = 0.53693074\n",
      "Iteration 179, loss = 0.53395008\n",
      "Iteration 180, loss = 0.53098394\n",
      "Iteration 181, loss = 0.52803240\n",
      "Iteration 182, loss = 0.52509554\n",
      "Iteration 183, loss = 0.52217343\n",
      "Iteration 184, loss = 0.51926614\n",
      "Iteration 185, loss = 0.51637372\n",
      "Iteration 186, loss = 0.51349625\n",
      "Iteration 187, loss = 0.51063377\n",
      "Iteration 188, loss = 0.50778634\n",
      "Iteration 189, loss = 0.50495400\n",
      "Iteration 190, loss = 0.50213681\n",
      "Iteration 191, loss = 0.49933479\n",
      "Iteration 192, loss = 0.49654799\n",
      "Iteration 193, loss = 0.49377645\n",
      "Iteration 194, loss = 0.49102019\n",
      "Iteration 195, loss = 0.48827923\n",
      "Iteration 196, loss = 0.48555361\n",
      "Iteration 197, loss = 0.48284334\n",
      "Iteration 198, loss = 0.48014845\n",
      "Iteration 199, loss = 0.47746894\n",
      "Iteration 200, loss = 0.47480482\n",
      "Iteration 201, loss = 0.47215612\n",
      "Iteration 202, loss = 0.46952282\n",
      "Iteration 203, loss = 0.46690493\n",
      "Iteration 204, loss = 0.46430247\n",
      "Iteration 205, loss = 0.46171541\n",
      "Iteration 206, loss = 0.45914376\n",
      "Iteration 207, loss = 0.45658751\n",
      "Iteration 208, loss = 0.45404665\n",
      "Iteration 209, loss = 0.45152117\n",
      "Iteration 210, loss = 0.44901106\n",
      "Iteration 211, loss = 0.44651629\n",
      "Iteration 212, loss = 0.44403685\n",
      "Iteration 213, loss = 0.44157272\n",
      "Iteration 214, loss = 0.43912387\n",
      "Iteration 215, loss = 0.43669028\n",
      "Iteration 216, loss = 0.43427193\n",
      "Iteration 217, loss = 0.43186878\n",
      "Iteration 218, loss = 0.42948081\n",
      "Iteration 219, loss = 0.42710798\n",
      "Iteration 220, loss = 0.42475026\n",
      "Iteration 221, loss = 0.42240761\n",
      "Iteration 222, loss = 0.42008000\n",
      "Iteration 223, loss = 0.41776738\n",
      "Iteration 224, loss = 0.41546973\n",
      "Iteration 225, loss = 0.41318699\n",
      "Iteration 226, loss = 0.41091912\n",
      "Iteration 227, loss = 0.40866608\n",
      "Iteration 228, loss = 0.40642782\n",
      "Iteration 229, loss = 0.40420430\n",
      "Iteration 230, loss = 0.40199547\n",
      "Iteration 231, loss = 0.39980128\n",
      "Iteration 232, loss = 0.39762167\n",
      "Iteration 233, loss = 0.39545660\n",
      "Iteration 234, loss = 0.39330602\n",
      "Iteration 235, loss = 0.39116986\n",
      "Iteration 236, loss = 0.38904808\n",
      "Iteration 237, loss = 0.38694062\n",
      "Iteration 238, loss = 0.38484741\n",
      "Iteration 239, loss = 0.38276842\n",
      "Iteration 240, loss = 0.38070356\n",
      "Iteration 241, loss = 0.37865280\n",
      "Iteration 242, loss = 0.37661605\n",
      "Iteration 243, loss = 0.37459328\n",
      "Iteration 244, loss = 0.37258440\n",
      "Iteration 245, loss = 0.37058937\n",
      "Iteration 246, loss = 0.36860811\n",
      "Iteration 247, loss = 0.36664056\n",
      "Iteration 248, loss = 0.36468665\n",
      "Iteration 249, loss = 0.36274633\n",
      "Iteration 250, loss = 0.36081953\n",
      "Iteration 251, loss = 0.35890617\n",
      "Iteration 252, loss = 0.35700619\n",
      "Iteration 253, loss = 0.35511953\n",
      "Iteration 254, loss = 0.35324611\n",
      "Iteration 255, loss = 0.35138586\n",
      "Iteration 256, loss = 0.34953872\n",
      "Iteration 257, loss = 0.34770462\n",
      "Iteration 258, loss = 0.34588348\n",
      "Iteration 259, loss = 0.34407524\n",
      "Iteration 260, loss = 0.34227982\n",
      "Iteration 261, loss = 0.34049715\n",
      "Iteration 262, loss = 0.33872716\n",
      "Iteration 263, loss = 0.33696978\n",
      "Iteration 264, loss = 0.33522493\n",
      "Iteration 265, loss = 0.33349254\n",
      "Iteration 266, loss = 0.33177254\n",
      "Iteration 267, loss = 0.33006486\n",
      "Iteration 268, loss = 0.32836942\n",
      "Iteration 269, loss = 0.32668614\n",
      "Iteration 270, loss = 0.32501496\n",
      "Iteration 271, loss = 0.32335579\n",
      "Iteration 272, loss = 0.32170857\n",
      "Iteration 273, loss = 0.32007322\n",
      "Iteration 274, loss = 0.31844966\n",
      "Iteration 275, loss = 0.31683782\n",
      "Iteration 276, loss = 0.31523762\n",
      "Iteration 277, loss = 0.31364899\n",
      "Iteration 278, loss = 0.31207185\n",
      "Iteration 279, loss = 0.31050613\n",
      "Iteration 280, loss = 0.30895175\n",
      "Iteration 281, loss = 0.30740863\n",
      "Iteration 282, loss = 0.30587671\n",
      "Iteration 283, loss = 0.30435590\n",
      "Iteration 284, loss = 0.30284613\n",
      "Iteration 285, loss = 0.30134732\n",
      "Iteration 286, loss = 0.29985940\n",
      "Iteration 287, loss = 0.29838230\n",
      "Iteration 288, loss = 0.29691593\n",
      "Iteration 289, loss = 0.29546022\n",
      "Iteration 290, loss = 0.29401510\n",
      "Iteration 291, loss = 0.29258049\n",
      "Iteration 292, loss = 0.29115632\n",
      "Iteration 293, loss = 0.28974251\n",
      "Iteration 294, loss = 0.28833899\n",
      "Iteration 295, loss = 0.28694568\n",
      "Iteration 296, loss = 0.28556250\n",
      "Iteration 297, loss = 0.28418939\n",
      "Iteration 298, loss = 0.28282627\n",
      "Iteration 299, loss = 0.28147307\n",
      "Iteration 300, loss = 0.28012971\n",
      "Iteration 301, loss = 0.27879611\n",
      "Iteration 302, loss = 0.27747222\n",
      "Iteration 303, loss = 0.27615794\n",
      "Iteration 304, loss = 0.27485322\n",
      "Iteration 305, loss = 0.27355797\n",
      "Iteration 306, loss = 0.27227213\n",
      "Iteration 307, loss = 0.27099562\n",
      "Iteration 308, loss = 0.26972837\n",
      "Iteration 309, loss = 0.26847031\n",
      "Iteration 310, loss = 0.26722138\n",
      "Iteration 311, loss = 0.26598149\n",
      "Iteration 312, loss = 0.26475058\n",
      "Iteration 313, loss = 0.26352857\n",
      "Iteration 314, loss = 0.26231541\n",
      "Iteration 315, loss = 0.26111101\n",
      "Iteration 316, loss = 0.25991532\n",
      "Iteration 317, loss = 0.25872825\n",
      "Iteration 318, loss = 0.25754975\n",
      "Iteration 319, loss = 0.25637974\n",
      "Iteration 320, loss = 0.25521816\n",
      "Iteration 321, loss = 0.25406494\n",
      "Iteration 322, loss = 0.25292001\n",
      "Iteration 323, loss = 0.25178330\n",
      "Iteration 324, loss = 0.25065476\n",
      "Iteration 325, loss = 0.24953431\n",
      "Iteration 326, loss = 0.24842189\n",
      "Iteration 327, loss = 0.24731743\n",
      "Iteration 328, loss = 0.24622087\n",
      "Iteration 329, loss = 0.24513214\n",
      "Iteration 330, loss = 0.24405119\n",
      "Iteration 331, loss = 0.24297794\n",
      "Iteration 332, loss = 0.24191233\n",
      "Iteration 333, loss = 0.24085431\n",
      "Iteration 334, loss = 0.23980381\n",
      "Iteration 335, loss = 0.23876076\n",
      "Iteration 336, loss = 0.23772511\n",
      "Iteration 337, loss = 0.23669679\n",
      "Iteration 338, loss = 0.23567575\n",
      "Iteration 339, loss = 0.23466192\n",
      "Iteration 340, loss = 0.23365524\n",
      "Iteration 341, loss = 0.23265566\n",
      "Iteration 342, loss = 0.23166312\n",
      "Iteration 343, loss = 0.23067755\n",
      "Iteration 344, loss = 0.22969890\n",
      "Iteration 345, loss = 0.22872711\n",
      "Iteration 346, loss = 0.22776213\n",
      "Iteration 347, loss = 0.22680390\n",
      "Iteration 348, loss = 0.22585235\n",
      "Iteration 349, loss = 0.22490744\n",
      "Iteration 350, loss = 0.22396911\n",
      "Iteration 351, loss = 0.22303731\n",
      "Iteration 352, loss = 0.22211197\n",
      "Iteration 353, loss = 0.22119305\n",
      "Iteration 354, loss = 0.22028050\n",
      "Iteration 355, loss = 0.21937425\n",
      "Iteration 356, loss = 0.21847425\n",
      "Iteration 357, loss = 0.21758046\n",
      "Iteration 358, loss = 0.21669282\n",
      "Iteration 359, loss = 0.21581127\n",
      "Iteration 360, loss = 0.21493577\n",
      "Iteration 361, loss = 0.21406627\n",
      "Iteration 362, loss = 0.21320272\n",
      "Iteration 363, loss = 0.21234506\n",
      "Iteration 364, loss = 0.21149324\n",
      "Iteration 365, loss = 0.21064723\n",
      "Iteration 366, loss = 0.20980696\n",
      "Iteration 367, loss = 0.20897238\n",
      "Iteration 368, loss = 0.20814346\n",
      "Iteration 369, loss = 0.20732015\n",
      "Iteration 370, loss = 0.20650239\n",
      "Iteration 371, loss = 0.20569014\n",
      "Iteration 372, loss = 0.20488335\n",
      "Iteration 373, loss = 0.20408197\n",
      "Iteration 374, loss = 0.20328597\n",
      "Iteration 375, loss = 0.20249530\n",
      "Iteration 376, loss = 0.20170990\n",
      "Iteration 377, loss = 0.20092974\n",
      "Iteration 378, loss = 0.20015477\n",
      "Iteration 379, loss = 0.19938495\n",
      "Iteration 380, loss = 0.19862024\n",
      "Iteration 381, loss = 0.19786058\n",
      "Iteration 382, loss = 0.19710595\n",
      "Iteration 383, loss = 0.19635629\n",
      "Iteration 384, loss = 0.19561157\n",
      "Iteration 385, loss = 0.19487174\n",
      "Iteration 386, loss = 0.19413676\n",
      "Iteration 387, loss = 0.19340659\n",
      "Iteration 388, loss = 0.19268120\n",
      "Iteration 389, loss = 0.19196053\n",
      "Iteration 390, loss = 0.19124456\n",
      "Iteration 391, loss = 0.19053324\n",
      "Iteration 392, loss = 0.18982653\n",
      "Iteration 393, loss = 0.18912439\n",
      "Iteration 394, loss = 0.18842679\n",
      "Iteration 395, loss = 0.18773369\n",
      "Iteration 396, loss = 0.18704505\n",
      "Iteration 397, loss = 0.18636083\n",
      "Iteration 398, loss = 0.18568100\n",
      "Iteration 399, loss = 0.18500552\n",
      "Iteration 400, loss = 0.18433434\n",
      "Iteration 401, loss = 0.18366745\n",
      "Iteration 402, loss = 0.18300480\n",
      "Iteration 403, loss = 0.18234635\n",
      "Iteration 404, loss = 0.18169208\n",
      "Iteration 405, loss = 0.18104194\n",
      "Iteration 406, loss = 0.18039590\n",
      "Iteration 407, loss = 0.17975393\n",
      "Iteration 408, loss = 0.17911599\n",
      "Iteration 409, loss = 0.17848206\n",
      "Iteration 410, loss = 0.17785209\n",
      "Iteration 411, loss = 0.17722606\n",
      "Iteration 412, loss = 0.17660393\n",
      "Iteration 413, loss = 0.17598567\n",
      "Iteration 414, loss = 0.17537125\n",
      "Iteration 415, loss = 0.17476063\n",
      "Iteration 416, loss = 0.17415379\n",
      "Iteration 417, loss = 0.17355070\n",
      "Iteration 418, loss = 0.17295132\n",
      "Iteration 419, loss = 0.17235562\n",
      "Iteration 420, loss = 0.17176357\n",
      "Iteration 421, loss = 0.17117515\n",
      "Iteration 422, loss = 0.17059032\n",
      "Iteration 423, loss = 0.17000905\n",
      "Iteration 424, loss = 0.16943132\n",
      "Iteration 425, loss = 0.16885709\n",
      "Iteration 426, loss = 0.16828635\n",
      "Iteration 427, loss = 0.16771905\n",
      "Iteration 428, loss = 0.16715517\n",
      "Iteration 429, loss = 0.16659468\n",
      "Iteration 430, loss = 0.16603756\n",
      "Iteration 431, loss = 0.16548378\n",
      "Iteration 432, loss = 0.16493332\n",
      "Iteration 433, loss = 0.16438613\n",
      "Iteration 434, loss = 0.16384221\n",
      "Iteration 435, loss = 0.16330151\n",
      "Iteration 436, loss = 0.16276402\n",
      "Iteration 437, loss = 0.16222972\n",
      "Iteration 438, loss = 0.16169856\n",
      "Iteration 439, loss = 0.16117054\n",
      "Iteration 440, loss = 0.16064562\n",
      "Iteration 441, loss = 0.16012378\n",
      "Iteration 442, loss = 0.15960500\n",
      "Iteration 443, loss = 0.15908924\n",
      "Iteration 444, loss = 0.15857650\n",
      "Iteration 445, loss = 0.15806673\n",
      "Iteration 446, loss = 0.15755993\n",
      "Iteration 447, loss = 0.15705606\n",
      "Iteration 448, loss = 0.15655510\n",
      "Iteration 449, loss = 0.15605703\n",
      "Iteration 450, loss = 0.15556182\n",
      "Iteration 451, loss = 0.15506946\n",
      "Iteration 452, loss = 0.15457993\n",
      "Iteration 453, loss = 0.15409319\n",
      "Iteration 454, loss = 0.15360922\n",
      "Iteration 455, loss = 0.15312802\n",
      "Iteration 456, loss = 0.15264954\n",
      "Iteration 457, loss = 0.15217378\n",
      "Iteration 458, loss = 0.15170071\n",
      "Iteration 459, loss = 0.15123031\n",
      "Iteration 460, loss = 0.15076256\n",
      "Iteration 461, loss = 0.15029743\n",
      "Iteration 462, loss = 0.14983492\n",
      "Iteration 463, loss = 0.14937499\n",
      "Iteration 464, loss = 0.14891763\n",
      "Iteration 465, loss = 0.14846282\n",
      "Iteration 466, loss = 0.14801054\n",
      "Iteration 467, loss = 0.14756076\n",
      "Iteration 468, loss = 0.14711348\n",
      "Iteration 469, loss = 0.14666867\n",
      "Iteration 470, loss = 0.14622630\n",
      "Iteration 471, loss = 0.14578637\n",
      "Iteration 472, loss = 0.14534886\n",
      "Iteration 473, loss = 0.14491373\n",
      "Iteration 474, loss = 0.14448099\n",
      "Iteration 475, loss = 0.14405061\n",
      "Iteration 476, loss = 0.14362256\n",
      "Iteration 477, loss = 0.14319684\n",
      "Iteration 478, loss = 0.14277343\n",
      "Iteration 479, loss = 0.14235230\n",
      "Iteration 480, loss = 0.14193345\n",
      "Iteration 481, loss = 0.14151684\n",
      "Iteration 482, loss = 0.14110248\n",
      "Iteration 483, loss = 0.14069033\n",
      "Iteration 484, loss = 0.14028039\n",
      "Iteration 485, loss = 0.13987264\n",
      "Iteration 486, loss = 0.13946705\n",
      "Iteration 487, loss = 0.13906362\n",
      "Iteration 488, loss = 0.13866233\n",
      "Iteration 489, loss = 0.13826316\n",
      "Iteration 490, loss = 0.13786610\n",
      "Iteration 491, loss = 0.13747112\n",
      "Iteration 492, loss = 0.13707822\n",
      "Iteration 493, loss = 0.13668738\n",
      "Iteration 494, loss = 0.13629859\n",
      "Iteration 495, loss = 0.13591182\n",
      "Iteration 496, loss = 0.13552707\n",
      "Iteration 497, loss = 0.13514432\n",
      "Iteration 498, loss = 0.13476356\n",
      "Iteration 499, loss = 0.13438476\n",
      "Iteration 500, loss = 0.13400792\n",
      "Iteration 501, loss = 0.13363302\n",
      "Iteration 502, loss = 0.13326005\n",
      "Iteration 503, loss = 0.13288899\n",
      "Iteration 504, loss = 0.13251983\n",
      "Iteration 505, loss = 0.13215256\n",
      "Iteration 506, loss = 0.13178716\n",
      "Iteration 507, loss = 0.13142361\n",
      "Iteration 508, loss = 0.13106192\n",
      "Iteration 509, loss = 0.13070205\n",
      "Iteration 510, loss = 0.13034400\n",
      "Iteration 511, loss = 0.12998776\n",
      "Iteration 512, loss = 0.12963331\n",
      "Iteration 513, loss = 0.12928063\n",
      "Iteration 514, loss = 0.12892973\n",
      "Iteration 515, loss = 0.12858058\n",
      "Iteration 516, loss = 0.12823317\n",
      "Iteration 517, loss = 0.12788749\n",
      "Iteration 518, loss = 0.12754352\n",
      "Iteration 519, loss = 0.12720126\n",
      "Iteration 520, loss = 0.12686070\n",
      "Iteration 521, loss = 0.12652181\n",
      "Iteration 522, loss = 0.12618459\n",
      "Iteration 523, loss = 0.12584903\n",
      "Iteration 524, loss = 0.12551512\n",
      "Iteration 525, loss = 0.12518283\n",
      "Iteration 526, loss = 0.12485217\n",
      "Iteration 527, loss = 0.12452312\n",
      "Iteration 528, loss = 0.12419567\n",
      "Iteration 529, loss = 0.12386981\n",
      "Iteration 530, loss = 0.12354553\n",
      "Iteration 531, loss = 0.12322281\n",
      "Iteration 532, loss = 0.12290165\n",
      "Iteration 533, loss = 0.12258203\n",
      "Iteration 534, loss = 0.12226394\n",
      "Iteration 535, loss = 0.12194738\n",
      "Iteration 536, loss = 0.12163233\n",
      "Iteration 537, loss = 0.12131878\n",
      "Iteration 538, loss = 0.12100673\n",
      "Iteration 539, loss = 0.12069616\n",
      "Iteration 540, loss = 0.12038705\n",
      "Iteration 541, loss = 0.12007941\n",
      "Iteration 542, loss = 0.11977322\n",
      "Iteration 543, loss = 0.11946848\n",
      "Iteration 544, loss = 0.11916516\n",
      "Iteration 545, loss = 0.11886326\n",
      "Iteration 546, loss = 0.11856278\n",
      "Iteration 547, loss = 0.11826370\n",
      "Iteration 548, loss = 0.11796602\n",
      "Iteration 549, loss = 0.11766971\n",
      "Iteration 550, loss = 0.11737479\n",
      "Iteration 551, loss = 0.11708122\n",
      "Iteration 552, loss = 0.11678902\n",
      "Iteration 553, loss = 0.11649815\n",
      "Iteration 554, loss = 0.11620863\n",
      "Iteration 555, loss = 0.11592044\n",
      "Iteration 556, loss = 0.11563356\n",
      "Iteration 557, loss = 0.11534800\n",
      "Iteration 558, loss = 0.11506374\n",
      "Iteration 559, loss = 0.11478078\n",
      "Iteration 560, loss = 0.11449909\n",
      "Iteration 561, loss = 0.11421869\n",
      "Iteration 562, loss = 0.11393955\n",
      "Iteration 563, loss = 0.11366168\n",
      "Iteration 564, loss = 0.11338505\n",
      "Iteration 565, loss = 0.11310967\n",
      "Iteration 566, loss = 0.11283552\n",
      "Iteration 567, loss = 0.11256260\n",
      "Iteration 568, loss = 0.11229090\n",
      "Iteration 569, loss = 0.11202041\n",
      "Iteration 570, loss = 0.11175113\n",
      "Iteration 571, loss = 0.11148304\n",
      "Iteration 572, loss = 0.11121613\n",
      "Iteration 573, loss = 0.11095041\n",
      "Iteration 574, loss = 0.11068586\n",
      "Iteration 575, loss = 0.11042247\n",
      "Iteration 576, loss = 0.11016024\n",
      "Iteration 577, loss = 0.10989916\n",
      "Iteration 578, loss = 0.10963922\n",
      "Iteration 579, loss = 0.10938042\n",
      "Iteration 580, loss = 0.10912274\n",
      "Iteration 581, loss = 0.10886619\n",
      "Iteration 582, loss = 0.10861074\n",
      "Iteration 583, loss = 0.10835641\n",
      "Iteration 584, loss = 0.10810317\n",
      "Iteration 585, loss = 0.10785102\n",
      "Iteration 586, loss = 0.10759996\n",
      "Iteration 587, loss = 0.10734998\n",
      "Iteration 588, loss = 0.10710107\n",
      "Iteration 589, loss = 0.10685322\n",
      "Iteration 590, loss = 0.10660643\n",
      "Iteration 591, loss = 0.10636069\n",
      "Iteration 592, loss = 0.10611600\n",
      "Iteration 593, loss = 0.10587234\n",
      "Iteration 594, loss = 0.10562972\n",
      "Iteration 595, loss = 0.10538812\n",
      "Iteration 596, loss = 0.10514754\n",
      "Iteration 597, loss = 0.10490797\n",
      "Iteration 598, loss = 0.10466941\n",
      "Iteration 599, loss = 0.10443184\n",
      "Iteration 600, loss = 0.10419527\n",
      "Iteration 601, loss = 0.10395969\n",
      "Iteration 602, loss = 0.10372509\n",
      "Iteration 603, loss = 0.10349146\n",
      "Iteration 604, loss = 0.10325881\n",
      "Iteration 605, loss = 0.10302711\n",
      "Iteration 606, loss = 0.10279637\n",
      "Iteration 607, loss = 0.10256659\n",
      "Iteration 608, loss = 0.10233775\n",
      "Iteration 609, loss = 0.10210985\n",
      "Iteration 610, loss = 0.10188288\n",
      "Iteration 611, loss = 0.10165684\n",
      "Iteration 612, loss = 0.10143173\n",
      "Iteration 613, loss = 0.10120753\n",
      "Iteration 614, loss = 0.10098424\n",
      "Iteration 615, loss = 0.10076186\n",
      "Iteration 616, loss = 0.10054038\n",
      "Iteration 617, loss = 0.10031979\n",
      "Iteration 618, loss = 0.10010009\n",
      "Iteration 619, loss = 0.09988128\n",
      "Iteration 620, loss = 0.09966335\n",
      "Iteration 621, loss = 0.09944628\n",
      "Iteration 622, loss = 0.09923009\n",
      "Iteration 623, loss = 0.09901476\n",
      "Iteration 624, loss = 0.09880029\n",
      "Iteration 625, loss = 0.09858667\n",
      "Iteration 626, loss = 0.09837390\n",
      "Iteration 627, loss = 0.09816198\n",
      "Iteration 628, loss = 0.09795089\n",
      "Iteration 629, loss = 0.09774063\n",
      "Iteration 630, loss = 0.09753120\n",
      "Iteration 631, loss = 0.09732259\n",
      "Iteration 632, loss = 0.09711480\n",
      "Iteration 633, loss = 0.09690783\n",
      "Iteration 634, loss = 0.09670166\n",
      "Iteration 635, loss = 0.09649630\n",
      "Iteration 636, loss = 0.09629173\n",
      "Iteration 637, loss = 0.09608796\n",
      "Iteration 638, loss = 0.09588498\n",
      "Iteration 639, loss = 0.09568279\n",
      "Iteration 640, loss = 0.09548137\n",
      "Iteration 641, loss = 0.09528074\n",
      "Iteration 642, loss = 0.09508087\n",
      "Iteration 643, loss = 0.09488177\n",
      "Iteration 644, loss = 0.09468343\n",
      "Iteration 645, loss = 0.09448585\n",
      "Iteration 646, loss = 0.09428903\n",
      "Iteration 647, loss = 0.09409295\n",
      "Iteration 648, loss = 0.09389762\n",
      "Iteration 649, loss = 0.09370303\n",
      "Iteration 650, loss = 0.09350918\n",
      "Iteration 651, loss = 0.09331606\n",
      "Iteration 652, loss = 0.09312366\n",
      "Iteration 653, loss = 0.09293199\n",
      "Iteration 654, loss = 0.09274104\n",
      "Iteration 655, loss = 0.09255081\n",
      "Iteration 656, loss = 0.09236129\n",
      "Iteration 657, loss = 0.09217248\n",
      "Iteration 658, loss = 0.09198437\n",
      "Iteration 659, loss = 0.09179696\n",
      "Iteration 660, loss = 0.09161024\n",
      "Iteration 661, loss = 0.09142422\n",
      "Iteration 662, loss = 0.09123888\n",
      "Iteration 663, loss = 0.09105423\n",
      "Iteration 664, loss = 0.09087026\n",
      "Iteration 665, loss = 0.09068697\n",
      "Iteration 666, loss = 0.09050435\n",
      "Iteration 667, loss = 0.09032239\n",
      "Iteration 668, loss = 0.09014111\n",
      "Iteration 669, loss = 0.08996048\n",
      "Iteration 670, loss = 0.08978051\n",
      "Iteration 671, loss = 0.08960120\n",
      "Iteration 672, loss = 0.08942254\n",
      "Iteration 673, loss = 0.08924452\n",
      "Iteration 674, loss = 0.08906715\n",
      "Iteration 675, loss = 0.08889041\n",
      "Iteration 676, loss = 0.08871431\n",
      "Iteration 677, loss = 0.08853885\n",
      "Iteration 678, loss = 0.08836401\n",
      "Iteration 679, loss = 0.08818981\n",
      "Iteration 680, loss = 0.08801622\n",
      "Iteration 681, loss = 0.08784325\n",
      "Iteration 682, loss = 0.08767090\n",
      "Iteration 683, loss = 0.08749916\n",
      "Iteration 684, loss = 0.08732803\n",
      "Iteration 685, loss = 0.08715750\n",
      "Iteration 686, loss = 0.08698758\n",
      "Iteration 687, loss = 0.08681825\n",
      "Iteration 688, loss = 0.08664952\n",
      "Iteration 689, loss = 0.08648139\n",
      "Iteration 690, loss = 0.08631384\n",
      "Iteration 691, loss = 0.08614688\n",
      "Iteration 692, loss = 0.08598050\n",
      "Iteration 693, loss = 0.08581471\n",
      "Iteration 694, loss = 0.08564949\n",
      "Iteration 695, loss = 0.08548484\n",
      "Iteration 696, loss = 0.08532076\n",
      "Iteration 697, loss = 0.08515725\n",
      "Iteration 698, loss = 0.08499431\n",
      "Iteration 699, loss = 0.08483193\n",
      "Iteration 700, loss = 0.08467011\n",
      "Iteration 701, loss = 0.08450884\n",
      "Iteration 702, loss = 0.08434812\n",
      "Iteration 703, loss = 0.08418796\n",
      "Iteration 704, loss = 0.08402834\n",
      "Iteration 705, loss = 0.08386926\n",
      "Iteration 706, loss = 0.08371073\n",
      "Iteration 707, loss = 0.08355273\n",
      "Iteration 708, loss = 0.08339527\n",
      "Iteration 709, loss = 0.08323835\n",
      "Iteration 710, loss = 0.08308195\n",
      "Iteration 711, loss = 0.08292608\n",
      "Iteration 712, loss = 0.08277073\n",
      "Iteration 713, loss = 0.08261591\n",
      "Iteration 714, loss = 0.08246161\n",
      "Iteration 715, loss = 0.08230782\n",
      "Iteration 716, loss = 0.08215454\n",
      "Iteration 717, loss = 0.08200178\n",
      "Iteration 718, loss = 0.08184952\n",
      "Iteration 719, loss = 0.08169777\n",
      "Iteration 720, loss = 0.08154652\n",
      "Iteration 721, loss = 0.08139577\n",
      "Iteration 722, loss = 0.08124552\n",
      "Iteration 723, loss = 0.08109577\n",
      "Iteration 724, loss = 0.08094651\n",
      "Iteration 725, loss = 0.08079773\n",
      "Iteration 726, loss = 0.08064945\n",
      "Iteration 727, loss = 0.08050165\n",
      "Iteration 728, loss = 0.08035433\n",
      "Iteration 729, loss = 0.08020750\n",
      "Iteration 730, loss = 0.08006114\n",
      "Iteration 731, loss = 0.07991526\n",
      "Iteration 732, loss = 0.07976985\n",
      "Iteration 733, loss = 0.07962491\n",
      "Iteration 734, loss = 0.07948044\n",
      "Iteration 735, loss = 0.07933643\n",
      "Iteration 736, loss = 0.07919289\n",
      "Iteration 737, loss = 0.07904981\n",
      "Iteration 738, loss = 0.07890719\n",
      "Iteration 739, loss = 0.07876502\n",
      "Iteration 740, loss = 0.07862331\n",
      "Iteration 741, loss = 0.07848205\n",
      "Iteration 742, loss = 0.07834124\n",
      "Iteration 743, loss = 0.07820088\n",
      "Iteration 744, loss = 0.07806096\n",
      "Iteration 745, loss = 0.07792149\n",
      "Iteration 746, loss = 0.07778246\n",
      "Iteration 747, loss = 0.07764386\n",
      "Iteration 748, loss = 0.07750571\n",
      "Iteration 749, loss = 0.07736798\n",
      "Iteration 750, loss = 0.07723069\n",
      "Iteration 751, loss = 0.07709383\n",
      "Iteration 752, loss = 0.07695740\n",
      "Iteration 753, loss = 0.07682139\n",
      "Iteration 754, loss = 0.07668580\n",
      "Iteration 755, loss = 0.07655064\n",
      "Iteration 756, loss = 0.07641590\n",
      "Iteration 757, loss = 0.07628157\n",
      "Iteration 758, loss = 0.07614766\n",
      "Iteration 759, loss = 0.07601417\n",
      "Iteration 760, loss = 0.07588108\n",
      "Iteration 761, loss = 0.07574841\n",
      "Iteration 762, loss = 0.07561614\n",
      "Iteration 763, loss = 0.07548428\n",
      "Iteration 764, loss = 0.07535282\n",
      "Iteration 765, loss = 0.07522176\n",
      "Iteration 766, loss = 0.07509110\n",
      "Iteration 767, loss = 0.07496084\n",
      "Iteration 768, loss = 0.07483098\n",
      "Iteration 769, loss = 0.07470151\n",
      "Iteration 770, loss = 0.07457243\n",
      "Iteration 771, loss = 0.07444374\n",
      "Iteration 772, loss = 0.07431544\n",
      "Iteration 773, loss = 0.07418753\n",
      "Iteration 774, loss = 0.07406000\n",
      "Iteration 775, loss = 0.07393285\n",
      "Iteration 776, loss = 0.07380608\n",
      "Iteration 777, loss = 0.07367970\n",
      "Iteration 778, loss = 0.07355369\n",
      "Iteration 779, loss = 0.07342805\n",
      "Iteration 780, loss = 0.07330279\n",
      "Iteration 781, loss = 0.07317790\n",
      "Iteration 782, loss = 0.07305339\n",
      "Iteration 783, loss = 0.07292924\n",
      "Iteration 784, loss = 0.07280545\n",
      "Iteration 785, loss = 0.07268204\n",
      "Iteration 786, loss = 0.07255898\n",
      "Iteration 787, loss = 0.07243629\n",
      "Iteration 788, loss = 0.07231395\n",
      "Iteration 789, loss = 0.07219198\n",
      "Iteration 790, loss = 0.07207036\n",
      "Iteration 791, loss = 0.07194910\n",
      "Iteration 792, loss = 0.07182819\n",
      "Iteration 793, loss = 0.07170763\n",
      "Iteration 794, loss = 0.07158742\n",
      "Iteration 795, loss = 0.07146756\n",
      "Iteration 796, loss = 0.07134804\n",
      "Iteration 797, loss = 0.07122887\n",
      "Iteration 798, loss = 0.07111005\n",
      "Iteration 799, loss = 0.07099157\n",
      "Iteration 800, loss = 0.07087342\n",
      "Iteration 801, loss = 0.07075562\n",
      "Iteration 802, loss = 0.07063815\n",
      "Iteration 803, loss = 0.07052102\n",
      "Iteration 804, loss = 0.07040422\n",
      "Iteration 805, loss = 0.07028776\n",
      "Iteration 806, loss = 0.07017163\n",
      "Iteration 807, loss = 0.07005582\n",
      "Iteration 808, loss = 0.06994035\n",
      "Iteration 809, loss = 0.06982520\n",
      "Iteration 810, loss = 0.06971038\n",
      "Iteration 811, loss = 0.06959588\n",
      "Iteration 812, loss = 0.06948170\n",
      "Iteration 813, loss = 0.06936784\n",
      "Iteration 814, loss = 0.06925431\n",
      "Iteration 815, loss = 0.06914109\n",
      "Iteration 816, loss = 0.06902818\n",
      "Iteration 817, loss = 0.06891560\n",
      "Iteration 818, loss = 0.06880332\n",
      "Iteration 819, loss = 0.06869136\n",
      "Iteration 820, loss = 0.06857971\n",
      "Iteration 821, loss = 0.06846837\n",
      "Iteration 822, loss = 0.06835733\n",
      "Iteration 823, loss = 0.06824661\n",
      "Iteration 824, loss = 0.06813618\n",
      "Iteration 825, loss = 0.06802607\n",
      "Iteration 826, loss = 0.06791625\n",
      "Iteration 827, loss = 0.06780674\n",
      "Iteration 828, loss = 0.06769753\n",
      "Iteration 829, loss = 0.06758861\n",
      "Iteration 830, loss = 0.06747999\n",
      "Iteration 831, loss = 0.06737167\n",
      "Iteration 832, loss = 0.06726365\n",
      "Iteration 833, loss = 0.06715591\n",
      "Iteration 834, loss = 0.06704847\n",
      "Iteration 835, loss = 0.06694132\n",
      "Iteration 836, loss = 0.06683446\n",
      "Iteration 837, loss = 0.06672789\n",
      "Iteration 838, loss = 0.06662161\n",
      "Iteration 839, loss = 0.06651561\n",
      "Iteration 840, loss = 0.06640990\n",
      "Iteration 841, loss = 0.06630447\n",
      "Iteration 842, loss = 0.06619932\n",
      "Iteration 843, loss = 0.06609445\n",
      "Iteration 844, loss = 0.06598986\n",
      "Iteration 845, loss = 0.06588555\n",
      "Iteration 846, loss = 0.06578152\n",
      "Iteration 847, loss = 0.06567777\n",
      "Iteration 848, loss = 0.06557429\n",
      "Iteration 849, loss = 0.06547108\n",
      "Iteration 850, loss = 0.06536815\n",
      "Iteration 851, loss = 0.06526548\n",
      "Iteration 852, loss = 0.06516309\n",
      "Iteration 853, loss = 0.06506097\n",
      "Iteration 854, loss = 0.06495911\n",
      "Iteration 855, loss = 0.06485753\n",
      "Iteration 856, loss = 0.06475621\n",
      "Iteration 857, loss = 0.06465515\n",
      "Iteration 858, loss = 0.06455435\n",
      "Iteration 859, loss = 0.06445382\n",
      "Iteration 860, loss = 0.06435355\n",
      "Iteration 861, loss = 0.06425354\n",
      "Iteration 862, loss = 0.06415379\n",
      "Iteration 863, loss = 0.06405430\n",
      "Iteration 864, loss = 0.06395507\n",
      "Iteration 865, loss = 0.06385609\n",
      "Iteration 866, loss = 0.06375736\n",
      "Iteration 867, loss = 0.06365889\n",
      "Iteration 868, loss = 0.06356067\n",
      "Iteration 869, loss = 0.06346271\n",
      "Iteration 870, loss = 0.06336499\n",
      "Iteration 871, loss = 0.06326752\n",
      "Iteration 872, loss = 0.06317031\n",
      "Iteration 873, loss = 0.06307334\n",
      "Iteration 874, loss = 0.06297661\n",
      "Iteration 875, loss = 0.06288013\n",
      "Iteration 876, loss = 0.06278390\n",
      "Iteration 877, loss = 0.06268791\n",
      "Iteration 878, loss = 0.06259216\n",
      "Iteration 879, loss = 0.06249666\n",
      "Iteration 880, loss = 0.06240139\n",
      "Iteration 881, loss = 0.06230637\n",
      "Iteration 882, loss = 0.06221158\n",
      "Iteration 883, loss = 0.06211703\n",
      "Iteration 884, loss = 0.06202272\n",
      "Iteration 885, loss = 0.06192864\n",
      "Iteration 886, loss = 0.06183480\n",
      "Iteration 887, loss = 0.06174119\n",
      "Iteration 888, loss = 0.06164781\n",
      "Iteration 889, loss = 0.06155467\n",
      "Iteration 890, loss = 0.06146175\n",
      "Iteration 891, loss = 0.06136907\n",
      "Iteration 892, loss = 0.06127662\n",
      "Iteration 893, loss = 0.06118439\n",
      "Iteration 894, loss = 0.06109239\n",
      "Iteration 895, loss = 0.06100062\n",
      "Iteration 896, loss = 0.06090907\n",
      "Iteration 897, loss = 0.06081775\n",
      "Iteration 898, loss = 0.06072665\n",
      "Iteration 899, loss = 0.06063577\n",
      "Iteration 900, loss = 0.06054512\n",
      "Iteration 901, loss = 0.06045469\n",
      "Iteration 902, loss = 0.06036447\n",
      "Iteration 903, loss = 0.06027448\n",
      "Iteration 904, loss = 0.06018470\n",
      "Iteration 905, loss = 0.06009515\n",
      "Iteration 906, loss = 0.06000581\n",
      "Iteration 907, loss = 0.05991668\n",
      "Iteration 908, loss = 0.05982777\n",
      "Iteration 909, loss = 0.05973907\n",
      "Iteration 910, loss = 0.05965059\n",
      "Iteration 911, loss = 0.05956232\n",
      "Iteration 912, loss = 0.05947426\n",
      "Iteration 913, loss = 0.05938641\n",
      "Iteration 914, loss = 0.05929877\n",
      "Iteration 915, loss = 0.05921134\n",
      "Iteration 916, loss = 0.05912412\n",
      "Iteration 917, loss = 0.05903711\n",
      "Iteration 918, loss = 0.05895030\n",
      "Iteration 919, loss = 0.05886370\n",
      "Iteration 920, loss = 0.05877731\n",
      "Iteration 921, loss = 0.05869112\n",
      "Iteration 922, loss = 0.05860513\n",
      "Iteration 923, loss = 0.05851934\n",
      "Iteration 924, loss = 0.05843376\n",
      "Iteration 925, loss = 0.05834838\n",
      "Iteration 926, loss = 0.05826320\n",
      "Iteration 927, loss = 0.05817821\n",
      "Iteration 928, loss = 0.05809343\n",
      "Iteration 929, loss = 0.05800885\n",
      "Iteration 930, loss = 0.05792446\n",
      "Iteration 931, loss = 0.05784027\n",
      "Iteration 932, loss = 0.05775627\n",
      "Iteration 933, loss = 0.05767247\n",
      "Iteration 934, loss = 0.05758887\n",
      "Iteration 935, loss = 0.05750545\n",
      "Iteration 936, loss = 0.05742224\n",
      "Iteration 937, loss = 0.05733921\n",
      "Iteration 938, loss = 0.05725637\n",
      "Iteration 939, loss = 0.05717373\n",
      "Iteration 940, loss = 0.05709127\n",
      "Iteration 941, loss = 0.05700901\n",
      "Iteration 942, loss = 0.05692693\n",
      "Iteration 943, loss = 0.05684504\n",
      "Iteration 944, loss = 0.05676334\n",
      "Iteration 945, loss = 0.05668182\n",
      "Iteration 946, loss = 0.05660049\n",
      "Iteration 947, loss = 0.05651935\n",
      "Iteration 948, loss = 0.05643839\n",
      "Iteration 949, loss = 0.05635761\n",
      "Iteration 950, loss = 0.05627702\n",
      "Iteration 951, loss = 0.05619660\n",
      "Iteration 952, loss = 0.05611637\n",
      "Iteration 953, loss = 0.05603633\n",
      "Iteration 954, loss = 0.05595646\n",
      "Iteration 955, loss = 0.05587677\n",
      "Iteration 956, loss = 0.05579726\n",
      "Iteration 957, loss = 0.05571792\n",
      "Iteration 958, loss = 0.05563877\n",
      "Iteration 959, loss = 0.05555979\n",
      "Iteration 960, loss = 0.05548099\n",
      "Iteration 961, loss = 0.05540236\n",
      "Iteration 962, loss = 0.05532391\n",
      "Iteration 963, loss = 0.05524564\n",
      "Iteration 964, loss = 0.05516753\n",
      "Iteration 965, loss = 0.05508960\n",
      "Iteration 966, loss = 0.05501185\n",
      "Iteration 967, loss = 0.05493426\n",
      "Iteration 968, loss = 0.05485685\n",
      "Iteration 969, loss = 0.05477960\n",
      "Iteration 970, loss = 0.05470253\n",
      "Iteration 971, loss = 0.05462562\n",
      "Iteration 972, loss = 0.05454889\n",
      "Iteration 973, loss = 0.05447232\n",
      "Iteration 974, loss = 0.05439592\n",
      "Iteration 975, loss = 0.05431969\n",
      "Iteration 976, loss = 0.05424362\n",
      "Iteration 977, loss = 0.05416772\n",
      "Iteration 978, loss = 0.05409198\n",
      "Iteration 979, loss = 0.05401641\n",
      "Iteration 980, loss = 0.05394100\n",
      "Iteration 981, loss = 0.05386575\n",
      "Iteration 982, loss = 0.05379067\n",
      "Iteration 983, loss = 0.05371575\n",
      "Iteration 984, loss = 0.05364099\n",
      "Iteration 985, loss = 0.05356639\n",
      "Iteration 986, loss = 0.05349196\n",
      "Iteration 987, loss = 0.05341768\n",
      "Iteration 988, loss = 0.05334356\n",
      "Iteration 989, loss = 0.05326960\n",
      "Iteration 990, loss = 0.05319580\n",
      "Iteration 991, loss = 0.05312215\n",
      "Iteration 992, loss = 0.05304866\n",
      "Iteration 993, loss = 0.05297533\n",
      "Iteration 994, loss = 0.05290215\n",
      "Iteration 995, loss = 0.05282913\n",
      "Iteration 996, loss = 0.05275627\n",
      "Iteration 997, loss = 0.05268356\n",
      "Iteration 998, loss = 0.05261100\n",
      "Iteration 999, loss = 0.05253859\n",
      "Iteration 1000, loss = 0.05246634\n",
      "Iteration 1001, loss = 0.05239424\n",
      "Iteration 1002, loss = 0.05232229\n",
      "Iteration 1003, loss = 0.05225049\n",
      "Iteration 1004, loss = 0.05217884\n",
      "Iteration 1005, loss = 0.05210734\n",
      "Iteration 1006, loss = 0.05203600\n",
      "Iteration 1007, loss = 0.05196480\n",
      "Iteration 1008, loss = 0.05189374\n",
      "Iteration 1009, loss = 0.05182284\n",
      "Iteration 1010, loss = 0.05175208\n",
      "Iteration 1011, loss = 0.05168147\n",
      "Iteration 1012, loss = 0.05161101\n",
      "Iteration 1013, loss = 0.05154069\n",
      "Iteration 1014, loss = 0.05147052\n",
      "Iteration 1015, loss = 0.05140049\n",
      "Iteration 1016, loss = 0.05133060\n",
      "Iteration 1017, loss = 0.05126086\n",
      "Iteration 1018, loss = 0.05119127\n",
      "Iteration 1019, loss = 0.05112181\n",
      "Iteration 1020, loss = 0.05105250\n",
      "Iteration 1021, loss = 0.05098333\n",
      "Iteration 1022, loss = 0.05091430\n",
      "Iteration 1023, loss = 0.05084541\n",
      "Iteration 1024, loss = 0.05077666\n",
      "Iteration 1025, loss = 0.05070805\n",
      "Iteration 1026, loss = 0.05063958\n",
      "Iteration 1027, loss = 0.05057125\n",
      "Iteration 1028, loss = 0.05050306\n",
      "Iteration 1029, loss = 0.05043500\n",
      "Iteration 1030, loss = 0.05036708\n",
      "Iteration 1031, loss = 0.05029930\n",
      "Iteration 1032, loss = 0.05023166\n",
      "Iteration 1033, loss = 0.05016415\n",
      "Iteration 1034, loss = 0.05009678\n",
      "Iteration 1035, loss = 0.05002954\n",
      "Iteration 1036, loss = 0.04996244\n",
      "Iteration 1037, loss = 0.04989547\n",
      "Iteration 1038, loss = 0.04982863\n",
      "Iteration 1039, loss = 0.04976193\n",
      "Iteration 1040, loss = 0.04969536\n",
      "Iteration 1041, loss = 0.04962893\n",
      "Iteration 1042, loss = 0.04956262\n",
      "Iteration 1043, loss = 0.04949645\n",
      "Iteration 1044, loss = 0.04943041\n",
      "Iteration 1045, loss = 0.04936449\n",
      "Iteration 1046, loss = 0.04929871\n",
      "Iteration 1047, loss = 0.04923306\n",
      "Iteration 1048, loss = 0.04916754\n",
      "Iteration 1049, loss = 0.04910214\n",
      "Iteration 1050, loss = 0.04903688\n",
      "Iteration 1051, loss = 0.04897174\n",
      "Iteration 1052, loss = 0.04890673\n",
      "Iteration 1053, loss = 0.04884185\n",
      "Iteration 1054, loss = 0.04877709\n",
      "Iteration 1055, loss = 0.04871246\n",
      "Iteration 1056, loss = 0.04864796\n",
      "Iteration 1057, loss = 0.04858358\n",
      "Iteration 1058, loss = 0.04851932\n",
      "Iteration 1059, loss = 0.04845520\n",
      "Iteration 1060, loss = 0.04839119\n",
      "Iteration 1061, loss = 0.04832731\n",
      "Iteration 1062, loss = 0.04826355\n",
      "Iteration 1063, loss = 0.04819992\n",
      "Iteration 1064, loss = 0.04813641\n",
      "Iteration 1065, loss = 0.04807302\n",
      "Iteration 1066, loss = 0.04800975\n",
      "Iteration 1067, loss = 0.04794660\n",
      "Iteration 1068, loss = 0.04788358\n",
      "Iteration 1069, loss = 0.04782067\n",
      "Iteration 1070, loss = 0.04775789\n",
      "Iteration 1071, loss = 0.04769522\n",
      "Iteration 1072, loss = 0.04763268\n",
      "Iteration 1073, loss = 0.04757025\n",
      "Iteration 1074, loss = 0.04750795\n",
      "Iteration 1075, loss = 0.04744576\n",
      "Iteration 1076, loss = 0.04738369\n",
      "Iteration 1077, loss = 0.04732173\n",
      "Iteration 1078, loss = 0.04725990\n",
      "Iteration 1079, loss = 0.04719818\n",
      "Iteration 1080, loss = 0.04713657\n",
      "Iteration 1081, loss = 0.04707509\n",
      "Iteration 1082, loss = 0.04701371\n",
      "Iteration 1083, loss = 0.04695246\n",
      "Iteration 1084, loss = 0.04689132\n",
      "Iteration 1085, loss = 0.04683029\n",
      "Iteration 1086, loss = 0.04676938\n",
      "Iteration 1087, loss = 0.04670858\n",
      "Iteration 1088, loss = 0.04664790\n",
      "Iteration 1089, loss = 0.04658732\n",
      "Iteration 1090, loss = 0.04652686\n",
      "Iteration 1091, loss = 0.04646652\n",
      "Iteration 1092, loss = 0.04640628\n",
      "Iteration 1093, loss = 0.04634616\n",
      "Iteration 1094, loss = 0.04628615\n",
      "Iteration 1095, loss = 0.04622625\n",
      "Iteration 1096, loss = 0.04616646\n",
      "Iteration 1097, loss = 0.04610678\n",
      "Iteration 1098, loss = 0.04604721\n",
      "Iteration 1099, loss = 0.04598775\n",
      "Iteration 1100, loss = 0.04592840\n",
      "Iteration 1101, loss = 0.04586916\n",
      "Iteration 1102, loss = 0.04581003\n",
      "Iteration 1103, loss = 0.04575100\n",
      "Iteration 1104, loss = 0.04569209\n",
      "Iteration 1105, loss = 0.04563328\n",
      "Iteration 1106, loss = 0.04557458\n",
      "Iteration 1107, loss = 0.04551598\n",
      "Iteration 1108, loss = 0.04545750\n",
      "Iteration 1109, loss = 0.04539911\n",
      "Iteration 1110, loss = 0.04534084\n",
      "Iteration 1111, loss = 0.04528267\n",
      "Iteration 1112, loss = 0.04522460\n",
      "Iteration 1113, loss = 0.04516665\n",
      "Iteration 1114, loss = 0.04510879\n",
      "Iteration 1115, loss = 0.04505104\n",
      "Iteration 1116, loss = 0.04499339\n",
      "Iteration 1117, loss = 0.04493585\n",
      "Iteration 1118, loss = 0.04487841\n",
      "Iteration 1119, loss = 0.04482108\n",
      "Iteration 1120, loss = 0.04476384\n",
      "Iteration 1121, loss = 0.04470671\n",
      "Iteration 1122, loss = 0.04464968\n",
      "Iteration 1123, loss = 0.04459276\n",
      "Iteration 1124, loss = 0.04453593\n",
      "Iteration 1125, loss = 0.04447921\n",
      "Iteration 1126, loss = 0.04442259\n",
      "Iteration 1127, loss = 0.04436606\n",
      "Iteration 1128, loss = 0.04430964\n",
      "Iteration 1129, loss = 0.04425332\n",
      "Iteration 1130, loss = 0.04419710\n",
      "Iteration 1131, loss = 0.04414097\n",
      "Iteration 1132, loss = 0.04408495\n",
      "Iteration 1133, loss = 0.04402903\n",
      "Iteration 1134, loss = 0.04397320\n",
      "Iteration 1135, loss = 0.04391747\n",
      "Iteration 1136, loss = 0.04386184\n",
      "Iteration 1137, loss = 0.04380631\n",
      "Iteration 1138, loss = 0.04375087\n",
      "Iteration 1139, loss = 0.04369554\n",
      "Iteration 1140, loss = 0.04364029\n",
      "Iteration 1141, loss = 0.04358515\n",
      "Iteration 1142, loss = 0.04353010\n",
      "Iteration 1143, loss = 0.04347515\n",
      "Iteration 1144, loss = 0.04342029\n",
      "Iteration 1145, loss = 0.04336553\n",
      "Iteration 1146, loss = 0.04331086\n",
      "Iteration 1147, loss = 0.04325629\n",
      "Iteration 1148, loss = 0.04320181\n",
      "Iteration 1149, loss = 0.04314743\n",
      "Iteration 1150, loss = 0.04309314\n",
      "Iteration 1151, loss = 0.04303895\n",
      "Iteration 1152, loss = 0.04298484\n",
      "Iteration 1153, loss = 0.04293084\n",
      "Iteration 1154, loss = 0.04287692\n",
      "Iteration 1155, loss = 0.04282310\n",
      "Iteration 1156, loss = 0.04276936\n",
      "Iteration 1157, loss = 0.04271572\n",
      "Iteration 1158, loss = 0.04266218\n",
      "Iteration 1159, loss = 0.04260872\n",
      "Iteration 1160, loss = 0.04255536\n",
      "Iteration 1161, loss = 0.04250208\n",
      "Iteration 1162, loss = 0.04244890\n",
      "Iteration 1163, loss = 0.04239580\n",
      "Iteration 1164, loss = 0.04234280\n",
      "Iteration 1165, loss = 0.04228989\n",
      "Iteration 1166, loss = 0.04223706\n",
      "Iteration 1167, loss = 0.04218433\n",
      "Iteration 1168, loss = 0.04213168\n",
      "Iteration 1169, loss = 0.04207913\n",
      "Iteration 1170, loss = 0.04202666\n",
      "Iteration 1171, loss = 0.04197428\n",
      "Iteration 1172, loss = 0.04192199\n",
      "Iteration 1173, loss = 0.04186978\n",
      "Iteration 1174, loss = 0.04181767\n",
      "Iteration 1175, loss = 0.04176564\n",
      "Iteration 1176, loss = 0.04171370\n",
      "Iteration 1177, loss = 0.04166184\n",
      "Iteration 1178, loss = 0.04161007\n",
      "Iteration 1179, loss = 0.04155839\n",
      "Iteration 1180, loss = 0.04150679\n",
      "Iteration 1181, loss = 0.04145528\n",
      "Iteration 1182, loss = 0.04140385\n",
      "Iteration 1183, loss = 0.04135251\n",
      "Iteration 1184, loss = 0.04130126\n",
      "Iteration 1185, loss = 0.04125008\n",
      "Iteration 1186, loss = 0.04119900\n",
      "Iteration 1187, loss = 0.04114799\n",
      "Iteration 1188, loss = 0.04109708\n",
      "Iteration 1189, loss = 0.04104624\n",
      "Iteration 1190, loss = 0.04099549\n",
      "Iteration 1191, loss = 0.04094482\n",
      "Iteration 1192, loss = 0.04089423\n",
      "Iteration 1193, loss = 0.04084373\n",
      "Iteration 1194, loss = 0.04079331\n",
      "Iteration 1195, loss = 0.04074297\n",
      "Iteration 1196, loss = 0.04069272\n",
      "Iteration 1197, loss = 0.04064254\n",
      "Iteration 1198, loss = 0.04059245\n",
      "Iteration 1199, loss = 0.04054244\n",
      "Iteration 1200, loss = 0.04049251\n",
      "Iteration 1201, loss = 0.04044266\n",
      "Iteration 1202, loss = 0.04039289\n",
      "Iteration 1203, loss = 0.04034320\n",
      "Iteration 1204, loss = 0.04029359\n",
      "Iteration 1205, loss = 0.04024406\n",
      "Iteration 1206, loss = 0.04019461\n",
      "Iteration 1207, loss = 0.04014524\n",
      "Iteration 1208, loss = 0.04009595\n",
      "Iteration 1209, loss = 0.04004674\n",
      "Iteration 1210, loss = 0.03999760\n",
      "Iteration 1211, loss = 0.03994855\n",
      "Iteration 1212, loss = 0.03989957\n",
      "Iteration 1213, loss = 0.03985067\n",
      "Iteration 1214, loss = 0.03980185\n",
      "Iteration 1215, loss = 0.03975311\n",
      "Iteration 1216, loss = 0.03970444\n",
      "Iteration 1217, loss = 0.03965585\n",
      "Iteration 1218, loss = 0.03960734\n",
      "Iteration 1219, loss = 0.03955890\n",
      "Iteration 1220, loss = 0.03951054\n",
      "Iteration 1221, loss = 0.03946226\n",
      "Iteration 1222, loss = 0.03941405\n",
      "Iteration 1223, loss = 0.03936592\n",
      "Iteration 1224, loss = 0.03931786\n",
      "Iteration 1225, loss = 0.03926988\n",
      "Iteration 1226, loss = 0.03922198\n",
      "Iteration 1227, loss = 0.03917415\n",
      "Iteration 1228, loss = 0.03912639\n",
      "Iteration 1229, loss = 0.03907871\n",
      "Iteration 1230, loss = 0.03903110\n",
      "Iteration 1231, loss = 0.03898357\n",
      "Iteration 1232, loss = 0.03893611\n",
      "Iteration 1233, loss = 0.03888872\n",
      "Iteration 1234, loss = 0.03884141\n",
      "Iteration 1235, loss = 0.03879417\n",
      "Iteration 1236, loss = 0.03874700\n",
      "Iteration 1237, loss = 0.03869991\n",
      "Iteration 1238, loss = 0.03865289\n",
      "Iteration 1239, loss = 0.03860594\n",
      "Iteration 1240, loss = 0.03855906\n",
      "Iteration 1241, loss = 0.03851226\n",
      "Iteration 1242, loss = 0.03846552\n",
      "Iteration 1243, loss = 0.03841886\n",
      "Iteration 1244, loss = 0.03837227\n",
      "Iteration 1245, loss = 0.03832575\n",
      "Iteration 1246, loss = 0.03827930\n",
      "Iteration 1247, loss = 0.03823293\n",
      "Iteration 1248, loss = 0.03818662\n",
      "Iteration 1249, loss = 0.03814038\n",
      "Iteration 1250, loss = 0.03809421\n",
      "Iteration 1251, loss = 0.03804812\n",
      "Iteration 1252, loss = 0.03800209\n",
      "Iteration 1253, loss = 0.03795613\n",
      "Iteration 1254, loss = 0.03791024\n",
      "Iteration 1255, loss = 0.03786443\n",
      "Iteration 1256, loss = 0.03781868\n",
      "Iteration 1257, loss = 0.03777299\n",
      "Iteration 1258, loss = 0.03772738\n",
      "Iteration 1259, loss = 0.03768184\n",
      "Iteration 1260, loss = 0.03763636\n",
      "Iteration 1261, loss = 0.03759095\n",
      "Iteration 1262, loss = 0.03754561\n",
      "Iteration 1263, loss = 0.03750034\n",
      "Iteration 1264, loss = 0.03745513\n",
      "Iteration 1265, loss = 0.03741000\n",
      "Iteration 1266, loss = 0.03736493\n",
      "Iteration 1267, loss = 0.03731992\n",
      "Iteration 1268, loss = 0.03727499\n",
      "Iteration 1269, loss = 0.03723011\n",
      "Iteration 1270, loss = 0.03718531\n",
      "Iteration 1271, loss = 0.03714057\n",
      "Iteration 1272, loss = 0.03709590\n",
      "Iteration 1273, loss = 0.03705130\n",
      "Iteration 1274, loss = 0.03700676\n",
      "Iteration 1275, loss = 0.03696228\n",
      "Iteration 1276, loss = 0.03691787\n",
      "Iteration 1277, loss = 0.03687353\n",
      "Iteration 1278, loss = 0.03682925\n",
      "Iteration 1279, loss = 0.03678504\n",
      "Iteration 1280, loss = 0.03674089\n",
      "Iteration 1281, loss = 0.03669680\n",
      "Iteration 1282, loss = 0.03665278\n",
      "Iteration 1283, loss = 0.03660883\n",
      "Iteration 1284, loss = 0.03656493\n",
      "Iteration 1285, loss = 0.03652111\n",
      "Iteration 1286, loss = 0.03647734\n",
      "Iteration 1287, loss = 0.03643364\n",
      "Iteration 1288, loss = 0.03639000\n",
      "Iteration 1289, loss = 0.03634643\n",
      "Iteration 1290, loss = 0.03630292\n",
      "Iteration 1291, loss = 0.03625947\n",
      "Iteration 1292, loss = 0.03621609\n",
      "Iteration 1293, loss = 0.03617277\n",
      "Iteration 1294, loss = 0.03612951\n",
      "Iteration 1295, loss = 0.03608631\n",
      "Iteration 1296, loss = 0.03604318\n",
      "Iteration 1297, loss = 0.03600010\n",
      "Iteration 1298, loss = 0.03595709\n",
      "Iteration 1299, loss = 0.03591414\n",
      "Iteration 1300, loss = 0.03587126\n",
      "Iteration 1301, loss = 0.03582843\n",
      "Iteration 1302, loss = 0.03578567\n",
      "Iteration 1303, loss = 0.03574297\n",
      "Iteration 1304, loss = 0.03570032\n",
      "Iteration 1305, loss = 0.03565774\n",
      "Iteration 1306, loss = 0.03561522\n",
      "Iteration 1307, loss = 0.03557277\n",
      "Iteration 1308, loss = 0.03553037\n",
      "Iteration 1309, loss = 0.03548803\n",
      "Iteration 1310, loss = 0.03544575\n",
      "Iteration 1311, loss = 0.03540354\n",
      "Iteration 1312, loss = 0.03536138\n",
      "Iteration 1313, loss = 0.03531928\n",
      "Iteration 1314, loss = 0.03527724\n",
      "Iteration 1315, loss = 0.03523527\n",
      "Iteration 1316, loss = 0.03519335\n",
      "Iteration 1317, loss = 0.03515149\n",
      "Iteration 1318, loss = 0.03510969\n",
      "Iteration 1319, loss = 0.03506795\n",
      "Iteration 1320, loss = 0.03502627\n",
      "Iteration 1321, loss = 0.03498465\n",
      "Iteration 1322, loss = 0.03494309\n",
      "Iteration 1323, loss = 0.03490159\n",
      "Iteration 1324, loss = 0.03486014\n",
      "Iteration 1325, loss = 0.03481876\n",
      "Iteration 1326, loss = 0.03477743\n",
      "Iteration 1327, loss = 0.03473616\n",
      "Iteration 1328, loss = 0.03469495\n",
      "Iteration 1329, loss = 0.03465380\n",
      "Iteration 1330, loss = 0.03461270\n",
      "Iteration 1331, loss = 0.03457167\n",
      "Iteration 1332, loss = 0.03453069\n",
      "Iteration 1333, loss = 0.03448977\n",
      "Iteration 1334, loss = 0.03444890\n",
      "Iteration 1335, loss = 0.03440810\n",
      "Iteration 1336, loss = 0.03436735\n",
      "Iteration 1337, loss = 0.03432666\n",
      "Iteration 1338, loss = 0.03428602\n",
      "Iteration 1339, loss = 0.03424545\n",
      "Iteration 1340, loss = 0.03420493\n",
      "Iteration 1341, loss = 0.03416446\n",
      "Iteration 1342, loss = 0.03412406\n",
      "Iteration 1343, loss = 0.03408371\n",
      "Iteration 1344, loss = 0.03404342\n",
      "Iteration 1345, loss = 0.03400318\n",
      "Iteration 1346, loss = 0.03396300\n",
      "Iteration 1347, loss = 0.03392288\n",
      "Iteration 1348, loss = 0.03388281\n",
      "Iteration 1349, loss = 0.03384280\n",
      "Iteration 1350, loss = 0.03380285\n",
      "Iteration 1351, loss = 0.03376295\n",
      "Iteration 1352, loss = 0.03372311\n",
      "Iteration 1353, loss = 0.03368333\n",
      "Iteration 1354, loss = 0.03364360\n",
      "Iteration 1355, loss = 0.03360392\n",
      "Iteration 1356, loss = 0.03356430\n",
      "Iteration 1357, loss = 0.03352474\n",
      "Iteration 1358, loss = 0.03348523\n",
      "Iteration 1359, loss = 0.03344578\n",
      "Iteration 1360, loss = 0.03340638\n",
      "Iteration 1361, loss = 0.03336704\n",
      "Iteration 1362, loss = 0.03332776\n",
      "Iteration 1363, loss = 0.03328853\n",
      "Iteration 1364, loss = 0.03324935\n",
      "Iteration 1365, loss = 0.03321023\n",
      "Iteration 1366, loss = 0.03317116\n",
      "Iteration 1367, loss = 0.03313215\n",
      "Iteration 1368, loss = 0.03309320\n",
      "Iteration 1369, loss = 0.03305429\n",
      "Iteration 1370, loss = 0.03301545\n",
      "Iteration 1371, loss = 0.03297665\n",
      "Iteration 1372, loss = 0.03293792\n",
      "Iteration 1373, loss = 0.03289923\n",
      "Iteration 1374, loss = 0.03286060\n",
      "Iteration 1375, loss = 0.03282203\n",
      "Iteration 1376, loss = 0.03278351\n",
      "Iteration 1377, loss = 0.03274504\n",
      "Iteration 1378, loss = 0.03270663\n",
      "Iteration 1379, loss = 0.03266827\n",
      "Iteration 1380, loss = 0.03262997\n",
      "Iteration 1381, loss = 0.03259172\n",
      "Iteration 1382, loss = 0.03255352\n",
      "Iteration 1383, loss = 0.03251537\n",
      "Iteration 1384, loss = 0.03247729\n",
      "Iteration 1385, loss = 0.03243925\n",
      "Iteration 1386, loss = 0.03240127\n",
      "Iteration 1387, loss = 0.03236334\n",
      "Iteration 1388, loss = 0.03232546\n",
      "Iteration 1389, loss = 0.03228764\n",
      "Iteration 1390, loss = 0.03224987\n",
      "Iteration 1391, loss = 0.03221216\n",
      "Iteration 1392, loss = 0.03217449\n",
      "Iteration 1393, loss = 0.03213689\n",
      "Iteration 1394, loss = 0.03209933\n",
      "Iteration 1395, loss = 0.03206183\n",
      "Iteration 1396, loss = 0.03202438\n",
      "Iteration 1397, loss = 0.03198698\n",
      "Iteration 1398, loss = 0.03194963\n",
      "Iteration 1399, loss = 0.03191234\n",
      "Iteration 1400, loss = 0.03187510\n",
      "Iteration 1401, loss = 0.03183792\n",
      "Iteration 1402, loss = 0.03180078\n",
      "Iteration 1403, loss = 0.03176370\n",
      "Iteration 1404, loss = 0.03172667\n",
      "Iteration 1405, loss = 0.03168970\n",
      "Iteration 1406, loss = 0.03165277\n",
      "Iteration 1407, loss = 0.03161590\n",
      "Iteration 1408, loss = 0.03157908\n",
      "Iteration 1409, loss = 0.03154231\n",
      "Iteration 1410, loss = 0.03150560\n",
      "Iteration 1411, loss = 0.03146894\n",
      "Iteration 1412, loss = 0.03143233\n",
      "Iteration 1413, loss = 0.03139577\n",
      "Iteration 1414, loss = 0.03135926\n",
      "Iteration 1415, loss = 0.03132280\n",
      "Iteration 1416, loss = 0.03128640\n",
      "Iteration 1417, loss = 0.03125005\n",
      "Iteration 1418, loss = 0.03121375\n",
      "Iteration 1419, loss = 0.03117750\n",
      "Iteration 1420, loss = 0.03114130\n",
      "Iteration 1421, loss = 0.03110516\n",
      "Iteration 1422, loss = 0.03106906\n",
      "Iteration 1423, loss = 0.03103302\n",
      "Iteration 1424, loss = 0.03099703\n",
      "Iteration 1425, loss = 0.03096109\n",
      "Iteration 1426, loss = 0.03092520\n",
      "Iteration 1427, loss = 0.03088936\n",
      "Iteration 1428, loss = 0.03085357\n",
      "Iteration 1429, loss = 0.03081784\n",
      "Iteration 1430, loss = 0.03078215\n",
      "Iteration 1431, loss = 0.03074652\n",
      "Iteration 1432, loss = 0.03071094\n",
      "Iteration 1433, loss = 0.03067540\n",
      "Iteration 1434, loss = 0.03063992\n",
      "Iteration 1435, loss = 0.03060449\n",
      "Iteration 1436, loss = 0.03056911\n",
      "Iteration 1437, loss = 0.03053378\n",
      "Iteration 1438, loss = 0.03049850\n",
      "Iteration 1439, loss = 0.03046328\n",
      "Iteration 1440, loss = 0.03042810\n",
      "Iteration 1441, loss = 0.03039297\n",
      "Iteration 1442, loss = 0.03035789\n",
      "Iteration 1443, loss = 0.03032286\n",
      "Iteration 1444, loss = 0.03028789\n",
      "Iteration 1445, loss = 0.03025296\n",
      "Iteration 1446, loss = 0.03021808\n",
      "Iteration 1447, loss = 0.03018326\n",
      "Iteration 1448, loss = 0.03014848\n",
      "Iteration 1449, loss = 0.03011375\n",
      "Iteration 1450, loss = 0.03007908\n",
      "Iteration 1451, loss = 0.03004445\n",
      "Iteration 1452, loss = 0.03000987\n",
      "Iteration 1453, loss = 0.02997534\n",
      "Iteration 1454, loss = 0.02994086\n",
      "Iteration 1455, loss = 0.02990644\n",
      "Iteration 1456, loss = 0.02987206\n",
      "Iteration 1457, loss = 0.02983773\n",
      "Iteration 1458, loss = 0.02980344\n",
      "Iteration 1459, loss = 0.02976921\n",
      "Iteration 1460, loss = 0.02973503\n",
      "Iteration 1461, loss = 0.02970090\n",
      "Iteration 1462, loss = 0.02966681\n",
      "Iteration 1463, loss = 0.02963278\n",
      "Iteration 1464, loss = 0.02959879\n",
      "Iteration 1465, loss = 0.02956486\n",
      "Iteration 1466, loss = 0.02953097\n",
      "Iteration 1467, loss = 0.02949713\n",
      "Iteration 1468, loss = 0.02946334\n",
      "Iteration 1469, loss = 0.02942960\n",
      "Iteration 1470, loss = 0.02939590\n",
      "Iteration 1471, loss = 0.02936226\n",
      "Iteration 1472, loss = 0.02932866\n",
      "Iteration 1473, loss = 0.02929511\n",
      "Iteration 1474, loss = 0.02926161\n",
      "Iteration 1475, loss = 0.02922816\n",
      "Iteration 1476, loss = 0.02919476\n",
      "Iteration 1477, loss = 0.02916141\n",
      "Iteration 1478, loss = 0.02912810\n",
      "Iteration 1479, loss = 0.02909484\n",
      "Iteration 1480, loss = 0.02906163\n",
      "Iteration 1481, loss = 0.02902847\n",
      "Iteration 1482, loss = 0.02899535\n",
      "Iteration 1483, loss = 0.02896229\n",
      "Iteration 1484, loss = 0.02892927\n",
      "Iteration 1485, loss = 0.02889629\n",
      "Iteration 1486, loss = 0.02886337\n",
      "Iteration 1487, loss = 0.02883049\n",
      "Iteration 1488, loss = 0.02879766\n",
      "Iteration 1489, loss = 0.02876488\n",
      "Iteration 1490, loss = 0.02873215\n",
      "Iteration 1491, loss = 0.02869946\n",
      "Iteration 1492, loss = 0.02866682\n",
      "Iteration 1493, loss = 0.02863423\n",
      "Iteration 1494, loss = 0.02860168\n",
      "Iteration 1495, loss = 0.02856919\n",
      "Iteration 1496, loss = 0.02853673\n",
      "Iteration 1497, loss = 0.02850433\n",
      "Iteration 1498, loss = 0.02847197\n",
      "Iteration 1499, loss = 0.02843966\n",
      "Iteration 1500, loss = 0.02840740\n",
      "Iteration 1501, loss = 0.02837518\n",
      "Iteration 1502, loss = 0.02834301\n",
      "Iteration 1503, loss = 0.02831088\n",
      "Iteration 1504, loss = 0.02827881\n",
      "Iteration 1505, loss = 0.02824677\n",
      "Iteration 1506, loss = 0.02821479\n",
      "Iteration 1507, loss = 0.02818285\n",
      "Iteration 1508, loss = 0.02815096\n",
      "Iteration 1509, loss = 0.02811911\n",
      "Iteration 1510, loss = 0.02808731\n",
      "Iteration 1511, loss = 0.02805555\n",
      "Iteration 1512, loss = 0.02802385\n",
      "Iteration 1513, loss = 0.02799218\n",
      "Iteration 1514, loss = 0.02796056\n",
      "Iteration 1515, loss = 0.02792899\n",
      "Iteration 1516, loss = 0.02789747\n",
      "Iteration 1517, loss = 0.02786599\n",
      "Iteration 1518, loss = 0.02783455\n",
      "Iteration 1519, loss = 0.02780316\n",
      "Iteration 1520, loss = 0.02777182\n",
      "Iteration 1521, loss = 0.02774052\n",
      "Iteration 1522, loss = 0.02770927\n",
      "Iteration 1523, loss = 0.02767806\n",
      "Iteration 1524, loss = 0.02764689\n",
      "Iteration 1525, loss = 0.02761578\n",
      "Iteration 1526, loss = 0.02758470\n",
      "Iteration 1527, loss = 0.02755367\n",
      "Iteration 1528, loss = 0.02752269\n",
      "Iteration 1529, loss = 0.02749175\n",
      "Iteration 1530, loss = 0.02746086\n",
      "Iteration 1531, loss = 0.02743001\n",
      "Iteration 1532, loss = 0.02739920\n",
      "Iteration 1533, loss = 0.02736844\n",
      "Iteration 1534, loss = 0.02733773\n",
      "Iteration 1535, loss = 0.02730706\n",
      "Iteration 1536, loss = 0.02727643\n",
      "Iteration 1537, loss = 0.02724585\n",
      "Iteration 1538, loss = 0.02721531\n",
      "Iteration 1539, loss = 0.02718481\n",
      "Iteration 1540, loss = 0.02715436\n",
      "Iteration 1541, loss = 0.02712396\n",
      "Iteration 1542, loss = 0.02709359\n",
      "Iteration 1543, loss = 0.02706327\n",
      "Iteration 1544, loss = 0.02703300\n",
      "Iteration 1545, loss = 0.02700277\n",
      "Iteration 1546, loss = 0.02697258\n",
      "Iteration 1547, loss = 0.02694244\n",
      "Iteration 1548, loss = 0.02691233\n",
      "Iteration 1549, loss = 0.02688228\n",
      "Iteration 1550, loss = 0.02685226\n",
      "Iteration 1551, loss = 0.02682229\n",
      "Iteration 1552, loss = 0.02679237\n",
      "Iteration 1553, loss = 0.02676248\n",
      "Iteration 1554, loss = 0.02673264\n",
      "Iteration 1555, loss = 0.02670284\n",
      "Iteration 1556, loss = 0.02667309\n",
      "Iteration 1557, loss = 0.02664337\n",
      "Iteration 1558, loss = 0.02661370\n",
      "Iteration 1559, loss = 0.02658408\n",
      "Iteration 1560, loss = 0.02655449\n",
      "Iteration 1561, loss = 0.02652495\n",
      "Iteration 1562, loss = 0.02649545\n",
      "Iteration 1563, loss = 0.02646600\n",
      "Iteration 1564, loss = 0.02643658\n",
      "Iteration 1565, loss = 0.02640721\n",
      "Iteration 1566, loss = 0.02637788\n",
      "Iteration 1567, loss = 0.02634860\n",
      "Iteration 1568, loss = 0.02631935\n",
      "Iteration 1569, loss = 0.02629015\n",
      "Iteration 1570, loss = 0.02626099\n",
      "Iteration 1571, loss = 0.02623187\n",
      "Iteration 1572, loss = 0.02620279\n",
      "Iteration 1573, loss = 0.02617376\n",
      "Iteration 1574, loss = 0.02614476\n",
      "Iteration 1575, loss = 0.02611581\n",
      "Iteration 1576, loss = 0.02608690\n",
      "Iteration 1577, loss = 0.02605803\n",
      "Iteration 1578, loss = 0.02602921\n",
      "Iteration 1579, loss = 0.02600042\n",
      "Iteration 1580, loss = 0.02597168\n",
      "Iteration 1581, loss = 0.02594298\n",
      "Iteration 1582, loss = 0.02591431\n",
      "Iteration 1583, loss = 0.02588569\n",
      "Iteration 1584, loss = 0.02585712\n",
      "Iteration 1585, loss = 0.02582858\n",
      "Iteration 1586, loss = 0.02580008\n",
      "Iteration 1587, loss = 0.02577162\n",
      "Iteration 1588, loss = 0.02574321\n",
      "Iteration 1589, loss = 0.02571483\n",
      "Iteration 1590, loss = 0.02568650\n",
      "Iteration 1591, loss = 0.02565821\n",
      "Iteration 1592, loss = 0.02562995\n",
      "Iteration 1593, loss = 0.02560174\n",
      "Iteration 1594, loss = 0.02557357\n",
      "Iteration 1595, loss = 0.02554544\n",
      "Iteration 1596, loss = 0.02551735\n",
      "Iteration 1597, loss = 0.02548930\n",
      "Iteration 1598, loss = 0.02546129\n",
      "Iteration 1599, loss = 0.02543332\n",
      "Iteration 1600, loss = 0.02540539\n",
      "Iteration 1601, loss = 0.02537750\n",
      "Iteration 1602, loss = 0.02534965\n",
      "Iteration 1603, loss = 0.02532184\n",
      "Iteration 1604, loss = 0.02529407\n",
      "Iteration 1605, loss = 0.02526634\n",
      "Iteration 1606, loss = 0.02523865\n",
      "Iteration 1607, loss = 0.02521099\n",
      "Iteration 1608, loss = 0.02518338\n",
      "Iteration 1609, loss = 0.02515581\n",
      "Iteration 1610, loss = 0.02512828\n",
      "Iteration 1611, loss = 0.02510078\n",
      "Iteration 1612, loss = 0.02507333\n",
      "Iteration 1613, loss = 0.02504592\n",
      "Iteration 1614, loss = 0.02501854\n",
      "Iteration 1615, loss = 0.02499120\n",
      "Iteration 1616, loss = 0.02496391\n",
      "Iteration 1617, loss = 0.02493665\n",
      "Iteration 1618, loss = 0.02490943\n",
      "Iteration 1619, loss = 0.02488225\n",
      "Iteration 1620, loss = 0.02485511\n",
      "Iteration 1621, loss = 0.02482800\n",
      "Iteration 1622, loss = 0.02480094\n",
      "Iteration 1623, loss = 0.02477391\n",
      "Iteration 1624, loss = 0.02474693\n",
      "Iteration 1625, loss = 0.02471998\n",
      "Iteration 1626, loss = 0.02469307\n",
      "Iteration 1627, loss = 0.02466620\n",
      "Iteration 1628, loss = 0.02463936\n",
      "Iteration 1629, loss = 0.02461257\n",
      "Iteration 1630, loss = 0.02458581\n",
      "Iteration 1631, loss = 0.02455909\n",
      "Iteration 1632, loss = 0.02453241\n",
      "Iteration 1633, loss = 0.02450577\n",
      "Iteration 1634, loss = 0.02447916\n",
      "Iteration 1635, loss = 0.02445260\n",
      "Iteration 1636, loss = 0.02442607\n",
      "Iteration 1637, loss = 0.02439958\n",
      "Iteration 1638, loss = 0.02437312\n",
      "Iteration 1639, loss = 0.02434671\n",
      "Iteration 1640, loss = 0.02432033\n",
      "Iteration 1641, loss = 0.02429399\n",
      "Iteration 1642, loss = 0.02426768\n",
      "Iteration 1643, loss = 0.02424142\n",
      "Iteration 1644, loss = 0.02421519\n",
      "Iteration 1645, loss = 0.02418900\n",
      "Iteration 1646, loss = 0.02416284\n",
      "Iteration 1647, loss = 0.02413673\n",
      "Iteration 1648, loss = 0.02411065\n",
      "Iteration 1649, loss = 0.02408460\n",
      "Iteration 1650, loss = 0.02405860\n",
      "Iteration 1651, loss = 0.02403263\n",
      "Iteration 1652, loss = 0.02400670\n",
      "Iteration 1653, loss = 0.02398080\n",
      "Iteration 1654, loss = 0.02395494\n",
      "Iteration 1655, loss = 0.02392912\n",
      "Iteration 1656, loss = 0.02390334\n",
      "Iteration 1657, loss = 0.02387759\n",
      "Iteration 1658, loss = 0.02385188\n",
      "Iteration 1659, loss = 0.02382620\n",
      "Iteration 1660, loss = 0.02380056\n",
      "Iteration 1661, loss = 0.02377496\n",
      "Iteration 1662, loss = 0.02374939\n",
      "Iteration 1663, loss = 0.02372386\n",
      "Iteration 1664, loss = 0.02369837\n",
      "Iteration 1665, loss = 0.02367291\n",
      "Iteration 1666, loss = 0.02364748\n",
      "Iteration 1667, loss = 0.02362210\n",
      "Iteration 1668, loss = 0.02359675\n",
      "Iteration 1669, loss = 0.02357143\n",
      "Iteration 1670, loss = 0.02354615\n",
      "Iteration 1671, loss = 0.02352091\n",
      "Iteration 1672, loss = 0.02349570\n",
      "Iteration 1673, loss = 0.02347053\n",
      "Iteration 1674, loss = 0.02344539\n",
      "Iteration 1675, loss = 0.02342029\n",
      "Iteration 1676, loss = 0.02339523\n",
      "Iteration 1677, loss = 0.02337020\n",
      "Iteration 1678, loss = 0.02334520\n",
      "Iteration 1679, loss = 0.02332024\n",
      "Iteration 1680, loss = 0.02329532\n",
      "Iteration 1681, loss = 0.02327043\n",
      "Iteration 1682, loss = 0.02324558\n",
      "Iteration 1683, loss = 0.02322076\n",
      "Iteration 1684, loss = 0.02319597\n",
      "Iteration 1685, loss = 0.02317122\n",
      "Iteration 1686, loss = 0.02314651\n",
      "Iteration 1687, loss = 0.02312183\n",
      "Iteration 1688, loss = 0.02309719\n",
      "Iteration 1689, loss = 0.02307257\n",
      "Iteration 1690, loss = 0.02304800\n",
      "Iteration 1691, loss = 0.02302346\n",
      "Iteration 1692, loss = 0.02299895\n",
      "Iteration 1693, loss = 0.02297448\n",
      "Iteration 1694, loss = 0.02295004\n",
      "Iteration 1695, loss = 0.02292564\n",
      "Iteration 1696, loss = 0.02290127\n",
      "Iteration 1697, loss = 0.02287693\n",
      "Iteration 1698, loss = 0.02285263\n",
      "Iteration 1699, loss = 0.02282837\n",
      "Iteration 1700, loss = 0.02280413\n",
      "Iteration 1701, loss = 0.02277994\n",
      "Iteration 1702, loss = 0.02275577\n",
      "Iteration 1703, loss = 0.02273164\n",
      "Iteration 1704, loss = 0.02270754\n",
      "Iteration 1705, loss = 0.02268348\n",
      "Iteration 1706, loss = 0.02265945\n",
      "Iteration 1707, loss = 0.02263545\n",
      "Iteration 1708, loss = 0.02261149\n",
      "Iteration 1709, loss = 0.02258756\n",
      "Iteration 1710, loss = 0.02256367\n",
      "Iteration 1711, loss = 0.02253980\n",
      "Iteration 1712, loss = 0.02251598\n",
      "Iteration 1713, loss = 0.02249218\n",
      "Iteration 1714, loss = 0.02246842\n",
      "Iteration 1715, loss = 0.02244469\n",
      "Iteration 1716, loss = 0.02242099\n",
      "Iteration 1717, loss = 0.02239733\n",
      "Iteration 1718, loss = 0.02237370\n",
      "Iteration 1719, loss = 0.02235010\n",
      "Iteration 1720, loss = 0.02232654\n",
      "Iteration 1721, loss = 0.02230301\n",
      "Iteration 1722, loss = 0.02227951\n",
      "Iteration 1723, loss = 0.02225605\n",
      "Iteration 1724, loss = 0.02223261\n",
      "Iteration 1725, loss = 0.02220921\n",
      "Iteration 1726, loss = 0.02218585\n",
      "Iteration 1727, loss = 0.02216251\n",
      "Iteration 1728, loss = 0.02213921\n",
      "Iteration 1729, loss = 0.02211594\n",
      "Iteration 1730, loss = 0.02209270\n",
      "Iteration 1731, loss = 0.02206949\n",
      "Iteration 1732, loss = 0.02204632\n",
      "Iteration 1733, loss = 0.02202318\n",
      "Iteration 1734, loss = 0.02200007\n",
      "Iteration 1735, loss = 0.02197699\n",
      "Iteration 1736, loss = 0.02195395\n",
      "Iteration 1737, loss = 0.02193093\n",
      "Iteration 1738, loss = 0.02190795\n",
      "Iteration 1739, loss = 0.02188500\n",
      "Iteration 1740, loss = 0.02186209\n",
      "Iteration 1741, loss = 0.02183920\n",
      "Iteration 1742, loss = 0.02181635\n",
      "Iteration 1743, loss = 0.02179352\n",
      "Iteration 1744, loss = 0.02177073\n",
      "Iteration 1745, loss = 0.02174797\n",
      "Iteration 1746, loss = 0.02172524\n",
      "Iteration 1747, loss = 0.02170255\n",
      "Iteration 1748, loss = 0.02167988\n",
      "Iteration 1749, loss = 0.02165725\n",
      "Iteration 1750, loss = 0.02163465\n",
      "Iteration 1751, loss = 0.02161208\n",
      "Iteration 1752, loss = 0.02158953\n",
      "Iteration 1753, loss = 0.02156703\n",
      "Iteration 1754, loss = 0.02154455\n",
      "Iteration 1755, loss = 0.02152210\n",
      "Iteration 1756, loss = 0.02149968\n",
      "Iteration 1757, loss = 0.02147730\n",
      "Iteration 1758, loss = 0.02145494\n",
      "Iteration 1759, loss = 0.02143262\n",
      "Iteration 1760, loss = 0.02141033\n",
      "Iteration 1761, loss = 0.02138806\n",
      "Iteration 1762, loss = 0.02136583\n",
      "Iteration 1763, loss = 0.02134363\n",
      "Iteration 1764, loss = 0.02132146\n",
      "Iteration 1765, loss = 0.02129932\n",
      "Iteration 1766, loss = 0.02127721\n",
      "Iteration 1767, loss = 0.02125513\n",
      "Iteration 1768, loss = 0.02123308\n",
      "Iteration 1769, loss = 0.02121106\n",
      "Iteration 1770, loss = 0.02118907\n",
      "Iteration 1771, loss = 0.02116711\n",
      "Iteration 1772, loss = 0.02114518\n",
      "Iteration 1773, loss = 0.02112328\n",
      "Iteration 1774, loss = 0.02110142\n",
      "Iteration 1775, loss = 0.02107958\n",
      "Iteration 1776, loss = 0.02105777\n",
      "Iteration 1777, loss = 0.02103599\n",
      "Iteration 1778, loss = 0.02101424\n",
      "Iteration 1779, loss = 0.02099252\n",
      "Iteration 1780, loss = 0.02097083\n",
      "Iteration 1781, loss = 0.02094917\n",
      "Iteration 1782, loss = 0.02092754\n",
      "Iteration 1783, loss = 0.02090594\n",
      "Iteration 1784, loss = 0.02088436\n",
      "Iteration 1785, loss = 0.02086282\n",
      "Iteration 1786, loss = 0.02084131\n",
      "Iteration 1787, loss = 0.02081982\n",
      "Iteration 1788, loss = 0.02079837\n",
      "Iteration 1789, loss = 0.02077694\n",
      "Iteration 1790, loss = 0.02075555\n",
      "Iteration 1791, loss = 0.02073418\n",
      "Iteration 1792, loss = 0.02071284\n",
      "Iteration 1793, loss = 0.02069153\n",
      "Iteration 1794, loss = 0.02067025\n",
      "Iteration 1795, loss = 0.02064900\n",
      "Iteration 1796, loss = 0.02062778\n",
      "Iteration 1797, loss = 0.02060658\n",
      "Iteration 1798, loss = 0.02058542\n",
      "Iteration 1799, loss = 0.02056428\n",
      "Iteration 1800, loss = 0.02054317\n",
      "Iteration 1801, loss = 0.02052209\n",
      "Iteration 1802, loss = 0.02050104\n",
      "Iteration 1803, loss = 0.02048002\n",
      "Iteration 1804, loss = 0.02045902\n",
      "Iteration 1805, loss = 0.02043806\n",
      "Iteration 1806, loss = 0.02041712\n",
      "Iteration 1807, loss = 0.02039621\n",
      "Iteration 1808, loss = 0.02037533\n",
      "Iteration 1809, loss = 0.02035448\n",
      "Iteration 1810, loss = 0.02033365\n",
      "Iteration 1811, loss = 0.02031286\n",
      "Iteration 1812, loss = 0.02029209\n",
      "Iteration 1813, loss = 0.02027135\n",
      "Iteration 1814, loss = 0.02025063\n",
      "Iteration 1815, loss = 0.02022995\n",
      "Iteration 1816, loss = 0.02020929\n",
      "Iteration 1817, loss = 0.02018866\n",
      "Iteration 1818, loss = 0.02016806\n",
      "Iteration 1819, loss = 0.02014749\n",
      "Iteration 1820, loss = 0.02012694\n",
      "Iteration 1821, loss = 0.02010642\n",
      "Iteration 1822, loss = 0.02008593\n",
      "Iteration 1823, loss = 0.02006547\n",
      "Iteration 1824, loss = 0.02004503\n",
      "Iteration 1825, loss = 0.02002462\n",
      "Iteration 1826, loss = 0.02000424\n",
      "Iteration 1827, loss = 0.01998389\n",
      "Iteration 1828, loss = 0.01996356\n",
      "Iteration 1829, loss = 0.01994326\n",
      "Iteration 1830, loss = 0.01992299\n",
      "Iteration 1831, loss = 0.01990274\n",
      "Iteration 1832, loss = 0.01988253\n",
      "Iteration 1833, loss = 0.01986233\n",
      "Iteration 1834, loss = 0.01984217\n",
      "Iteration 1835, loss = 0.01982203\n",
      "Iteration 1836, loss = 0.01980192\n",
      "Iteration 1837, loss = 0.01978184\n",
      "Iteration 1838, loss = 0.01976178\n",
      "Iteration 1839, loss = 0.01974175\n",
      "Iteration 1840, loss = 0.01972175\n",
      "Iteration 1841, loss = 0.01970177\n",
      "Iteration 1842, loss = 0.01968182\n",
      "Iteration 1843, loss = 0.01966189\n",
      "Iteration 1844, loss = 0.01964200\n",
      "Iteration 1845, loss = 0.01962213\n",
      "Iteration 1846, loss = 0.01960228\n",
      "Iteration 1847, loss = 0.01958246\n",
      "Iteration 1848, loss = 0.01956267\n",
      "Iteration 1849, loss = 0.01954290\n",
      "Iteration 1850, loss = 0.01952316\n",
      "Iteration 1851, loss = 0.01950345\n",
      "Iteration 1852, loss = 0.01948376\n",
      "Iteration 1853, loss = 0.01946410\n",
      "Iteration 1854, loss = 0.01944447\n",
      "Iteration 1855, loss = 0.01942486\n",
      "Iteration 1856, loss = 0.01940527\n",
      "Iteration 1857, loss = 0.01938571\n",
      "Iteration 1858, loss = 0.01936618\n",
      "Iteration 1859, loss = 0.01934668\n",
      "Iteration 1860, loss = 0.01932720\n",
      "Iteration 1861, loss = 0.01930774\n",
      "Iteration 1862, loss = 0.01928831\n",
      "Iteration 1863, loss = 0.01926891\n",
      "Iteration 1864, loss = 0.01924953\n",
      "Iteration 1865, loss = 0.01923018\n",
      "Iteration 1866, loss = 0.01921085\n",
      "Iteration 1867, loss = 0.01919155\n",
      "Iteration 1868, loss = 0.01917227\n",
      "Iteration 1869, loss = 0.01915302\n",
      "Iteration 1870, loss = 0.01913380\n",
      "Iteration 1871, loss = 0.01911460\n",
      "Iteration 1872, loss = 0.01909542\n",
      "Iteration 1873, loss = 0.01907627\n",
      "Iteration 1874, loss = 0.01905715\n",
      "Iteration 1875, loss = 0.01903805\n",
      "Iteration 1876, loss = 0.01901897\n",
      "Iteration 1877, loss = 0.01899992\n",
      "Iteration 1878, loss = 0.01898090\n",
      "Iteration 1879, loss = 0.01896190\n",
      "Iteration 1880, loss = 0.01894292\n",
      "Iteration 1881, loss = 0.01892397\n",
      "Iteration 1882, loss = 0.01890504\n",
      "Iteration 1883, loss = 0.01888614\n",
      "Iteration 1884, loss = 0.01886727\n",
      "Iteration 1885, loss = 0.01884842\n",
      "Iteration 1886, loss = 0.01882959\n",
      "Iteration 1887, loss = 0.01881079\n",
      "Iteration 1888, loss = 0.01879201\n",
      "Iteration 1889, loss = 0.01877325\n",
      "Iteration 1890, loss = 0.01875452\n",
      "Iteration 1891, loss = 0.01873582\n",
      "Iteration 1892, loss = 0.01871714\n",
      "Iteration 1893, loss = 0.01869848\n",
      "Iteration 1894, loss = 0.01867985\n",
      "Iteration 1895, loss = 0.01866124\n",
      "Iteration 1896, loss = 0.01864266\n",
      "Iteration 1897, loss = 0.01862410\n",
      "Iteration 1898, loss = 0.01860556\n",
      "Iteration 1899, loss = 0.01858705\n",
      "Iteration 1900, loss = 0.01856856\n",
      "Iteration 1901, loss = 0.01855010\n",
      "Iteration 1902, loss = 0.01853166\n",
      "Iteration 1903, loss = 0.01851324\n",
      "Iteration 1904, loss = 0.01849485\n",
      "Iteration 1905, loss = 0.01847648\n",
      "Iteration 1906, loss = 0.01845814\n",
      "Iteration 1907, loss = 0.01843982\n",
      "Iteration 1908, loss = 0.01842152\n",
      "Iteration 1909, loss = 0.01840324\n",
      "Iteration 1910, loss = 0.01838499\n",
      "Iteration 1911, loss = 0.01836677\n",
      "Iteration 1912, loss = 0.01834856\n",
      "Iteration 1913, loss = 0.01833038\n",
      "Iteration 1914, loss = 0.01831223\n",
      "Iteration 1915, loss = 0.01829409\n",
      "Iteration 1916, loss = 0.01827598\n",
      "Iteration 1917, loss = 0.01825790\n",
      "Iteration 1918, loss = 0.01823983\n",
      "Iteration 1919, loss = 0.01822179\n",
      "Iteration 1920, loss = 0.01820377\n",
      "Iteration 1921, loss = 0.01818578\n",
      "Iteration 1922, loss = 0.01816781\n",
      "Iteration 1923, loss = 0.01814986\n",
      "Iteration 1924, loss = 0.01813194\n",
      "Iteration 1925, loss = 0.01811403\n",
      "Iteration 1926, loss = 0.01809616\n",
      "Iteration 1927, loss = 0.01807830\n",
      "Iteration 1928, loss = 0.01806047\n",
      "Iteration 1929, loss = 0.01804265\n",
      "Iteration 1930, loss = 0.01802487\n",
      "Iteration 1931, loss = 0.01800710\n",
      "Iteration 1932, loss = 0.01798936\n",
      "Iteration 1933, loss = 0.01797164\n",
      "Iteration 1934, loss = 0.01795394\n",
      "Iteration 1935, loss = 0.01793627\n",
      "Iteration 1936, loss = 0.01791861\n",
      "Iteration 1937, loss = 0.01790098\n",
      "Iteration 1938, loss = 0.01788337\n",
      "Iteration 1939, loss = 0.01786579\n",
      "Iteration 1940, loss = 0.01784823\n",
      "Iteration 1941, loss = 0.01783069\n",
      "Iteration 1942, loss = 0.01781317\n",
      "Iteration 1943, loss = 0.01779567\n",
      "Iteration 1944, loss = 0.01777820\n",
      "Iteration 1945, loss = 0.01776075\n",
      "Iteration 1946, loss = 0.01774332\n",
      "Iteration 1947, loss = 0.01772591\n",
      "Iteration 1948, loss = 0.01770852\n",
      "Iteration 1949, loss = 0.01769116\n",
      "Iteration 1950, loss = 0.01767382\n",
      "Iteration 1951, loss = 0.01765650\n",
      "Iteration 1952, loss = 0.01763920\n",
      "Iteration 1953, loss = 0.01762192\n",
      "Iteration 1954, loss = 0.01760467\n",
      "Iteration 1955, loss = 0.01758744\n",
      "Iteration 1956, loss = 0.01757023\n",
      "Iteration 1957, loss = 0.01755304\n",
      "Iteration 1958, loss = 0.01753587\n",
      "Iteration 1959, loss = 0.01751872\n",
      "Iteration 1960, loss = 0.01750160\n",
      "Iteration 1961, loss = 0.01748450\n",
      "Iteration 1962, loss = 0.01746741\n",
      "Iteration 1963, loss = 0.01745035\n",
      "Iteration 1964, loss = 0.01743332\n",
      "Iteration 1965, loss = 0.01741630\n",
      "Iteration 1966, loss = 0.01739930\n",
      "Iteration 1967, loss = 0.01738233\n",
      "Iteration 1968, loss = 0.01736538\n",
      "Iteration 1969, loss = 0.01734844\n",
      "Iteration 1970, loss = 0.01733153\n",
      "Iteration 1971, loss = 0.01731464\n",
      "Iteration 1972, loss = 0.01729778\n",
      "Iteration 1973, loss = 0.01728093\n",
      "Iteration 1974, loss = 0.01726410\n",
      "Iteration 1975, loss = 0.01724730\n",
      "Iteration 1976, loss = 0.01723051\n",
      "Iteration 1977, loss = 0.01721375\n",
      "Iteration 1978, loss = 0.01719701\n",
      "Iteration 1979, loss = 0.01718028\n",
      "Iteration 1980, loss = 0.01716358\n",
      "Iteration 1981, loss = 0.01714690\n",
      "Iteration 1982, loss = 0.01713024\n",
      "Iteration 1983, loss = 0.01711361\n",
      "Iteration 1984, loss = 0.01709699\n",
      "Iteration 1985, loss = 0.01708039\n",
      "Iteration 1986, loss = 0.01706382\n",
      "Iteration 1987, loss = 0.01704726\n",
      "Iteration 1988, loss = 0.01703072\n",
      "Iteration 1989, loss = 0.01701421\n",
      "Iteration 1990, loss = 0.01699771\n",
      "Iteration 1991, loss = 0.01698124\n",
      "Iteration 1992, loss = 0.01696479\n",
      "Iteration 1993, loss = 0.01694835\n",
      "Iteration 1994, loss = 0.01693194\n",
      "Iteration 1995, loss = 0.01691555\n",
      "Iteration 1996, loss = 0.01689918\n",
      "Iteration 1997, loss = 0.01688282\n",
      "Iteration 1998, loss = 0.01686649\n",
      "Iteration 1999, loss = 0.01685018\n",
      "Iteration 2000, loss = 0.01683389\n",
      "Iteration 2001, loss = 0.01681762\n",
      "Iteration 2002, loss = 0.01680137\n",
      "Iteration 2003, loss = 0.01678513\n",
      "Iteration 2004, loss = 0.01676892\n",
      "Iteration 2005, loss = 0.01675273\n",
      "Iteration 2006, loss = 0.01673656\n",
      "Iteration 2007, loss = 0.01672041\n",
      "Iteration 2008, loss = 0.01670428\n",
      "Iteration 2009, loss = 0.01668816\n",
      "Iteration 2010, loss = 0.01667207\n",
      "Iteration 2011, loss = 0.01665600\n",
      "Iteration 2012, loss = 0.01663995\n",
      "Iteration 2013, loss = 0.01662391\n",
      "Iteration 2014, loss = 0.01660790\n",
      "Iteration 2015, loss = 0.01659191\n",
      "Iteration 2016, loss = 0.01657593\n",
      "Iteration 2017, loss = 0.01655998\n",
      "Iteration 2018, loss = 0.01654404\n",
      "Iteration 2019, loss = 0.01652813\n",
      "Iteration 2020, loss = 0.01651223\n",
      "Iteration 2021, loss = 0.01649635\n",
      "Iteration 2022, loss = 0.01648050\n",
      "Iteration 2023, loss = 0.01646466\n",
      "Iteration 2024, loss = 0.01644884\n",
      "Iteration 2025, loss = 0.01643304\n",
      "Iteration 2026, loss = 0.01641726\n",
      "Iteration 2027, loss = 0.01640150\n",
      "Iteration 2028, loss = 0.01638576\n",
      "Iteration 2029, loss = 0.01637003\n",
      "Iteration 2030, loss = 0.01635433\n",
      "Iteration 2031, loss = 0.01633864\n",
      "Iteration 2032, loss = 0.01632298\n",
      "Iteration 2033, loss = 0.01630733\n",
      "Iteration 2034, loss = 0.01629171\n",
      "Iteration 2035, loss = 0.01627610\n",
      "Iteration 2036, loss = 0.01626051\n",
      "Iteration 2037, loss = 0.01624494\n",
      "Iteration 2038, loss = 0.01622938\n",
      "Iteration 2039, loss = 0.01621385\n",
      "Iteration 2040, loss = 0.01619834\n",
      "Iteration 2041, loss = 0.01618284\n",
      "Iteration 2042, loss = 0.01616736\n",
      "Iteration 2043, loss = 0.01615191\n",
      "Iteration 2044, loss = 0.01613647\n",
      "Iteration 2045, loss = 0.01612105\n",
      "Iteration 2046, loss = 0.01610564\n",
      "Iteration 2047, loss = 0.01609026\n",
      "Iteration 2048, loss = 0.01607490\n",
      "Iteration 2049, loss = 0.01605955\n",
      "Iteration 2050, loss = 0.01604422\n",
      "Iteration 2051, loss = 0.01602891\n",
      "Iteration 2052, loss = 0.01601362\n",
      "Iteration 2053, loss = 0.01599835\n",
      "Iteration 2054, loss = 0.01598309\n",
      "Iteration 2055, loss = 0.01596786\n",
      "Iteration 2056, loss = 0.01595264\n",
      "Iteration 2057, loss = 0.01593744\n",
      "Iteration 2058, loss = 0.01592226\n",
      "Iteration 2059, loss = 0.01590709\n",
      "Iteration 2060, loss = 0.01589195\n",
      "Iteration 2061, loss = 0.01587682\n",
      "Iteration 2062, loss = 0.01586171\n",
      "Iteration 2063, loss = 0.01584662\n",
      "Iteration 2064, loss = 0.01583155\n",
      "Iteration 2065, loss = 0.01581650\n",
      "Iteration 2066, loss = 0.01580146\n",
      "Iteration 2067, loss = 0.01578644\n",
      "Iteration 2068, loss = 0.01577144\n",
      "Iteration 2069, loss = 0.01575646\n",
      "Iteration 2070, loss = 0.01574149\n",
      "Iteration 2071, loss = 0.01572654\n",
      "Iteration 2072, loss = 0.01571162\n",
      "Iteration 2073, loss = 0.01569670\n",
      "Iteration 2074, loss = 0.01568181\n",
      "Iteration 2075, loss = 0.01566693\n",
      "Iteration 2076, loss = 0.01565208\n",
      "Iteration 2077, loss = 0.01563723\n",
      "Iteration 2078, loss = 0.01562241\n",
      "Iteration 2079, loss = 0.01560761\n",
      "Iteration 2080, loss = 0.01559282\n",
      "Iteration 2081, loss = 0.01557805\n",
      "Iteration 2082, loss = 0.01556329\n",
      "Iteration 2083, loss = 0.01554856\n",
      "Iteration 2084, loss = 0.01553384\n",
      "Iteration 2085, loss = 0.01551914\n",
      "Iteration 2086, loss = 0.01550446\n",
      "Iteration 2087, loss = 0.01548979\n",
      "Iteration 2088, loss = 0.01547514\n",
      "Iteration 2089, loss = 0.01546051\n",
      "Iteration 2090, loss = 0.01544590\n",
      "Iteration 2091, loss = 0.01543130\n",
      "Iteration 2092, loss = 0.01541672\n",
      "Iteration 2093, loss = 0.01540216\n",
      "Iteration 2094, loss = 0.01538761\n",
      "Iteration 2095, loss = 0.01537309\n",
      "Iteration 2096, loss = 0.01535857\n",
      "Iteration 2097, loss = 0.01534408\n",
      "Iteration 2098, loss = 0.01532960\n",
      "Iteration 2099, loss = 0.01531514\n",
      "Iteration 2100, loss = 0.01530070\n",
      "Iteration 2101, loss = 0.01528628\n",
      "Iteration 2102, loss = 0.01527187\n",
      "Iteration 2103, loss = 0.01525748\n",
      "Iteration 2104, loss = 0.01524310\n",
      "Iteration 2105, loss = 0.01522874\n",
      "Iteration 2106, loss = 0.01521440\n",
      "Iteration 2107, loss = 0.01520008\n",
      "Iteration 2108, loss = 0.01518577\n",
      "Iteration 2109, loss = 0.01517148\n",
      "Iteration 2110, loss = 0.01515720\n",
      "Iteration 2111, loss = 0.01514295\n",
      "Iteration 2112, loss = 0.01512870\n",
      "Iteration 2113, loss = 0.01511448\n",
      "Iteration 2114, loss = 0.01510027\n",
      "Iteration 2115, loss = 0.01508608\n",
      "Iteration 2116, loss = 0.01507191\n",
      "Iteration 2117, loss = 0.01505775\n",
      "Iteration 2118, loss = 0.01504361\n",
      "Iteration 2119, loss = 0.01502948\n",
      "Iteration 2120, loss = 0.01501537\n",
      "Iteration 2121, loss = 0.01500128\n",
      "Iteration 2122, loss = 0.01498721\n",
      "Iteration 2123, loss = 0.01497315\n",
      "Iteration 2124, loss = 0.01495910\n",
      "Iteration 2125, loss = 0.01494508\n",
      "Iteration 2126, loss = 0.01493107\n",
      "Iteration 2127, loss = 0.01491707\n",
      "Iteration 2128, loss = 0.01490309\n",
      "Iteration 2129, loss = 0.01488913\n",
      "Iteration 2130, loss = 0.01487519\n",
      "Iteration 2131, loss = 0.01486126\n",
      "Iteration 2132, loss = 0.01484734\n",
      "Iteration 2133, loss = 0.01483345\n",
      "Iteration 2134, loss = 0.01481956\n",
      "Iteration 2135, loss = 0.01480570\n",
      "Iteration 2136, loss = 0.01479185\n",
      "Iteration 2137, loss = 0.01477802\n",
      "Iteration 2138, loss = 0.01476420\n",
      "Iteration 2139, loss = 0.01475040\n",
      "Iteration 2140, loss = 0.01473661\n",
      "Iteration 2141, loss = 0.01472284\n",
      "Iteration 2142, loss = 0.01470909\n",
      "Iteration 2143, loss = 0.01469535\n",
      "Iteration 2144, loss = 0.01468163\n",
      "Iteration 2145, loss = 0.01466792\n",
      "Iteration 2146, loss = 0.01465423\n",
      "Iteration 2147, loss = 0.01464056\n",
      "Iteration 2148, loss = 0.01462690\n",
      "Iteration 2149, loss = 0.01461326\n",
      "Iteration 2150, loss = 0.01459963\n",
      "Iteration 2151, loss = 0.01458602\n",
      "Iteration 2152, loss = 0.01457242\n",
      "Iteration 2153, loss = 0.01455884\n",
      "Iteration 2154, loss = 0.01454527\n",
      "Iteration 2155, loss = 0.01453172\n",
      "Iteration 2156, loss = 0.01451819\n",
      "Iteration 2157, loss = 0.01450467\n",
      "Iteration 2158, loss = 0.01449117\n",
      "Iteration 2159, loss = 0.01447768\n",
      "Iteration 2160, loss = 0.01446421\n",
      "Iteration 2161, loss = 0.01445075\n",
      "Iteration 2162, loss = 0.01443731\n",
      "Iteration 2163, loss = 0.01442388\n",
      "Iteration 2164, loss = 0.01441047\n",
      "Iteration 2165, loss = 0.01439708\n",
      "Iteration 2166, loss = 0.01438370\n",
      "Iteration 2167, loss = 0.01437033\n",
      "Iteration 2168, loss = 0.01435698\n",
      "Iteration 2169, loss = 0.01434365\n",
      "Iteration 2170, loss = 0.01433033\n",
      "Iteration 2171, loss = 0.01431702\n",
      "Iteration 2172, loss = 0.01430373\n",
      "Iteration 2173, loss = 0.01429046\n",
      "Iteration 2174, loss = 0.01427720\n",
      "Iteration 2175, loss = 0.01426396\n",
      "Iteration 2176, loss = 0.01425073\n",
      "Iteration 2177, loss = 0.01423751\n",
      "Iteration 2178, loss = 0.01422431\n",
      "Iteration 2179, loss = 0.01421113\n",
      "Iteration 2180, loss = 0.01419796\n",
      "Iteration 2181, loss = 0.01418481\n",
      "Iteration 2182, loss = 0.01417167\n",
      "Iteration 2183, loss = 0.01415854\n",
      "Iteration 2184, loss = 0.01414543\n",
      "Iteration 2185, loss = 0.01413234\n",
      "Iteration 2186, loss = 0.01411926\n",
      "Iteration 2187, loss = 0.01410619\n",
      "Iteration 2188, loss = 0.01409314\n",
      "Iteration 2189, loss = 0.01408010\n",
      "Iteration 2190, loss = 0.01406708\n",
      "Iteration 2191, loss = 0.01405408\n",
      "Iteration 2192, loss = 0.01404109\n",
      "Iteration 2193, loss = 0.01402811\n",
      "Iteration 2194, loss = 0.01401515\n",
      "Iteration 2195, loss = 0.01400220\n",
      "Iteration 2196, loss = 0.01398926\n",
      "Iteration 2197, loss = 0.01397634\n",
      "Iteration 2198, loss = 0.01396344\n",
      "Iteration 2199, loss = 0.01395055\n",
      "Iteration 2200, loss = 0.01393767\n",
      "Iteration 2201, loss = 0.01392481\n",
      "Iteration 2202, loss = 0.01391197\n",
      "Iteration 2203, loss = 0.01389913\n",
      "Iteration 2204, loss = 0.01388632\n",
      "Iteration 2205, loss = 0.01387351\n",
      "Iteration 2206, loss = 0.01386072\n",
      "Iteration 2207, loss = 0.01384795\n",
      "Iteration 2208, loss = 0.01383519\n",
      "Iteration 2209, loss = 0.01382244\n",
      "Iteration 2210, loss = 0.01380971\n",
      "Iteration 2211, loss = 0.01379699\n",
      "Iteration 2212, loss = 0.01378429\n",
      "Iteration 2213, loss = 0.01377160\n",
      "Iteration 2214, loss = 0.01375892\n",
      "Iteration 2215, loss = 0.01374626\n",
      "Iteration 2216, loss = 0.01373362\n",
      "Iteration 2217, loss = 0.01372098\n",
      "Iteration 2218, loss = 0.01370836\n",
      "Iteration 2219, loss = 0.01369576\n",
      "Iteration 2220, loss = 0.01368317\n",
      "Iteration 2221, loss = 0.01367059\n",
      "Iteration 2222, loss = 0.01365803\n",
      "Iteration 2223, loss = 0.01364548\n",
      "Iteration 2224, loss = 0.01363295\n",
      "Iteration 2225, loss = 0.01362042\n",
      "Iteration 2226, loss = 0.01360792\n",
      "Iteration 2227, loss = 0.01359542\n",
      "Iteration 2228, loss = 0.01358294\n",
      "Iteration 2229, loss = 0.01357048\n",
      "Iteration 2230, loss = 0.01355803\n",
      "Iteration 2231, loss = 0.01354559\n",
      "Iteration 2232, loss = 0.01353317\n",
      "Iteration 2233, loss = 0.01352076\n",
      "Iteration 2234, loss = 0.01350836\n",
      "Iteration 2235, loss = 0.01349598\n",
      "Iteration 2236, loss = 0.01348361\n",
      "Iteration 2237, loss = 0.01347125\n",
      "Iteration 2238, loss = 0.01345891\n",
      "Iteration 2239, loss = 0.01344658\n",
      "Iteration 2240, loss = 0.01343427\n",
      "Iteration 2241, loss = 0.01342196\n",
      "Iteration 2242, loss = 0.01340968\n",
      "Iteration 2243, loss = 0.01339740\n",
      "Iteration 2244, loss = 0.01338514\n",
      "Iteration 2245, loss = 0.01337290\n",
      "Iteration 2246, loss = 0.01336066\n",
      "Iteration 2247, loss = 0.01334844\n",
      "Iteration 2248, loss = 0.01333624\n",
      "Iteration 2249, loss = 0.01332404\n",
      "Iteration 2250, loss = 0.01331186\n",
      "Iteration 2251, loss = 0.01329970\n",
      "Iteration 2252, loss = 0.01328754\n",
      "Iteration 2253, loss = 0.01327540\n",
      "Iteration 2254, loss = 0.01326328\n",
      "Iteration 2255, loss = 0.01325117\n",
      "Iteration 2256, loss = 0.01323907\n",
      "Iteration 2257, loss = 0.01322698\n",
      "Iteration 2258, loss = 0.01321491\n",
      "Iteration 2259, loss = 0.01320285\n",
      "Iteration 2260, loss = 0.01319080\n",
      "Iteration 2261, loss = 0.01317877\n",
      "Iteration 2262, loss = 0.01316675\n",
      "Iteration 2263, loss = 0.01315474\n",
      "Iteration 2264, loss = 0.01314275\n",
      "Iteration 2265, loss = 0.01313076\n",
      "Iteration 2266, loss = 0.01311880\n",
      "Iteration 2267, loss = 0.01310684\n",
      "Iteration 2268, loss = 0.01309490\n",
      "Iteration 2269, loss = 0.01308297\n",
      "Iteration 2270, loss = 0.01307106\n",
      "Iteration 2271, loss = 0.01305915\n",
      "Iteration 2272, loss = 0.01304726\n",
      "Iteration 2273, loss = 0.01303539\n",
      "Iteration 2274, loss = 0.01302352\n",
      "Iteration 2275, loss = 0.01301167\n",
      "Iteration 2276, loss = 0.01299983\n",
      "Iteration 2277, loss = 0.01298801\n",
      "Iteration 2278, loss = 0.01297620\n",
      "Iteration 2279, loss = 0.01296440\n",
      "Iteration 2280, loss = 0.01295261\n",
      "Iteration 2281, loss = 0.01294084\n",
      "Iteration 2282, loss = 0.01292908\n",
      "Iteration 2283, loss = 0.01291733\n",
      "Iteration 2284, loss = 0.01290559\n",
      "Iteration 2285, loss = 0.01289387\n",
      "Iteration 2286, loss = 0.01288216\n",
      "Iteration 2287, loss = 0.01287046\n",
      "Iteration 2288, loss = 0.01285878\n",
      "Iteration 2289, loss = 0.01284711\n",
      "Iteration 2290, loss = 0.01283545\n",
      "Iteration 2291, loss = 0.01282380\n",
      "Iteration 2292, loss = 0.01281217\n",
      "Iteration 2293, loss = 0.01280055\n",
      "Iteration 2294, loss = 0.01278894\n",
      "Iteration 2295, loss = 0.01277734\n",
      "Iteration 2296, loss = 0.01276576\n",
      "Iteration 2297, loss = 0.01275419\n",
      "Iteration 2298, loss = 0.01274263\n",
      "Iteration 2299, loss = 0.01273108\n",
      "Iteration 2300, loss = 0.01271955\n",
      "Iteration 2301, loss = 0.01270803\n",
      "Iteration 2302, loss = 0.01269652\n",
      "Iteration 2303, loss = 0.01268502\n",
      "Iteration 2304, loss = 0.01267354\n",
      "Iteration 2305, loss = 0.01266207\n",
      "Iteration 2306, loss = 0.01265061\n",
      "Iteration 2307, loss = 0.01263916\n",
      "Iteration 2308, loss = 0.01262773\n",
      "Iteration 2309, loss = 0.01261630\n",
      "Iteration 2310, loss = 0.01260489\n",
      "Iteration 2311, loss = 0.01259350\n",
      "Iteration 2312, loss = 0.01258211\n",
      "Iteration 2313, loss = 0.01257074\n",
      "Iteration 2314, loss = 0.01255938\n",
      "Iteration 2315, loss = 0.01254803\n",
      "Iteration 2316, loss = 0.01253669\n",
      "Iteration 2317, loss = 0.01252537\n",
      "Iteration 2318, loss = 0.01251406\n",
      "Iteration 2319, loss = 0.01250276\n",
      "Iteration 2320, loss = 0.01249147\n",
      "Iteration 2321, loss = 0.01248019\n",
      "Iteration 2322, loss = 0.01246893\n",
      "Iteration 2323, loss = 0.01245768\n",
      "Iteration 2324, loss = 0.01244644\n",
      "Iteration 2325, loss = 0.01243521\n",
      "Iteration 2326, loss = 0.01242400\n",
      "Iteration 2327, loss = 0.01241279\n",
      "Iteration 2328, loss = 0.01240160\n",
      "Iteration 2329, loss = 0.01239042\n",
      "Iteration 2330, loss = 0.01237925\n",
      "Iteration 2331, loss = 0.01236810\n",
      "Iteration 2332, loss = 0.01235695\n",
      "Iteration 2333, loss = 0.01234582\n",
      "Iteration 2334, loss = 0.01233470\n",
      "Iteration 2335, loss = 0.01232360\n",
      "Iteration 2336, loss = 0.01231250\n",
      "Iteration 2337, loss = 0.01230142\n",
      "Iteration 2338, loss = 0.01229034\n",
      "Iteration 2339, loss = 0.01227928\n",
      "Iteration 2340, loss = 0.01226823\n",
      "Iteration 2341, loss = 0.01225720\n",
      "Iteration 2342, loss = 0.01224617\n",
      "Iteration 2343, loss = 0.01223516\n",
      "Iteration 2344, loss = 0.01222416\n",
      "Iteration 2345, loss = 0.01221317\n",
      "Iteration 2346, loss = 0.01220219\n",
      "Iteration 2347, loss = 0.01219122\n",
      "Iteration 2348, loss = 0.01218027\n",
      "Iteration 2349, loss = 0.01216932\n",
      "Iteration 2350, loss = 0.01215839\n",
      "Iteration 2351, loss = 0.01214747\n",
      "Iteration 2352, loss = 0.01213656\n",
      "Iteration 2353, loss = 0.01212567\n",
      "Iteration 2354, loss = 0.01211478\n",
      "Iteration 2355, loss = 0.01210391\n",
      "Iteration 2356, loss = 0.01209305\n",
      "Iteration 2357, loss = 0.01208220\n",
      "Iteration 2358, loss = 0.01207136\n",
      "Iteration 2359, loss = 0.01206053\n",
      "Iteration 2360, loss = 0.01204972\n",
      "Iteration 2361, loss = 0.01203891\n",
      "Iteration 2362, loss = 0.01202812\n",
      "Iteration 2363, loss = 0.01201734\n",
      "Iteration 2364, loss = 0.01200657\n",
      "Iteration 2365, loss = 0.01199581\n",
      "Iteration 2366, loss = 0.01198506\n",
      "Iteration 2367, loss = 0.01197432\n",
      "Iteration 2368, loss = 0.01196360\n",
      "Iteration 2369, loss = 0.01195289\n",
      "Iteration 2370, loss = 0.01194219\n",
      "Iteration 2371, loss = 0.01193150\n",
      "Iteration 2372, loss = 0.01192082\n",
      "Iteration 2373, loss = 0.01191015\n",
      "Iteration 2374, loss = 0.01189949\n",
      "Iteration 2375, loss = 0.01188885\n",
      "Iteration 2376, loss = 0.01187821\n",
      "Iteration 2377, loss = 0.01186759\n",
      "Iteration 2378, loss = 0.01185698\n",
      "Iteration 2379, loss = 0.01184638\n",
      "Iteration 2380, loss = 0.01183579\n",
      "Iteration 2381, loss = 0.01182521\n",
      "Iteration 2382, loss = 0.01181464\n",
      "Iteration 2383, loss = 0.01180409\n",
      "Iteration 2384, loss = 0.01179354\n",
      "Iteration 2385, loss = 0.01178301\n",
      "Iteration 2386, loss = 0.01177249\n",
      "Iteration 2387, loss = 0.01176198\n",
      "Iteration 2388, loss = 0.01175148\n",
      "Iteration 2389, loss = 0.01174099\n",
      "Iteration 2390, loss = 0.01173051\n",
      "Iteration 2391, loss = 0.01172004\n",
      "Iteration 2392, loss = 0.01170959\n",
      "Iteration 2393, loss = 0.01169914\n",
      "Iteration 2394, loss = 0.01168871\n",
      "Iteration 2395, loss = 0.01167829\n",
      "Iteration 2396, loss = 0.01166788\n",
      "Iteration 2397, loss = 0.01165748\n",
      "Iteration 2398, loss = 0.01164709\n",
      "Iteration 2399, loss = 0.01163671\n",
      "Iteration 2400, loss = 0.01162634\n",
      "Iteration 2401, loss = 0.01161598\n",
      "Iteration 2402, loss = 0.01160564\n",
      "Iteration 2403, loss = 0.01159530\n",
      "Iteration 2404, loss = 0.01158498\n",
      "Iteration 2405, loss = 0.01157466\n",
      "Iteration 2406, loss = 0.01156436\n",
      "Iteration 2407, loss = 0.01155407\n",
      "Iteration 2408, loss = 0.01154379\n",
      "Iteration 2409, loss = 0.01153352\n",
      "Iteration 2410, loss = 0.01152326\n",
      "Iteration 2411, loss = 0.01151301\n",
      "Iteration 2412, loss = 0.01150277\n",
      "Iteration 2413, loss = 0.01149254\n",
      "Iteration 2414, loss = 0.01148233\n",
      "Iteration 2415, loss = 0.01147212\n",
      "Iteration 2416, loss = 0.01146193\n",
      "Iteration 2417, loss = 0.01145174\n",
      "Iteration 2418, loss = 0.01144157\n",
      "Iteration 2419, loss = 0.01143141\n",
      "Iteration 2420, loss = 0.01142125\n",
      "Iteration 2421, loss = 0.01141111\n",
      "Iteration 2422, loss = 0.01140098\n",
      "Iteration 2423, loss = 0.01139086\n",
      "Iteration 2424, loss = 0.01138075\n",
      "Iteration 2425, loss = 0.01137065\n",
      "Iteration 2426, loss = 0.01136056\n",
      "Iteration 2427, loss = 0.01135049\n",
      "Iteration 2428, loss = 0.01134042\n",
      "Iteration 2429, loss = 0.01133036\n",
      "Iteration 2430, loss = 0.01132032\n",
      "Iteration 2431, loss = 0.01131028\n",
      "Iteration 2432, loss = 0.01130025\n",
      "Iteration 2433, loss = 0.01129024\n",
      "Iteration 2434, loss = 0.01128024\n",
      "Iteration 2435, loss = 0.01127024\n",
      "Iteration 2436, loss = 0.01126026\n",
      "Iteration 2437, loss = 0.01125029\n",
      "Iteration 2438, loss = 0.01124032\n",
      "Iteration 2439, loss = 0.01123037\n",
      "Iteration 2440, loss = 0.01122043\n",
      "Iteration 2441, loss = 0.01121050\n",
      "Iteration 2442, loss = 0.01120058\n",
      "Iteration 2443, loss = 0.01119067\n",
      "Iteration 2444, loss = 0.01118077\n",
      "Iteration 2445, loss = 0.01117088\n",
      "Iteration 2446, loss = 0.01116100\n",
      "Iteration 2447, loss = 0.01115113\n",
      "Iteration 2448, loss = 0.01114127\n",
      "Iteration 2449, loss = 0.01113142\n",
      "Iteration 2450, loss = 0.01112158\n",
      "Iteration 2451, loss = 0.01111175\n",
      "Iteration 2452, loss = 0.01110194\n",
      "Iteration 2453, loss = 0.01109213\n",
      "Iteration 2454, loss = 0.01108233\n",
      "Iteration 2455, loss = 0.01107255\n",
      "Iteration 2456, loss = 0.01106277\n",
      "Iteration 2457, loss = 0.01105300\n",
      "Iteration 2458, loss = 0.01104325\n",
      "Iteration 2459, loss = 0.01103350\n",
      "Iteration 2460, loss = 0.01102376\n",
      "Iteration 2461, loss = 0.01101404\n",
      "Iteration 2462, loss = 0.01100432\n",
      "Iteration 2463, loss = 0.01099462\n",
      "Iteration 2464, loss = 0.01098492\n",
      "Iteration 2465, loss = 0.01097524\n",
      "Iteration 2466, loss = 0.01096556\n",
      "Iteration 2467, loss = 0.01095590\n",
      "Iteration 2468, loss = 0.01094624\n",
      "Iteration 2469, loss = 0.01093660\n",
      "Iteration 2470, loss = 0.01092696\n",
      "Iteration 2471, loss = 0.01091734\n",
      "Iteration 2472, loss = 0.01090772\n",
      "Iteration 2473, loss = 0.01089812\n",
      "Iteration 2474, loss = 0.01088852\n",
      "Iteration 2475, loss = 0.01087894\n",
      "Iteration 2476, loss = 0.01086936\n",
      "Iteration 2477, loss = 0.01085980\n",
      "Iteration 2478, loss = 0.01085024\n",
      "Iteration 2479, loss = 0.01084070\n",
      "Iteration 2480, loss = 0.01083116\n",
      "Iteration 2481, loss = 0.01082164\n",
      "Iteration 2482, loss = 0.01081212\n",
      "Iteration 2483, loss = 0.01080262\n",
      "Iteration 2484, loss = 0.01079312\n",
      "Iteration 2485, loss = 0.01078364\n",
      "Iteration 2486, loss = 0.01077416\n",
      "Iteration 2487, loss = 0.01076469\n",
      "Iteration 2488, loss = 0.01075524\n",
      "Iteration 2489, loss = 0.01074579\n",
      "Iteration 2490, loss = 0.01073636\n",
      "Iteration 2491, loss = 0.01072693\n",
      "Iteration 2492, loss = 0.01071751\n",
      "Iteration 2493, loss = 0.01070811\n",
      "Iteration 2494, loss = 0.01069871\n",
      "Iteration 2495, loss = 0.01068932\n",
      "Iteration 2496, loss = 0.01067994\n",
      "Iteration 2497, loss = 0.01067058\n",
      "Iteration 2498, loss = 0.01066122\n",
      "Iteration 2499, loss = 0.01065187\n",
      "Iteration 2500, loss = 0.01064253\n",
      "Iteration 2501, loss = 0.01063320\n",
      "Iteration 2502, loss = 0.01062388\n",
      "Iteration 2503, loss = 0.01061457\n",
      "Iteration 2504, loss = 0.01060527\n",
      "Iteration 2505, loss = 0.01059598\n",
      "Iteration 2506, loss = 0.01058670\n",
      "Iteration 2507, loss = 0.01057743\n",
      "Iteration 2508, loss = 0.01056817\n",
      "Iteration 2509, loss = 0.01055892\n",
      "Iteration 2510, loss = 0.01054968\n",
      "Iteration 2511, loss = 0.01054045\n",
      "Iteration 2512, loss = 0.01053122\n",
      "Iteration 2513, loss = 0.01052201\n",
      "Iteration 2514, loss = 0.01051281\n",
      "Iteration 2515, loss = 0.01050361\n",
      "Iteration 2516, loss = 0.01049443\n",
      "Iteration 2517, loss = 0.01048525\n",
      "Iteration 2518, loss = 0.01047609\n",
      "Iteration 2519, loss = 0.01046693\n",
      "Iteration 2520, loss = 0.01045779\n",
      "Iteration 2521, loss = 0.01044865\n",
      "Iteration 2522, loss = 0.01043952\n",
      "Iteration 2523, loss = 0.01043040\n",
      "Iteration 2524, loss = 0.01042130\n",
      "Iteration 2525, loss = 0.01041220\n",
      "Iteration 2526, loss = 0.01040311\n",
      "Iteration 2527, loss = 0.01039403\n",
      "Iteration 2528, loss = 0.01038496\n",
      "Iteration 2529, loss = 0.01037589\n",
      "Iteration 2530, loss = 0.01036684\n",
      "Iteration 2531, loss = 0.01035780\n",
      "Iteration 2532, loss = 0.01034877\n",
      "Iteration 2533, loss = 0.01033974\n",
      "Iteration 2534, loss = 0.01033073\n",
      "Iteration 2535, loss = 0.01032172\n",
      "Iteration 2536, loss = 0.01031273\n",
      "Iteration 2537, loss = 0.01030374\n",
      "Iteration 2538, loss = 0.01029477\n",
      "Iteration 2539, loss = 0.01028580\n",
      "Iteration 2540, loss = 0.01027684\n",
      "Iteration 2541, loss = 0.01026789\n",
      "Iteration 2542, loss = 0.01025895\n",
      "Iteration 2543, loss = 0.01025002\n",
      "Iteration 2544, loss = 0.01024110\n",
      "Iteration 2545, loss = 0.01023219\n",
      "Iteration 2546, loss = 0.01022328\n",
      "Iteration 2547, loss = 0.01021439\n",
      "Iteration 2548, loss = 0.01020551\n",
      "Iteration 2549, loss = 0.01019663\n",
      "Iteration 2550, loss = 0.01018777\n",
      "Iteration 2551, loss = 0.01017891\n",
      "Iteration 2552, loss = 0.01017006\n",
      "Iteration 2553, loss = 0.01016122\n",
      "Iteration 2554, loss = 0.01015239\n",
      "Iteration 2555, loss = 0.01014357\n",
      "Iteration 2556, loss = 0.01013476\n",
      "Iteration 2557, loss = 0.01012596\n",
      "Iteration 2558, loss = 0.01011717\n",
      "Iteration 2559, loss = 0.01010838\n",
      "Iteration 2560, loss = 0.01009961\n",
      "Iteration 2561, loss = 0.01009084\n",
      "Iteration 2562, loss = 0.01008209\n",
      "Iteration 2563, loss = 0.01007334\n",
      "Iteration 2564, loss = 0.01006460\n",
      "Iteration 2565, loss = 0.01005587\n",
      "Iteration 2566, loss = 0.01004715\n",
      "Iteration 2567, loss = 0.01003844\n",
      "Iteration 2568, loss = 0.01002974\n",
      "Iteration 2569, loss = 0.01002105\n",
      "Iteration 2570, loss = 0.01001236\n",
      "Iteration 2571, loss = 0.01000369\n",
      "Iteration 2572, loss = 0.00999502\n",
      "Iteration 2573, loss = 0.00998637\n",
      "Iteration 2574, loss = 0.00997772\n",
      "Iteration 2575, loss = 0.00996908\n",
      "Iteration 2576, loss = 0.00996045\n",
      "Iteration 2577, loss = 0.00995183\n",
      "Iteration 2578, loss = 0.00994322\n",
      "Iteration 2579, loss = 0.00993461\n",
      "Iteration 2580, loss = 0.00992602\n",
      "Iteration 2581, loss = 0.00991743\n",
      "Iteration 2582, loss = 0.00990886\n",
      "Iteration 2583, loss = 0.00990029\n",
      "Iteration 2584, loss = 0.00989173\n",
      "Iteration 2585, loss = 0.00988318\n",
      "Iteration 2586, loss = 0.00987464\n",
      "Iteration 2587, loss = 0.00986611\n",
      "Iteration 2588, loss = 0.00985758\n",
      "Iteration 2589, loss = 0.00984907\n",
      "Iteration 2590, loss = 0.00984056\n",
      "Iteration 2591, loss = 0.00983207\n",
      "Iteration 2592, loss = 0.00982358\n",
      "Iteration 2593, loss = 0.00981510\n",
      "Iteration 2594, loss = 0.00980663\n",
      "Iteration 2595, loss = 0.00979817\n",
      "Iteration 2596, loss = 0.00978971\n",
      "Iteration 2597, loss = 0.00978127\n",
      "Iteration 2598, loss = 0.00977283\n",
      "Iteration 2599, loss = 0.00976441\n",
      "Iteration 2600, loss = 0.00975599\n",
      "Iteration 2601, loss = 0.00974758\n",
      "Iteration 2602, loss = 0.00973918\n",
      "Iteration 2603, loss = 0.00973079\n",
      "Iteration 2604, loss = 0.00972240\n",
      "Iteration 2605, loss = 0.00971403\n",
      "Iteration 2606, loss = 0.00970566\n",
      "Iteration 2607, loss = 0.00969731\n",
      "Iteration 2608, loss = 0.00968896\n",
      "Iteration 2609, loss = 0.00968062\n",
      "Iteration 2610, loss = 0.00967229\n",
      "Iteration 2611, loss = 0.00966396\n",
      "Iteration 2612, loss = 0.00965565\n",
      "Iteration 2613, loss = 0.00964734\n",
      "Iteration 2614, loss = 0.00963905\n",
      "Iteration 2615, loss = 0.00963076\n",
      "Iteration 2616, loss = 0.00962248\n",
      "Iteration 2617, loss = 0.00961421\n",
      "Iteration 2618, loss = 0.00960595\n",
      "Iteration 2619, loss = 0.00959769\n",
      "Iteration 2620, loss = 0.00958945\n",
      "Iteration 2621, loss = 0.00958121\n",
      "Iteration 2622, loss = 0.00957298\n",
      "Iteration 2623, loss = 0.00956476\n",
      "Iteration 2624, loss = 0.00955655\n",
      "Iteration 2625, loss = 0.00954835\n",
      "Iteration 2626, loss = 0.00954015\n",
      "Iteration 2627, loss = 0.00953196\n",
      "Iteration 2628, loss = 0.00952379\n",
      "Iteration 2629, loss = 0.00951562\n",
      "Iteration 2630, loss = 0.00950746\n",
      "Iteration 2631, loss = 0.00949931\n",
      "Iteration 2632, loss = 0.00949116\n",
      "Iteration 2633, loss = 0.00948303\n",
      "Iteration 2634, loss = 0.00947490\n",
      "Iteration 2635, loss = 0.00946678\n",
      "Iteration 2636, loss = 0.00945867\n",
      "Iteration 2637, loss = 0.00945057\n",
      "Iteration 2638, loss = 0.00944248\n",
      "Iteration 2639, loss = 0.00943439\n",
      "Iteration 2640, loss = 0.00942631\n",
      "Iteration 2641, loss = 0.00941825\n",
      "Iteration 2642, loss = 0.00941019\n",
      "Iteration 2643, loss = 0.00940213\n",
      "Iteration 2644, loss = 0.00939409\n",
      "Iteration 2645, loss = 0.00938606\n",
      "Iteration 2646, loss = 0.00937803\n",
      "Iteration 2647, loss = 0.00937001\n",
      "Iteration 2648, loss = 0.00936200\n",
      "Iteration 2649, loss = 0.00935400\n",
      "Iteration 2650, loss = 0.00934600\n",
      "Iteration 2651, loss = 0.00933802\n",
      "Iteration 2652, loss = 0.00933004\n",
      "Iteration 2653, loss = 0.00932207\n",
      "Iteration 2654, loss = 0.00931411\n",
      "Iteration 2655, loss = 0.00930616\n",
      "Iteration 2656, loss = 0.00929822\n",
      "Iteration 2657, loss = 0.00929028\n",
      "Iteration 2658, loss = 0.00928235\n",
      "Iteration 2659, loss = 0.00927443\n",
      "Iteration 2660, loss = 0.00926652\n",
      "Iteration 2661, loss = 0.00925862\n",
      "Iteration 2662, loss = 0.00925072\n",
      "Iteration 2663, loss = 0.00924284\n",
      "Iteration 2664, loss = 0.00923496\n",
      "Iteration 2665, loss = 0.00922709\n",
      "Iteration 2666, loss = 0.00921923\n",
      "Iteration 2667, loss = 0.00921137\n",
      "Iteration 2668, loss = 0.00920352\n",
      "Iteration 2669, loss = 0.00919569\n",
      "Iteration 2670, loss = 0.00918786\n",
      "Iteration 2671, loss = 0.00918003\n",
      "Iteration 2672, loss = 0.00917222\n",
      "Iteration 2673, loss = 0.00916441\n",
      "Iteration 2674, loss = 0.00915662\n",
      "Iteration 2675, loss = 0.00914883\n",
      "Iteration 2676, loss = 0.00914105\n",
      "Iteration 2677, loss = 0.00913327\n",
      "Iteration 2678, loss = 0.00912551\n",
      "Iteration 2679, loss = 0.00911775\n",
      "Iteration 2680, loss = 0.00911000\n",
      "Iteration 2681, loss = 0.00910226\n",
      "Iteration 2682, loss = 0.00909452\n",
      "Iteration 2683, loss = 0.00908680\n",
      "Iteration 2684, loss = 0.00907908\n",
      "Iteration 2685, loss = 0.00907137\n",
      "Iteration 2686, loss = 0.00906367\n",
      "Iteration 2687, loss = 0.00905598\n",
      "Iteration 2688, loss = 0.00904829\n",
      "Iteration 2689, loss = 0.00904061\n",
      "Iteration 2690, loss = 0.00903294\n",
      "Iteration 2691, loss = 0.00902528\n",
      "Iteration 2692, loss = 0.00901763\n",
      "Iteration 2693, loss = 0.00900998\n",
      "Iteration 2694, loss = 0.00900234\n",
      "Iteration 2695, loss = 0.00899471\n",
      "Iteration 2696, loss = 0.00898709\n",
      "Iteration 2697, loss = 0.00897947\n",
      "Iteration 2698, loss = 0.00897187\n",
      "Iteration 2699, loss = 0.00896427\n",
      "Iteration 2700, loss = 0.00895668\n",
      "Iteration 2701, loss = 0.00894909\n",
      "Iteration 2702, loss = 0.00894152\n",
      "Iteration 2703, loss = 0.00893395\n",
      "Iteration 2704, loss = 0.00892639\n",
      "Iteration 2705, loss = 0.00891884\n",
      "Iteration 2706, loss = 0.00891129\n",
      "Iteration 2707, loss = 0.00890376\n",
      "Iteration 2708, loss = 0.00889623\n",
      "Iteration 2709, loss = 0.00888871\n",
      "Iteration 2710, loss = 0.00888119\n",
      "Iteration 2711, loss = 0.00887369\n",
      "Iteration 2712, loss = 0.00886619\n",
      "Iteration 2713, loss = 0.00885870\n",
      "Iteration 2714, loss = 0.00885122\n",
      "Iteration 2715, loss = 0.00884374\n",
      "Iteration 2716, loss = 0.00883627\n",
      "Iteration 2717, loss = 0.00882881\n",
      "Iteration 2718, loss = 0.00882136\n",
      "Iteration 2719, loss = 0.00881392\n",
      "Iteration 2720, loss = 0.00880648\n",
      "Iteration 2721, loss = 0.00879905\n",
      "Iteration 2722, loss = 0.00879163\n",
      "Iteration 2723, loss = 0.00878422\n",
      "Iteration 2724, loss = 0.00877681\n",
      "Iteration 2725, loss = 0.00876941\n",
      "Iteration 2726, loss = 0.00876202\n",
      "Iteration 2727, loss = 0.00875464\n",
      "Iteration 2728, loss = 0.00874727\n",
      "Iteration 2729, loss = 0.00873990\n",
      "Iteration 2730, loss = 0.00873254\n",
      "Iteration 2731, loss = 0.00872518\n",
      "Iteration 2732, loss = 0.00871784\n",
      "Iteration 2733, loss = 0.00871050\n",
      "Iteration 2734, loss = 0.00870317\n",
      "Iteration 2735, loss = 0.00869585\n",
      "Iteration 2736, loss = 0.00868853\n",
      "Iteration 2737, loss = 0.00868123\n",
      "Iteration 2738, loss = 0.00867393\n",
      "Iteration 2739, loss = 0.00866664\n",
      "Iteration 2740, loss = 0.00865935\n",
      "Iteration 2741, loss = 0.00865207\n",
      "Iteration 2742, loss = 0.00864480\n",
      "Iteration 2743, loss = 0.00863754\n",
      "Iteration 2744, loss = 0.00863029\n",
      "Iteration 2745, loss = 0.00862304\n",
      "Iteration 2746, loss = 0.00861580\n",
      "Iteration 2747, loss = 0.00860857\n",
      "Iteration 2748, loss = 0.00860134\n",
      "Iteration 2749, loss = 0.00859412\n",
      "Iteration 2750, loss = 0.00858691\n",
      "Iteration 2751, loss = 0.00857971\n",
      "Iteration 2752, loss = 0.00857251\n",
      "Iteration 2753, loss = 0.00856532\n",
      "Iteration 2754, loss = 0.00855814\n",
      "Iteration 2755, loss = 0.00855097\n",
      "Iteration 2756, loss = 0.00854380\n",
      "Iteration 2757, loss = 0.00853665\n",
      "Iteration 2758, loss = 0.00852949\n",
      "Iteration 2759, loss = 0.00852235\n",
      "Iteration 2760, loss = 0.00851521\n",
      "Iteration 2761, loss = 0.00850808\n",
      "Iteration 2762, loss = 0.00850096\n",
      "Iteration 2763, loss = 0.00849385\n",
      "Iteration 2764, loss = 0.00848674\n",
      "Iteration 2765, loss = 0.00847964\n",
      "Iteration 2766, loss = 0.00847255\n",
      "Iteration 2767, loss = 0.00846546\n",
      "Iteration 2768, loss = 0.00845838\n",
      "Iteration 2769, loss = 0.00845131\n",
      "Iteration 2770, loss = 0.00844425\n",
      "Iteration 2771, loss = 0.00843719\n",
      "Iteration 2772, loss = 0.00843014\n",
      "Iteration 2773, loss = 0.00842310\n",
      "Iteration 2774, loss = 0.00841606\n",
      "Iteration 2775, loss = 0.00840904\n",
      "Iteration 2776, loss = 0.00840202\n",
      "Iteration 2777, loss = 0.00839500\n",
      "Iteration 2778, loss = 0.00838800\n",
      "Iteration 2779, loss = 0.00838100\n",
      "Iteration 2780, loss = 0.00837401\n",
      "Iteration 2781, loss = 0.00836702\n",
      "Iteration 2782, loss = 0.00836004\n",
      "Iteration 2783, loss = 0.00835307\n",
      "Iteration 2784, loss = 0.00834611\n",
      "Iteration 2785, loss = 0.00833915\n",
      "Iteration 2786, loss = 0.00833220\n",
      "Iteration 2787, loss = 0.00832526\n",
      "Iteration 2788, loss = 0.00831833\n",
      "Iteration 2789, loss = 0.00831140\n",
      "Iteration 2790, loss = 0.00830448\n",
      "Iteration 2791, loss = 0.00829757\n",
      "Iteration 2792, loss = 0.00829066\n",
      "Iteration 2793, loss = 0.00828376\n",
      "Iteration 2794, loss = 0.00827687\n",
      "Iteration 2795, loss = 0.00826998\n",
      "Iteration 2796, loss = 0.00826310\n",
      "Iteration 2797, loss = 0.00825623\n",
      "Iteration 2798, loss = 0.00824937\n",
      "Iteration 2799, loss = 0.00824251\n",
      "Iteration 2800, loss = 0.00823566\n",
      "Iteration 2801, loss = 0.00822882\n",
      "Iteration 2802, loss = 0.00822198\n",
      "Iteration 2803, loss = 0.00821515\n",
      "Iteration 2804, loss = 0.00820833\n",
      "Iteration 2805, loss = 0.00820151\n",
      "Iteration 2806, loss = 0.00819471\n",
      "Iteration 2807, loss = 0.00818790\n",
      "Iteration 2808, loss = 0.00818111\n",
      "Iteration 2809, loss = 0.00817432\n",
      "Iteration 2810, loss = 0.00816754\n",
      "Iteration 2811, loss = 0.00816077\n",
      "Iteration 2812, loss = 0.00815400\n",
      "Iteration 2813, loss = 0.00814724\n",
      "Iteration 2814, loss = 0.00814049\n",
      "Iteration 2815, loss = 0.00813374\n",
      "Iteration 2816, loss = 0.00812700\n",
      "Iteration 2817, loss = 0.00812027\n",
      "Iteration 2818, loss = 0.00811354\n",
      "Iteration 2819, loss = 0.00810683\n",
      "Iteration 2820, loss = 0.00810011\n",
      "Iteration 2821, loss = 0.00809341\n",
      "Iteration 2822, loss = 0.00808671\n",
      "Iteration 2823, loss = 0.00808002\n",
      "Iteration 2824, loss = 0.00807333\n",
      "Iteration 2825, loss = 0.00806666\n",
      "Iteration 2826, loss = 0.00805999\n",
      "Iteration 2827, loss = 0.00805332\n",
      "Iteration 2828, loss = 0.00804666\n",
      "Iteration 2829, loss = 0.00804001\n",
      "Iteration 2830, loss = 0.00803337\n",
      "Iteration 2831, loss = 0.00802673\n",
      "Iteration 2832, loss = 0.00802010\n",
      "Iteration 2833, loss = 0.00801348\n",
      "Iteration 2834, loss = 0.00800686\n",
      "Iteration 2835, loss = 0.00800025\n",
      "Iteration 2836, loss = 0.00799365\n",
      "Iteration 2837, loss = 0.00798705\n",
      "Iteration 2838, loss = 0.00798046\n",
      "Iteration 2839, loss = 0.00797388\n",
      "Iteration 2840, loss = 0.00796730\n",
      "Iteration 2841, loss = 0.00796073\n",
      "Iteration 2842, loss = 0.00795417\n",
      "Iteration 2843, loss = 0.00794761\n",
      "Iteration 2844, loss = 0.00794106\n",
      "Iteration 2845, loss = 0.00793452\n",
      "Iteration 2846, loss = 0.00792798\n",
      "Iteration 2847, loss = 0.00792145\n",
      "Iteration 2848, loss = 0.00791493\n",
      "Iteration 2849, loss = 0.00790841\n",
      "Iteration 2850, loss = 0.00790190\n",
      "Iteration 2851, loss = 0.00789540\n",
      "Iteration 2852, loss = 0.00788890\n",
      "Iteration 2853, loss = 0.00788241\n",
      "Iteration 2854, loss = 0.00787593\n",
      "Iteration 2855, loss = 0.00786945\n",
      "Iteration 2856, loss = 0.00786298\n",
      "Iteration 2857, loss = 0.00785651\n",
      "Iteration 2858, loss = 0.00785006\n",
      "Iteration 2859, loss = 0.00784361\n",
      "Iteration 2860, loss = 0.00783716\n",
      "Iteration 2861, loss = 0.00783072\n",
      "Iteration 2862, loss = 0.00782429\n",
      "Iteration 2863, loss = 0.00781787\n",
      "Iteration 2864, loss = 0.00781145\n",
      "Iteration 2865, loss = 0.00780504\n",
      "Iteration 2866, loss = 0.00779863\n",
      "Iteration 2867, loss = 0.00779223\n",
      "Iteration 2868, loss = 0.00778584\n",
      "Iteration 2869, loss = 0.00777945\n",
      "Iteration 2870, loss = 0.00777307\n",
      "Iteration 2871, loss = 0.00776670\n",
      "Iteration 2872, loss = 0.00776033\n",
      "Iteration 2873, loss = 0.00775397\n",
      "Iteration 2874, loss = 0.00774762\n",
      "Iteration 2875, loss = 0.00774127\n",
      "Iteration 2876, loss = 0.00773493\n",
      "Iteration 2877, loss = 0.00772860\n",
      "Iteration 2878, loss = 0.00772227\n",
      "Iteration 2879, loss = 0.00771595\n",
      "Iteration 2880, loss = 0.00770963\n",
      "Iteration 2881, loss = 0.00770332\n",
      "Iteration 2882, loss = 0.00769702\n",
      "Iteration 2883, loss = 0.00769072\n",
      "Iteration 2884, loss = 0.00768443\n",
      "Iteration 2885, loss = 0.00767815\n",
      "Iteration 2886, loss = 0.00767187\n",
      "Iteration 2887, loss = 0.00766560\n",
      "Iteration 2888, loss = 0.00765934\n",
      "Iteration 2889, loss = 0.00765308\n",
      "Iteration 2890, loss = 0.00764682\n",
      "Iteration 2891, loss = 0.00764058\n",
      "Iteration 2892, loss = 0.00763434\n",
      "Iteration 2893, loss = 0.00762811\n",
      "Iteration 2894, loss = 0.00762188\n",
      "Iteration 2895, loss = 0.00761566\n",
      "Iteration 2896, loss = 0.00760944\n",
      "Iteration 2897, loss = 0.00760324\n",
      "Iteration 2898, loss = 0.00759703\n",
      "Iteration 2899, loss = 0.00759084\n",
      "Iteration 2900, loss = 0.00758465\n",
      "Iteration 2901, loss = 0.00757847\n",
      "Iteration 2902, loss = 0.00757229\n",
      "Iteration 2903, loss = 0.00756612\n",
      "Iteration 2904, loss = 0.00755995\n",
      "Iteration 2905, loss = 0.00755379\n",
      "Iteration 2906, loss = 0.00754764\n",
      "Iteration 2907, loss = 0.00754150\n",
      "Iteration 2908, loss = 0.00753536\n",
      "Iteration 2909, loss = 0.00752922\n",
      "Iteration 2910, loss = 0.00752310\n",
      "Iteration 2911, loss = 0.00751697\n",
      "Iteration 2912, loss = 0.00751086\n",
      "Iteration 2913, loss = 0.00750475\n",
      "Iteration 2914, loss = 0.00749865\n",
      "Iteration 2915, loss = 0.00749255\n",
      "Iteration 2916, loss = 0.00748646\n",
      "Iteration 2917, loss = 0.00748037\n",
      "Iteration 2918, loss = 0.00747430\n",
      "Iteration 2919, loss = 0.00746822\n",
      "Iteration 2920, loss = 0.00746216\n",
      "Iteration 2921, loss = 0.00745610\n",
      "Iteration 2922, loss = 0.00745004\n",
      "Iteration 2923, loss = 0.00744399\n",
      "Iteration 2924, loss = 0.00743795\n",
      "Iteration 2925, loss = 0.00743192\n",
      "Iteration 2926, loss = 0.00742589\n",
      "Iteration 2927, loss = 0.00741986\n",
      "Iteration 2928, loss = 0.00741384\n",
      "Iteration 2929, loss = 0.00740783\n",
      "Iteration 2930, loss = 0.00740183\n",
      "Iteration 2931, loss = 0.00739583\n",
      "Iteration 2932, loss = 0.00738983\n",
      "Iteration 2933, loss = 0.00738384\n",
      "Iteration 2934, loss = 0.00737786\n",
      "Iteration 2935, loss = 0.00737189\n",
      "Iteration 2936, loss = 0.00736592\n",
      "Iteration 2937, loss = 0.00735995\n",
      "Iteration 2938, loss = 0.00735399\n",
      "Iteration 2939, loss = 0.00734804\n",
      "Iteration 2940, loss = 0.00734209\n",
      "Iteration 2941, loss = 0.00733615\n",
      "Iteration 2942, loss = 0.00733022\n",
      "Iteration 2943, loss = 0.00732429\n",
      "Iteration 2944, loss = 0.00731837\n",
      "Iteration 2945, loss = 0.00731245\n",
      "Iteration 2946, loss = 0.00730654\n",
      "Iteration 2947, loss = 0.00730064\n",
      "Iteration 2948, loss = 0.00729474\n",
      "Iteration 2949, loss = 0.00728884\n",
      "Iteration 2950, loss = 0.00728296\n",
      "Iteration 2951, loss = 0.00727708\n",
      "Iteration 2952, loss = 0.00727120\n",
      "Iteration 2953, loss = 0.00726533\n",
      "Iteration 2954, loss = 0.00725947\n",
      "Iteration 2955, loss = 0.00725361\n",
      "Iteration 2956, loss = 0.00724776\n",
      "Iteration 2957, loss = 0.00724191\n",
      "Iteration 2958, loss = 0.00723607\n",
      "Iteration 2959, loss = 0.00723023\n",
      "Iteration 2960, loss = 0.00722441\n",
      "Iteration 2961, loss = 0.00721858\n",
      "Iteration 2962, loss = 0.00721277\n",
      "Iteration 2963, loss = 0.00720695\n",
      "Iteration 2964, loss = 0.00720115\n",
      "Iteration 2965, loss = 0.00719535\n",
      "Iteration 2966, loss = 0.00718955\n",
      "Iteration 2967, loss = 0.00718376\n",
      "Iteration 2968, loss = 0.00717798\n",
      "Iteration 2969, loss = 0.00717220\n",
      "Iteration 2970, loss = 0.00716643\n",
      "Iteration 2971, loss = 0.00716067\n",
      "Iteration 2972, loss = 0.00715491\n",
      "Iteration 2973, loss = 0.00714915\n",
      "Iteration 2974, loss = 0.00714341\n",
      "Iteration 2975, loss = 0.00713766\n",
      "Iteration 2976, loss = 0.00713193\n",
      "Iteration 2977, loss = 0.00712619\n",
      "Iteration 2978, loss = 0.00712047\n",
      "Iteration 2979, loss = 0.00711475\n",
      "Iteration 2980, loss = 0.00710903\n",
      "Iteration 2981, loss = 0.00710332\n",
      "Iteration 2982, loss = 0.00709762\n",
      "Iteration 2983, loss = 0.00709192\n",
      "Iteration 2984, loss = 0.00708623\n",
      "Iteration 2985, loss = 0.00708055\n",
      "Iteration 2986, loss = 0.00707486\n",
      "Iteration 2987, loss = 0.00706919\n",
      "Iteration 2988, loss = 0.00706352\n",
      "Iteration 2989, loss = 0.00705786\n",
      "Iteration 2990, loss = 0.00705220\n",
      "Iteration 2991, loss = 0.00704655\n",
      "Iteration 2992, loss = 0.00704090\n",
      "Iteration 2993, loss = 0.00703526\n",
      "Iteration 2994, loss = 0.00702962\n",
      "Iteration 2995, loss = 0.00702399\n",
      "Iteration 2996, loss = 0.00701837\n",
      "Iteration 2997, loss = 0.00701275\n",
      "Iteration 2998, loss = 0.00700713\n",
      "Iteration 2999, loss = 0.00700152\n",
      "Iteration 3000, loss = 0.00699592\n",
      "Iteration 3001, loss = 0.00699032\n",
      "Iteration 3002, loss = 0.00698473\n",
      "Iteration 3003, loss = 0.00697915\n",
      "Iteration 3004, loss = 0.00697357\n",
      "Iteration 3005, loss = 0.00696799\n",
      "Iteration 3006, loss = 0.00696242\n",
      "Iteration 3007, loss = 0.00695686\n",
      "Iteration 3008, loss = 0.00695130\n",
      "Iteration 3009, loss = 0.00694574\n",
      "Iteration 3010, loss = 0.00694020\n",
      "Iteration 3011, loss = 0.00693465\n",
      "Iteration 3012, loss = 0.00692912\n",
      "Iteration 3013, loss = 0.00692359\n",
      "Iteration 3014, loss = 0.00691806\n",
      "Iteration 3015, loss = 0.00691254\n",
      "Iteration 3016, loss = 0.00690702\n",
      "Iteration 3017, loss = 0.00690151\n",
      "Iteration 3018, loss = 0.00689601\n",
      "Iteration 3019, loss = 0.00689051\n",
      "Iteration 3020, loss = 0.00688502\n",
      "Iteration 3021, loss = 0.00687953\n",
      "Iteration 3022, loss = 0.00687405\n",
      "Iteration 3023, loss = 0.00686857\n",
      "Iteration 3024, loss = 0.00686310\n",
      "Iteration 3025, loss = 0.00685763\n",
      "Iteration 3026, loss = 0.00685217\n",
      "Iteration 3027, loss = 0.00684671\n",
      "Iteration 3028, loss = 0.00684126\n",
      "Iteration 3029, loss = 0.00683581\n",
      "Iteration 3030, loss = 0.00683037\n",
      "Iteration 3031, loss = 0.00682494\n",
      "Iteration 3032, loss = 0.00681951\n",
      "Iteration 3033, loss = 0.00681409\n",
      "Iteration 3034, loss = 0.00680867\n",
      "Iteration 3035, loss = 0.00680325\n",
      "Iteration 3036, loss = 0.00679785\n",
      "Iteration 3037, loss = 0.00679244\n",
      "Iteration 3038, loss = 0.00678704\n",
      "Iteration 3039, loss = 0.00678165\n",
      "Iteration 3040, loss = 0.00677626\n",
      "Iteration 3041, loss = 0.00677088\n",
      "Iteration 3042, loss = 0.00676551\n",
      "Iteration 3043, loss = 0.00676013\n",
      "Iteration 3044, loss = 0.00675477\n",
      "Iteration 3045, loss = 0.00674941\n",
      "Iteration 3046, loss = 0.00674405\n",
      "Iteration 3047, loss = 0.00673870\n",
      "Iteration 3048, loss = 0.00673336\n",
      "Iteration 3049, loss = 0.00672802\n",
      "Iteration 3050, loss = 0.00672268\n",
      "Iteration 3051, loss = 0.00671735\n",
      "Iteration 3052, loss = 0.00671203\n",
      "Iteration 3053, loss = 0.00670671\n",
      "Iteration 3054, loss = 0.00670139\n",
      "Iteration 3055, loss = 0.00669608\n",
      "Iteration 3056, loss = 0.00669078\n",
      "Iteration 3057, loss = 0.00668548\n",
      "Iteration 3058, loss = 0.00668019\n",
      "Iteration 3059, loss = 0.00667490\n",
      "Iteration 3060, loss = 0.00666962\n",
      "Iteration 3061, loss = 0.00666434\n",
      "Iteration 3062, loss = 0.00665906\n",
      "Iteration 3063, loss = 0.00665380\n",
      "Iteration 3064, loss = 0.00664853\n",
      "Iteration 3065, loss = 0.00664328\n",
      "Iteration 3066, loss = 0.00663802\n",
      "Iteration 3067, loss = 0.00663278\n",
      "Iteration 3068, loss = 0.00662753\n",
      "Iteration 3069, loss = 0.00662230\n",
      "Iteration 3070, loss = 0.00661706\n",
      "Iteration 3071, loss = 0.00661184\n",
      "Iteration 3072, loss = 0.00660661\n",
      "Iteration 3073, loss = 0.00660140\n",
      "Iteration 3074, loss = 0.00659618\n",
      "Iteration 3075, loss = 0.00659098\n",
      "Iteration 3076, loss = 0.00658578\n",
      "Iteration 3077, loss = 0.00658058\n",
      "Iteration 3078, loss = 0.00657539\n",
      "Iteration 3079, loss = 0.00657020\n",
      "Iteration 3080, loss = 0.00656502\n",
      "Iteration 3081, loss = 0.00655984\n",
      "Iteration 3082, loss = 0.00655467\n",
      "Iteration 3083, loss = 0.00654950\n",
      "Iteration 3084, loss = 0.00654434\n",
      "Iteration 3085, loss = 0.00653918\n",
      "Iteration 3086, loss = 0.00653403\n",
      "Iteration 3087, loss = 0.00652888\n",
      "Iteration 3088, loss = 0.00652374\n",
      "Iteration 3089, loss = 0.00651860\n",
      "Iteration 3090, loss = 0.00651347\n",
      "Iteration 3091, loss = 0.00650834\n",
      "Iteration 3092, loss = 0.00650322\n",
      "Iteration 3093, loss = 0.00649811\n",
      "Iteration 3094, loss = 0.00649299\n",
      "Iteration 3095, loss = 0.00648789\n",
      "Iteration 3096, loss = 0.00648278\n",
      "Iteration 3097, loss = 0.00647769\n",
      "Iteration 3098, loss = 0.00647259\n",
      "Iteration 3099, loss = 0.00646750\n",
      "Iteration 3100, loss = 0.00646242\n",
      "Iteration 3101, loss = 0.00645734\n",
      "Iteration 3102, loss = 0.00645227\n",
      "Iteration 3103, loss = 0.00644720\n",
      "Iteration 3104, loss = 0.00644214\n",
      "Iteration 3105, loss = 0.00643708\n",
      "Iteration 3106, loss = 0.00643203\n",
      "Iteration 3107, loss = 0.00642698\n",
      "Iteration 3108, loss = 0.00642193\n",
      "Iteration 3109, loss = 0.00641689\n",
      "Iteration 3110, loss = 0.00641186\n",
      "Iteration 3111, loss = 0.00640683\n",
      "Iteration 3112, loss = 0.00640180\n",
      "Iteration 3113, loss = 0.00639679\n",
      "Iteration 3114, loss = 0.00639177\n",
      "Iteration 3115, loss = 0.00638676\n",
      "Iteration 3116, loss = 0.00638175\n",
      "Iteration 3117, loss = 0.00637675\n",
      "Iteration 3118, loss = 0.00637176\n",
      "Iteration 3119, loss = 0.00636677\n",
      "Iteration 3120, loss = 0.00636178\n",
      "Iteration 3121, loss = 0.00635680\n",
      "Iteration 3122, loss = 0.00635182\n",
      "Iteration 3123, loss = 0.00634685\n",
      "Iteration 3124, loss = 0.00634188\n",
      "Iteration 3125, loss = 0.00633692\n",
      "Iteration 3126, loss = 0.00633196\n",
      "Iteration 3127, loss = 0.00632701\n",
      "Iteration 3128, loss = 0.00632206\n",
      "Iteration 3129, loss = 0.00631712\n",
      "Iteration 3130, loss = 0.00631218\n",
      "Iteration 3131, loss = 0.00630724\n",
      "Iteration 3132, loss = 0.00630231\n",
      "Iteration 3133, loss = 0.00629739\n",
      "Iteration 3134, loss = 0.00629247\n",
      "Iteration 3135, loss = 0.00628755\n",
      "Iteration 3136, loss = 0.00628264\n",
      "Iteration 3137, loss = 0.00627774\n",
      "Iteration 3138, loss = 0.00627283\n",
      "Iteration 3139, loss = 0.00626794\n",
      "Iteration 3140, loss = 0.00626305\n",
      "Iteration 3141, loss = 0.00625816\n",
      "Iteration 3142, loss = 0.00625327\n",
      "Iteration 3143, loss = 0.00624840\n",
      "Iteration 3144, loss = 0.00624352\n",
      "Iteration 3145, loss = 0.00623865\n",
      "Iteration 3146, loss = 0.00623379\n",
      "Iteration 3147, loss = 0.00622893\n",
      "Iteration 3148, loss = 0.00622407\n",
      "Iteration 3149, loss = 0.00621922\n",
      "Iteration 3150, loss = 0.00621438\n",
      "Iteration 3151, loss = 0.00620954\n",
      "Iteration 3152, loss = 0.00620470\n",
      "Iteration 3153, loss = 0.00619987\n",
      "Iteration 3154, loss = 0.00619504\n",
      "Iteration 3155, loss = 0.00619022\n",
      "Iteration 3156, loss = 0.00618540\n",
      "Iteration 3157, loss = 0.00618059\n",
      "Iteration 3158, loss = 0.00617578\n",
      "Iteration 3159, loss = 0.00617097\n",
      "Iteration 3160, loss = 0.00616617\n",
      "Iteration 3161, loss = 0.00616138\n",
      "Iteration 3162, loss = 0.00615659\n",
      "Iteration 3163, loss = 0.00615180\n",
      "Iteration 3164, loss = 0.00614702\n",
      "Iteration 3165, loss = 0.00614224\n",
      "Iteration 3166, loss = 0.00613747\n",
      "Iteration 3167, loss = 0.00613270\n",
      "Iteration 3168, loss = 0.00612793\n",
      "Iteration 3169, loss = 0.00612317\n",
      "Iteration 3170, loss = 0.00611842\n",
      "Iteration 3171, loss = 0.00611367\n",
      "Iteration 3172, loss = 0.00610892\n",
      "Iteration 3173, loss = 0.00610418\n",
      "Iteration 3174, loss = 0.00609944\n",
      "Iteration 3175, loss = 0.00609471\n",
      "Iteration 3176, loss = 0.00608998\n",
      "Iteration 3177, loss = 0.00608526\n",
      "Iteration 3178, loss = 0.00608054\n",
      "Iteration 3179, loss = 0.00607583\n",
      "Iteration 3180, loss = 0.00607112\n",
      "Iteration 3181, loss = 0.00606641\n",
      "Iteration 3182, loss = 0.00606171\n",
      "Iteration 3183, loss = 0.00605701\n",
      "Iteration 3184, loss = 0.00605232\n",
      "Iteration 3185, loss = 0.00604763\n",
      "Iteration 3186, loss = 0.00604295\n",
      "Iteration 3187, loss = 0.00603827\n",
      "Iteration 3188, loss = 0.00603359\n",
      "Iteration 3189, loss = 0.00602892\n",
      "Iteration 3190, loss = 0.00602426\n",
      "Iteration 3191, loss = 0.00601959\n",
      "Iteration 3192, loss = 0.00601494\n",
      "Iteration 3193, loss = 0.00601028\n",
      "Iteration 3194, loss = 0.00600563\n",
      "Iteration 3195, loss = 0.00600099\n",
      "Iteration 3196, loss = 0.00599635\n",
      "Iteration 3197, loss = 0.00599171\n",
      "Iteration 3198, loss = 0.00598708\n",
      "Iteration 3199, loss = 0.00598246\n",
      "Iteration 3200, loss = 0.00597783\n",
      "Iteration 3201, loss = 0.00597322\n",
      "Iteration 3202, loss = 0.00596860\n",
      "Iteration 3203, loss = 0.00596399\n",
      "Iteration 3204, loss = 0.00595939\n",
      "Iteration 3205, loss = 0.00595478\n",
      "Iteration 3206, loss = 0.00595019\n",
      "Iteration 3207, loss = 0.00594560\n",
      "Iteration 3208, loss = 0.00594101\n",
      "Iteration 3209, loss = 0.00593642\n",
      "Iteration 3210, loss = 0.00593184\n",
      "Iteration 3211, loss = 0.00592727\n",
      "Iteration 3212, loss = 0.00592270\n",
      "Iteration 3213, loss = 0.00591813\n",
      "Iteration 3214, loss = 0.00591357\n",
      "Iteration 3215, loss = 0.00590901\n",
      "Iteration 3216, loss = 0.00590446\n",
      "Iteration 3217, loss = 0.00589991\n",
      "Iteration 3218, loss = 0.00589536\n",
      "Iteration 3219, loss = 0.00589082\n",
      "Iteration 3220, loss = 0.00588628\n",
      "Iteration 3221, loss = 0.00588175\n",
      "Iteration 3222, loss = 0.00587722\n",
      "Iteration 3223, loss = 0.00587270\n",
      "Iteration 3224, loss = 0.00586818\n",
      "Iteration 3225, loss = 0.00586366\n",
      "Iteration 3226, loss = 0.00585915\n",
      "Iteration 3227, loss = 0.00585464\n",
      "Iteration 3228, loss = 0.00585014\n",
      "Iteration 3229, loss = 0.00584564\n",
      "Iteration 3230, loss = 0.00584114\n",
      "Iteration 3231, loss = 0.00583665\n",
      "Iteration 3232, loss = 0.00583217\n",
      "Iteration 3233, loss = 0.00582768\n",
      "Iteration 3234, loss = 0.00582321\n",
      "Iteration 3235, loss = 0.00581873\n",
      "Iteration 3236, loss = 0.00581426\n",
      "Iteration 3237, loss = 0.00580980\n",
      "Iteration 3238, loss = 0.00580533\n",
      "Iteration 3239, loss = 0.00580088\n",
      "Iteration 3240, loss = 0.00579642\n",
      "Iteration 3241, loss = 0.00579197\n",
      "Iteration 3242, loss = 0.00578753\n",
      "Iteration 3243, loss = 0.00578309\n",
      "Iteration 3244, loss = 0.00577865\n",
      "Iteration 3245, loss = 0.00577422\n",
      "Iteration 3246, loss = 0.00576979\n",
      "Iteration 3247, loss = 0.00576536\n",
      "Iteration 3248, loss = 0.00576094\n",
      "Iteration 3249, loss = 0.00575653\n",
      "Iteration 3250, loss = 0.00575211\n",
      "Iteration 3251, loss = 0.00574771\n",
      "Iteration 3252, loss = 0.00574330\n",
      "Iteration 3253, loss = 0.00573890\n",
      "Iteration 3254, loss = 0.00573450\n",
      "Iteration 3255, loss = 0.00573011\n",
      "Iteration 3256, loss = 0.00572572\n",
      "Iteration 3257, loss = 0.00572134\n",
      "Iteration 3258, loss = 0.00571696\n",
      "Iteration 3259, loss = 0.00571258\n",
      "Iteration 3260, loss = 0.00570821\n",
      "Iteration 3261, loss = 0.00570385\n",
      "Iteration 3262, loss = 0.00569948\n",
      "Iteration 3263, loss = 0.00569512\n",
      "Iteration 3264, loss = 0.00569077\n",
      "Iteration 3265, loss = 0.00568641\n",
      "Iteration 3266, loss = 0.00568207\n",
      "Iteration 3267, loss = 0.00567772\n",
      "Iteration 3268, loss = 0.00567338\n",
      "Iteration 3269, loss = 0.00566905\n",
      "Iteration 3270, loss = 0.00566471\n",
      "Iteration 3271, loss = 0.00566039\n",
      "Iteration 3272, loss = 0.00565606\n",
      "Iteration 3273, loss = 0.00565174\n",
      "Iteration 3274, loss = 0.00564743\n",
      "Iteration 3275, loss = 0.00564312\n",
      "Iteration 3276, loss = 0.00563881\n",
      "Iteration 3277, loss = 0.00563450\n",
      "Iteration 3278, loss = 0.00563020\n",
      "Iteration 3279, loss = 0.00562591\n",
      "Iteration 3280, loss = 0.00562162\n",
      "Iteration 3281, loss = 0.00561733\n",
      "Iteration 3282, loss = 0.00561304\n",
      "Iteration 3283, loss = 0.00560876\n",
      "Iteration 3284, loss = 0.00560449\n",
      "Iteration 3285, loss = 0.00560021\n",
      "Iteration 3286, loss = 0.00559594\n",
      "Iteration 3287, loss = 0.00559168\n",
      "Iteration 3288, loss = 0.00558742\n",
      "Iteration 3289, loss = 0.00558316\n",
      "Iteration 3290, loss = 0.00557891\n",
      "Iteration 3291, loss = 0.00557466\n",
      "Iteration 3292, loss = 0.00557041\n",
      "Iteration 3293, loss = 0.00556617\n",
      "Iteration 3294, loss = 0.00556194\n",
      "Iteration 3295, loss = 0.00555770\n",
      "Iteration 3296, loss = 0.00555347\n",
      "Iteration 3297, loss = 0.00554925\n",
      "Iteration 3298, loss = 0.00554502\n",
      "Iteration 3299, loss = 0.00554081\n",
      "Iteration 3300, loss = 0.00553659\n",
      "Iteration 3301, loss = 0.00553238\n",
      "Iteration 3302, loss = 0.00552817\n",
      "Iteration 3303, loss = 0.00552397\n",
      "Iteration 3304, loss = 0.00551977\n",
      "Iteration 3305, loss = 0.00551558\n",
      "Iteration 3306, loss = 0.00551139\n",
      "Iteration 3307, loss = 0.00550720\n",
      "Iteration 3308, loss = 0.00550301\n",
      "Iteration 3309, loss = 0.00549883\n",
      "Iteration 3310, loss = 0.00549466\n",
      "Iteration 3311, loss = 0.00549049\n",
      "Iteration 3312, loss = 0.00548632\n",
      "Iteration 3313, loss = 0.00548215\n",
      "Iteration 3314, loss = 0.00547799\n",
      "Iteration 3315, loss = 0.00547383\n",
      "Iteration 3316, loss = 0.00546968\n",
      "Iteration 3317, loss = 0.00546553\n",
      "Iteration 3318, loss = 0.00546139\n",
      "Iteration 3319, loss = 0.00545724\n",
      "Iteration 3320, loss = 0.00545311\n",
      "Iteration 3321, loss = 0.00544897\n",
      "Iteration 3322, loss = 0.00544484\n",
      "Iteration 3323, loss = 0.00544071\n",
      "Iteration 3324, loss = 0.00543659\n",
      "Iteration 3325, loss = 0.00543247\n",
      "Iteration 3326, loss = 0.00542835\n",
      "Iteration 3327, loss = 0.00542424\n",
      "Iteration 3328, loss = 0.00542013\n",
      "Iteration 3329, loss = 0.00541603\n",
      "Iteration 3330, loss = 0.00541193\n",
      "Iteration 3331, loss = 0.00540783\n",
      "Iteration 3332, loss = 0.00540374\n",
      "Iteration 3333, loss = 0.00539965\n",
      "Iteration 3334, loss = 0.00539556\n",
      "Iteration 3335, loss = 0.00539148\n",
      "Iteration 3336, loss = 0.00538740\n",
      "Iteration 3337, loss = 0.00538332\n",
      "Iteration 3338, loss = 0.00537925\n",
      "Iteration 3339, loss = 0.00537519\n",
      "Iteration 3340, loss = 0.00537112\n",
      "Iteration 3341, loss = 0.00536706\n",
      "Iteration 3342, loss = 0.00536300\n",
      "Iteration 3343, loss = 0.00535895\n",
      "Iteration 3344, loss = 0.00535490\n",
      "Iteration 3345, loss = 0.00535086\n",
      "Iteration 3346, loss = 0.00534681\n",
      "Iteration 3347, loss = 0.00534278\n",
      "Iteration 3348, loss = 0.00533874\n",
      "Iteration 3349, loss = 0.00533471\n",
      "Iteration 3350, loss = 0.00533068\n",
      "Iteration 3351, loss = 0.00532666\n",
      "Iteration 3352, loss = 0.00532264\n",
      "Iteration 3353, loss = 0.00531862\n",
      "Iteration 3354, loss = 0.00531461\n",
      "Iteration 3355, loss = 0.00531060\n",
      "Iteration 3356, loss = 0.00530659\n",
      "Iteration 3357, loss = 0.00530259\n",
      "Iteration 3358, loss = 0.00529859\n",
      "Iteration 3359, loss = 0.00529460\n",
      "Iteration 3360, loss = 0.00529060\n",
      "Iteration 3361, loss = 0.00528662\n",
      "Iteration 3362, loss = 0.00528263\n",
      "Iteration 3363, loss = 0.00527865\n",
      "Iteration 3364, loss = 0.00527467\n",
      "Iteration 3365, loss = 0.00527070\n",
      "Iteration 3366, loss = 0.00526673\n",
      "Iteration 3367, loss = 0.00526276\n",
      "Iteration 3368, loss = 0.00525880\n",
      "Iteration 3369, loss = 0.00525484\n",
      "Iteration 3370, loss = 0.00525088\n",
      "Iteration 3371, loss = 0.00524693\n",
      "Iteration 3372, loss = 0.00524298\n",
      "Iteration 3373, loss = 0.00523904\n",
      "Iteration 3374, loss = 0.00523510\n",
      "Iteration 3375, loss = 0.00523116\n",
      "Iteration 3376, loss = 0.00522722\n",
      "Iteration 3377, loss = 0.00522329\n",
      "Iteration 3378, loss = 0.00521936\n",
      "Iteration 3379, loss = 0.00521544\n",
      "Iteration 3380, loss = 0.00521152\n",
      "Iteration 3381, loss = 0.00520760\n",
      "Iteration 3382, loss = 0.00520369\n",
      "Iteration 3383, loss = 0.00519978\n",
      "Iteration 3384, loss = 0.00519587\n",
      "Iteration 3385, loss = 0.00519197\n",
      "Iteration 3386, loss = 0.00518807\n",
      "Iteration 3387, loss = 0.00518417\n",
      "Iteration 3388, loss = 0.00518028\n",
      "Iteration 3389, loss = 0.00517639\n",
      "Iteration 3390, loss = 0.00517250\n",
      "Iteration 3391, loss = 0.00516862\n",
      "Iteration 3392, loss = 0.00516474\n",
      "Iteration 3393, loss = 0.00516087\n",
      "Iteration 3394, loss = 0.00515700\n",
      "Iteration 3395, loss = 0.00515313\n",
      "Iteration 3396, loss = 0.00514926\n",
      "Iteration 3397, loss = 0.00514540\n",
      "Iteration 3398, loss = 0.00514154\n",
      "Iteration 3399, loss = 0.00513769\n",
      "Iteration 3400, loss = 0.00513384\n",
      "Iteration 3401, loss = 0.00512999\n",
      "Iteration 3402, loss = 0.00512614\n",
      "Iteration 3403, loss = 0.00512230\n",
      "Iteration 3404, loss = 0.00511846\n",
      "Iteration 3405, loss = 0.00511463\n",
      "Iteration 3406, loss = 0.00511080\n",
      "Iteration 3407, loss = 0.00510697\n",
      "Iteration 3408, loss = 0.00510315\n",
      "Iteration 3409, loss = 0.00509933\n",
      "Iteration 3410, loss = 0.00509551\n",
      "Iteration 3411, loss = 0.00509170\n",
      "Iteration 3412, loss = 0.00508788\n",
      "Iteration 3413, loss = 0.00508408\n",
      "Iteration 3414, loss = 0.00508027\n",
      "Iteration 3415, loss = 0.00507647\n",
      "Iteration 3416, loss = 0.00507268\n",
      "Iteration 3417, loss = 0.00506888\n",
      "Iteration 3418, loss = 0.00506509\n",
      "Iteration 3419, loss = 0.00506130\n",
      "Iteration 3420, loss = 0.00505752\n",
      "Iteration 3421, loss = 0.00505374\n",
      "Iteration 3422, loss = 0.00504996\n",
      "Iteration 3423, loss = 0.00504619\n",
      "Iteration 3424, loss = 0.00504242\n",
      "Iteration 3425, loss = 0.00503865\n",
      "Iteration 3426, loss = 0.00503489\n",
      "Iteration 3427, loss = 0.00503113\n",
      "Iteration 3428, loss = 0.00502737\n",
      "Iteration 3429, loss = 0.00502362\n",
      "Iteration 3430, loss = 0.00501987\n",
      "Iteration 3431, loss = 0.00501612\n",
      "Iteration 3432, loss = 0.00501238\n",
      "Iteration 3433, loss = 0.00500863\n",
      "Iteration 3434, loss = 0.00500490\n",
      "Iteration 3435, loss = 0.00500116\n",
      "Iteration 3436, loss = 0.00499743\n",
      "Iteration 3437, loss = 0.00499371\n",
      "Iteration 3438, loss = 0.00498998\n",
      "Iteration 3439, loss = 0.00498626\n",
      "Iteration 3440, loss = 0.00498254\n",
      "Iteration 3441, loss = 0.00497883\n",
      "Iteration 3442, loss = 0.00497512\n",
      "Iteration 3443, loss = 0.00497141\n",
      "Iteration 3444, loss = 0.00496771\n",
      "Iteration 3445, loss = 0.00496400\n",
      "Iteration 3446, loss = 0.00496031\n",
      "Iteration 3447, loss = 0.00495661\n",
      "Iteration 3448, loss = 0.00495292\n",
      "Iteration 3449, loss = 0.00494923\n",
      "Iteration 3450, loss = 0.00494555\n",
      "Iteration 3451, loss = 0.00494186\n",
      "Iteration 3452, loss = 0.00493819\n",
      "Iteration 3453, loss = 0.00493451\n",
      "Iteration 3454, loss = 0.00493084\n",
      "Iteration 3455, loss = 0.00492717\n",
      "Iteration 3456, loss = 0.00492350\n",
      "Iteration 3457, loss = 0.00491984\n",
      "Iteration 3458, loss = 0.00491618\n",
      "Iteration 3459, loss = 0.00491253\n",
      "Iteration 3460, loss = 0.00490887\n",
      "Iteration 3461, loss = 0.00490522\n",
      "Iteration 3462, loss = 0.00490158\n",
      "Iteration 3463, loss = 0.00489793\n",
      "Iteration 3464, loss = 0.00489429\n",
      "Iteration 3465, loss = 0.00489066\n",
      "Iteration 3466, loss = 0.00488702\n",
      "Iteration 3467, loss = 0.00488339\n",
      "Iteration 3468, loss = 0.00487976\n",
      "Iteration 3469, loss = 0.00487614\n",
      "Iteration 3470, loss = 0.00487252\n",
      "Iteration 3471, loss = 0.00486890\n",
      "Iteration 3472, loss = 0.00486529\n",
      "Iteration 3473, loss = 0.00486167\n",
      "Iteration 3474, loss = 0.00485806\n",
      "Iteration 3475, loss = 0.00485446\n",
      "Iteration 3476, loss = 0.00485086\n",
      "Iteration 3477, loss = 0.00484726\n",
      "Iteration 3478, loss = 0.00484366\n",
      "Iteration 3479, loss = 0.00484007\n",
      "Iteration 3480, loss = 0.00483648\n",
      "Iteration 3481, loss = 0.00483289\n",
      "Iteration 3482, loss = 0.00482931\n",
      "Iteration 3483, loss = 0.00482573\n",
      "Iteration 3484, loss = 0.00482215\n",
      "Iteration 3485, loss = 0.00481858\n",
      "Iteration 3486, loss = 0.00481501\n",
      "Iteration 3487, loss = 0.00481144\n",
      "Iteration 3488, loss = 0.00480787\n",
      "Iteration 3489, loss = 0.00480431\n",
      "Iteration 3490, loss = 0.00480075\n",
      "Iteration 3491, loss = 0.00479720\n",
      "Iteration 3492, loss = 0.00479365\n",
      "Iteration 3493, loss = 0.00479010\n",
      "Iteration 3494, loss = 0.00478655\n",
      "Iteration 3495, loss = 0.00478301\n",
      "Iteration 3496, loss = 0.00477947\n",
      "Iteration 3497, loss = 0.00477593\n",
      "Iteration 3498, loss = 0.00477239\n",
      "Iteration 3499, loss = 0.00476886\n",
      "Iteration 3500, loss = 0.00476534\n",
      "Iteration 3501, loss = 0.00476181\n",
      "Iteration 3502, loss = 0.00475829\n",
      "Iteration 3503, loss = 0.00475477\n",
      "Iteration 3504, loss = 0.00475125\n",
      "Iteration 3505, loss = 0.00474774\n",
      "Iteration 3506, loss = 0.00474423\n",
      "Iteration 3507, loss = 0.00474073\n",
      "Iteration 3508, loss = 0.00473722\n",
      "Iteration 3509, loss = 0.00473372\n",
      "Iteration 3510, loss = 0.00473022\n",
      "Iteration 3511, loss = 0.00472673\n",
      "Iteration 3512, loss = 0.00472324\n",
      "Iteration 3513, loss = 0.00471975\n",
      "Iteration 3514, loss = 0.00471626\n",
      "Iteration 3515, loss = 0.00471278\n",
      "Iteration 3516, loss = 0.00470930\n",
      "Iteration 3517, loss = 0.00470583\n",
      "Iteration 3518, loss = 0.00470235\n",
      "Iteration 3519, loss = 0.00469888\n",
      "Iteration 3520, loss = 0.00469541\n",
      "Iteration 3521, loss = 0.00469195\n",
      "Iteration 3522, loss = 0.00468849\n",
      "Iteration 3523, loss = 0.00468503\n",
      "Iteration 3524, loss = 0.00468157\n",
      "Iteration 3525, loss = 0.00467812\n",
      "Iteration 3526, loss = 0.00467467\n",
      "Iteration 3527, loss = 0.00467122\n",
      "Iteration 3528, loss = 0.00466778\n",
      "Iteration 3529, loss = 0.00466434\n",
      "Iteration 3530, loss = 0.00466090\n",
      "Iteration 3531, loss = 0.00465747\n",
      "Iteration 3532, loss = 0.00465404\n",
      "Iteration 3533, loss = 0.00465061\n",
      "Iteration 3534, loss = 0.00464718\n",
      "Iteration 3535, loss = 0.00464376\n",
      "Iteration 3536, loss = 0.00464034\n",
      "Iteration 3537, loss = 0.00463692\n",
      "Iteration 3538, loss = 0.00463351\n",
      "Iteration 3539, loss = 0.00463009\n",
      "Iteration 3540, loss = 0.00462669\n",
      "Iteration 3541, loss = 0.00462328\n",
      "Iteration 3542, loss = 0.00461988\n",
      "Iteration 3543, loss = 0.00461648\n",
      "Iteration 3544, loss = 0.00461308\n",
      "Iteration 3545, loss = 0.00460969\n",
      "Iteration 3546, loss = 0.00460630\n",
      "Iteration 3547, loss = 0.00460291\n",
      "Iteration 3548, loss = 0.00459952\n",
      "Iteration 3549, loss = 0.00459614\n",
      "Iteration 3550, loss = 0.00459276\n",
      "Iteration 3551, loss = 0.00458938\n",
      "Iteration 3552, loss = 0.00458601\n",
      "Iteration 3553, loss = 0.00458264\n",
      "Iteration 3554, loss = 0.00457927\n",
      "Iteration 3555, loss = 0.00457591\n",
      "Iteration 3556, loss = 0.00457255\n",
      "Iteration 3557, loss = 0.00456919\n",
      "Iteration 3558, loss = 0.00456583\n",
      "Iteration 3559, loss = 0.00456248\n",
      "Iteration 3560, loss = 0.00455913\n",
      "Iteration 3561, loss = 0.00455578\n",
      "Iteration 3562, loss = 0.00455243\n",
      "Iteration 3563, loss = 0.00454909\n",
      "Iteration 3564, loss = 0.00454575\n",
      "Iteration 3565, loss = 0.00454242\n",
      "Iteration 3566, loss = 0.00453908\n",
      "Iteration 3567, loss = 0.00453575\n",
      "Iteration 3568, loss = 0.00453242\n",
      "Iteration 3569, loss = 0.00452910\n",
      "Iteration 3570, loss = 0.00452578\n",
      "Iteration 3571, loss = 0.00452246\n",
      "Iteration 3572, loss = 0.00451914\n",
      "Iteration 3573, loss = 0.00451583\n",
      "Iteration 3574, loss = 0.00451251\n",
      "Iteration 3575, loss = 0.00450921\n",
      "Iteration 3576, loss = 0.00450590\n",
      "Iteration 3577, loss = 0.00450260\n",
      "Iteration 3578, loss = 0.00449930\n",
      "Iteration 3579, loss = 0.00449600\n",
      "Iteration 3580, loss = 0.00449271\n",
      "Iteration 3581, loss = 0.00448942\n",
      "Iteration 3582, loss = 0.00448613\n",
      "Iteration 3583, loss = 0.00448284\n",
      "Iteration 3584, loss = 0.00447956\n",
      "Iteration 3585, loss = 0.00447628\n",
      "Iteration 3586, loss = 0.00447300\n",
      "Iteration 3587, loss = 0.00446973\n",
      "Iteration 3588, loss = 0.00446646\n",
      "Iteration 3589, loss = 0.00446319\n",
      "Iteration 3590, loss = 0.00445992\n",
      "Iteration 3591, loss = 0.00445666\n",
      "Iteration 3592, loss = 0.00445340\n",
      "Iteration 3593, loss = 0.00445014\n",
      "Iteration 3594, loss = 0.00444688\n",
      "Iteration 3595, loss = 0.00444363\n",
      "Iteration 3596, loss = 0.00444038\n",
      "Iteration 3597, loss = 0.00443713\n",
      "Iteration 3598, loss = 0.00443389\n",
      "Iteration 3599, loss = 0.00443065\n",
      "Iteration 3600, loss = 0.00442741\n",
      "Iteration 3601, loss = 0.00442417\n",
      "Iteration 3602, loss = 0.00442094\n",
      "Iteration 3603, loss = 0.00441771\n",
      "Iteration 3604, loss = 0.00441448\n",
      "Iteration 3605, loss = 0.00441125\n",
      "Iteration 3606, loss = 0.00440803\n",
      "Iteration 3607, loss = 0.00440481\n",
      "Iteration 3608, loss = 0.00440159\n",
      "Iteration 3609, loss = 0.00439838\n",
      "Iteration 3610, loss = 0.00439517\n",
      "Iteration 3611, loss = 0.00439196\n",
      "Iteration 3612, loss = 0.00438875\n",
      "Iteration 3613, loss = 0.00438555\n",
      "Iteration 3614, loss = 0.00438235\n",
      "Iteration 3615, loss = 0.00437915\n",
      "Iteration 3616, loss = 0.00437595\n",
      "Iteration 3617, loss = 0.00437276\n",
      "Iteration 3618, loss = 0.00436957\n",
      "Iteration 3619, loss = 0.00436638\n",
      "Iteration 3620, loss = 0.00436320\n",
      "Iteration 3621, loss = 0.00436002\n",
      "Iteration 3622, loss = 0.00435684\n",
      "Iteration 3623, loss = 0.00435366\n",
      "Iteration 3624, loss = 0.00435049\n",
      "Iteration 3625, loss = 0.00434732\n",
      "Iteration 3626, loss = 0.00434415\n",
      "Iteration 3627, loss = 0.00434098\n",
      "Iteration 3628, loss = 0.00433782\n",
      "Iteration 3629, loss = 0.00433466\n",
      "Iteration 3630, loss = 0.00433150\n",
      "Iteration 3631, loss = 0.00432834\n",
      "Iteration 3632, loss = 0.00432519\n",
      "Iteration 3633, loss = 0.00432204\n",
      "Iteration 3634, loss = 0.00431889\n",
      "Iteration 3635, loss = 0.00431575\n",
      "Iteration 3636, loss = 0.00431260\n",
      "Iteration 3637, loss = 0.00430946\n",
      "Iteration 3638, loss = 0.00430633\n",
      "Iteration 3639, loss = 0.00430319\n",
      "Iteration 3640, loss = 0.00430006\n",
      "Iteration 3641, loss = 0.00429693\n",
      "Iteration 3642, loss = 0.00429380\n",
      "Iteration 3643, loss = 0.00429068\n",
      "Iteration 3644, loss = 0.00428756\n",
      "Iteration 3645, loss = 0.00428444\n",
      "Iteration 3646, loss = 0.00428132\n",
      "Iteration 3647, loss = 0.00427821\n",
      "Iteration 3648, loss = 0.00427510\n",
      "Iteration 3649, loss = 0.00427199\n",
      "Iteration 3650, loss = 0.00426889\n",
      "Iteration 3651, loss = 0.00426578\n",
      "Iteration 3652, loss = 0.00426268\n",
      "Iteration 3653, loss = 0.00425958\n",
      "Iteration 3654, loss = 0.00425649\n",
      "Iteration 3655, loss = 0.00425339\n",
      "Iteration 3656, loss = 0.00425030\n",
      "Iteration 3657, loss = 0.00424722\n",
      "Iteration 3658, loss = 0.00424413\n",
      "Iteration 3659, loss = 0.00424105\n",
      "Iteration 3660, loss = 0.00423797\n",
      "Iteration 3661, loss = 0.00423489\n",
      "Iteration 3662, loss = 0.00423182\n",
      "Iteration 3663, loss = 0.00422874\n",
      "Iteration 3664, loss = 0.00422567\n",
      "Iteration 3665, loss = 0.00422261\n",
      "Iteration 3666, loss = 0.00421954\n",
      "Iteration 3667, loss = 0.00421648\n",
      "Iteration 3668, loss = 0.00421342\n",
      "Iteration 3669, loss = 0.00421036\n",
      "Iteration 3670, loss = 0.00420731\n",
      "Iteration 3671, loss = 0.00420425\n",
      "Iteration 3672, loss = 0.00420121\n",
      "Iteration 3673, loss = 0.00419816\n",
      "Iteration 3674, loss = 0.00419511\n",
      "Iteration 3675, loss = 0.00419207\n",
      "Iteration 3676, loss = 0.00418903\n",
      "Iteration 3677, loss = 0.00418599\n",
      "Iteration 3678, loss = 0.00418296\n",
      "Iteration 3679, loss = 0.00417993\n",
      "Iteration 3680, loss = 0.00417690\n",
      "Iteration 3681, loss = 0.00417387\n",
      "Iteration 3682, loss = 0.00417085\n",
      "Iteration 3683, loss = 0.00416783\n",
      "Iteration 3684, loss = 0.00416481\n",
      "Iteration 3685, loss = 0.00416179\n",
      "Iteration 3686, loss = 0.00415877\n",
      "Iteration 3687, loss = 0.00415576\n",
      "Iteration 3688, loss = 0.00415275\n",
      "Iteration 3689, loss = 0.00414975\n",
      "Iteration 3690, loss = 0.00414674\n",
      "Iteration 3691, loss = 0.00414374\n",
      "Iteration 3692, loss = 0.00414074\n",
      "Iteration 3693, loss = 0.00413774\n",
      "Iteration 3694, loss = 0.00413475\n",
      "Iteration 3695, loss = 0.00413175\n",
      "Iteration 3696, loss = 0.00412876\n",
      "Iteration 3697, loss = 0.00412578\n",
      "Iteration 3698, loss = 0.00412279\n",
      "Iteration 3699, loss = 0.00411981\n",
      "Iteration 3700, loss = 0.00411683\n",
      "Iteration 3701, loss = 0.00411385\n",
      "Iteration 3702, loss = 0.00411088\n",
      "Iteration 3703, loss = 0.00410791\n",
      "Iteration 3704, loss = 0.00410494\n",
      "Iteration 3705, loss = 0.00410197\n",
      "Iteration 3706, loss = 0.00409900\n",
      "Iteration 3707, loss = 0.00409604\n",
      "Iteration 3708, loss = 0.00409308\n",
      "Iteration 3709, loss = 0.00409012\n",
      "Iteration 3710, loss = 0.00408717\n",
      "Iteration 3711, loss = 0.00408421\n",
      "Iteration 3712, loss = 0.00408126\n",
      "Iteration 3713, loss = 0.00407831\n",
      "Iteration 3714, loss = 0.00407537\n",
      "Iteration 3715, loss = 0.00407242\n",
      "Iteration 3716, loss = 0.00406948\n",
      "Iteration 3717, loss = 0.00406654\n",
      "Iteration 3718, loss = 0.00406361\n",
      "Iteration 3719, loss = 0.00406067\n",
      "Iteration 3720, loss = 0.00405774\n",
      "Iteration 3721, loss = 0.00405481\n",
      "Iteration 3722, loss = 0.00405189\n",
      "Iteration 3723, loss = 0.00404896\n",
      "Iteration 3724, loss = 0.00404604\n",
      "Iteration 3725, loss = 0.00404312\n",
      "Iteration 3726, loss = 0.00404020\n",
      "Iteration 3727, loss = 0.00403729\n",
      "Iteration 3728, loss = 0.00403438\n",
      "Iteration 3729, loss = 0.00403147\n",
      "Iteration 3730, loss = 0.00402856\n",
      "Iteration 3731, loss = 0.00402565\n",
      "Iteration 3732, loss = 0.00402275\n",
      "Iteration 3733, loss = 0.00401985\n",
      "Iteration 3734, loss = 0.00401695\n",
      "Iteration 3735, loss = 0.00401406\n",
      "Iteration 3736, loss = 0.00401116\n",
      "Iteration 3737, loss = 0.00400827\n",
      "Iteration 3738, loss = 0.00400538\n",
      "Iteration 3739, loss = 0.00400250\n",
      "Iteration 3740, loss = 0.00399961\n",
      "Iteration 3741, loss = 0.00399673\n",
      "Iteration 3742, loss = 0.00399385\n",
      "Iteration 3743, loss = 0.00399097\n",
      "Iteration 3744, loss = 0.00398810\n",
      "Iteration 3745, loss = 0.00398523\n",
      "Iteration 3746, loss = 0.00398236\n",
      "Iteration 3747, loss = 0.00397949\n",
      "Iteration 3748, loss = 0.00397662\n",
      "Iteration 3749, loss = 0.00397376\n",
      "Iteration 3750, loss = 0.00397090\n",
      "Iteration 3751, loss = 0.00396804\n",
      "Iteration 3752, loss = 0.00396519\n",
      "Iteration 3753, loss = 0.00396233\n",
      "Iteration 3754, loss = 0.00395948\n",
      "Iteration 3755, loss = 0.00395663\n",
      "Iteration 3756, loss = 0.00395379\n",
      "Iteration 3757, loss = 0.00395094\n",
      "Iteration 3758, loss = 0.00394810\n",
      "Iteration 3759, loss = 0.00394526\n",
      "Iteration 3760, loss = 0.00394242\n",
      "Iteration 3761, loss = 0.00393959\n",
      "Iteration 3762, loss = 0.00393675\n",
      "Iteration 3763, loss = 0.00393392\n",
      "Iteration 3764, loss = 0.00393109\n",
      "Iteration 3765, loss = 0.00392827\n",
      "Iteration 3766, loss = 0.00392544\n",
      "Iteration 3767, loss = 0.00392262\n",
      "Iteration 3768, loss = 0.00391980\n",
      "Iteration 3769, loss = 0.00391699\n",
      "Iteration 3770, loss = 0.00391417\n",
      "Iteration 3771, loss = 0.00391136\n",
      "Iteration 3772, loss = 0.00390855\n",
      "Iteration 3773, loss = 0.00390574\n",
      "Iteration 3774, loss = 0.00390293\n",
      "Iteration 3775, loss = 0.00390013\n",
      "Iteration 3776, loss = 0.00389733\n",
      "Iteration 3777, loss = 0.00389453\n",
      "Iteration 3778, loss = 0.00389173\n",
      "Iteration 3779, loss = 0.00388894\n",
      "Iteration 3780, loss = 0.00388615\n",
      "Iteration 3781, loss = 0.00388336\n",
      "Iteration 3782, loss = 0.00388057\n",
      "Iteration 3783, loss = 0.00387778\n",
      "Iteration 3784, loss = 0.00387500\n",
      "Iteration 3785, loss = 0.00387222\n",
      "Iteration 3786, loss = 0.00386944\n",
      "Iteration 3787, loss = 0.00386667\n",
      "Iteration 3788, loss = 0.00386389\n",
      "Iteration 3789, loss = 0.00386112\n",
      "Iteration 3790, loss = 0.00385835\n",
      "Iteration 3791, loss = 0.00385558\n",
      "Iteration 3792, loss = 0.00385282\n",
      "Iteration 3793, loss = 0.00385005\n",
      "Iteration 3794, loss = 0.00384729\n",
      "Iteration 3795, loss = 0.00384453\n",
      "Iteration 3796, loss = 0.00384178\n",
      "Iteration 3797, loss = 0.00383902\n",
      "Iteration 3798, loss = 0.00383627\n",
      "Iteration 3799, loss = 0.00383352\n",
      "Iteration 3800, loss = 0.00383077\n",
      "Iteration 3801, loss = 0.00382803\n",
      "Iteration 3802, loss = 0.00382528\n",
      "Iteration 3803, loss = 0.00382254\n",
      "Iteration 3804, loss = 0.00381980\n",
      "Iteration 3805, loss = 0.00381707\n",
      "Iteration 3806, loss = 0.00381433\n",
      "Iteration 3807, loss = 0.00381160\n",
      "Iteration 3808, loss = 0.00380887\n",
      "Iteration 3809, loss = 0.00380614\n",
      "Iteration 3810, loss = 0.00380342\n",
      "Iteration 3811, loss = 0.00380069\n",
      "Iteration 3812, loss = 0.00379797\n",
      "Iteration 3813, loss = 0.00379525\n",
      "Iteration 3814, loss = 0.00379254\n",
      "Iteration 3815, loss = 0.00378982\n",
      "Iteration 3816, loss = 0.00378711\n",
      "Iteration 3817, loss = 0.00378440\n",
      "Iteration 3818, loss = 0.00378169\n",
      "Iteration 3819, loss = 0.00377898\n",
      "Iteration 3820, loss = 0.00377628\n",
      "Iteration 3821, loss = 0.00377358\n",
      "Iteration 3822, loss = 0.00377088\n",
      "Iteration 3823, loss = 0.00376818\n",
      "Iteration 3824, loss = 0.00376548\n",
      "Iteration 3825, loss = 0.00376279\n",
      "Iteration 3826, loss = 0.00376010\n",
      "Iteration 3827, loss = 0.00375741\n",
      "Iteration 3828, loss = 0.00375472\n",
      "Iteration 3829, loss = 0.00375204\n",
      "Iteration 3830, loss = 0.00374935\n",
      "Iteration 3831, loss = 0.00374667\n",
      "Iteration 3832, loss = 0.00374400\n",
      "Iteration 3833, loss = 0.00374132\n",
      "Iteration 3834, loss = 0.00373864\n",
      "Iteration 3835, loss = 0.00373597\n",
      "Iteration 3836, loss = 0.00373330\n",
      "Iteration 3837, loss = 0.00373063\n",
      "Iteration 3838, loss = 0.00372797\n",
      "Iteration 3839, loss = 0.00372530\n",
      "Iteration 3840, loss = 0.00372264\n",
      "Iteration 3841, loss = 0.00371998\n",
      "Iteration 3842, loss = 0.00371733\n",
      "Iteration 3843, loss = 0.00371467\n",
      "Iteration 3844, loss = 0.00371202\n",
      "Iteration 3845, loss = 0.00370937\n",
      "Iteration 3846, loss = 0.00370672\n",
      "Iteration 3847, loss = 0.00370407\n",
      "Iteration 3848, loss = 0.00370143\n",
      "Iteration 3849, loss = 0.00369878\n",
      "Iteration 3850, loss = 0.00369614\n",
      "Iteration 3851, loss = 0.00369350\n",
      "Iteration 3852, loss = 0.00369087\n",
      "Iteration 3853, loss = 0.00368823\n",
      "Iteration 3854, loss = 0.00368560\n",
      "Iteration 3855, loss = 0.00368297\n",
      "Iteration 3856, loss = 0.00368034\n",
      "Iteration 3857, loss = 0.00367772\n",
      "Iteration 3858, loss = 0.00367509\n",
      "Iteration 3859, loss = 0.00367247\n",
      "Iteration 3860, loss = 0.00366985\n",
      "Iteration 3861, loss = 0.00366723\n",
      "Iteration 3862, loss = 0.00366462\n",
      "Iteration 3863, loss = 0.00366200\n",
      "Iteration 3864, loss = 0.00365939\n",
      "Iteration 3865, loss = 0.00365678\n",
      "Iteration 3866, loss = 0.00365417\n",
      "Iteration 3867, loss = 0.00365157\n",
      "Iteration 3868, loss = 0.00364897\n",
      "Iteration 3869, loss = 0.00364636\n",
      "Iteration 3870, loss = 0.00364376\n",
      "Iteration 3871, loss = 0.00364117\n",
      "Iteration 3872, loss = 0.00363857\n",
      "Iteration 3873, loss = 0.00363598\n",
      "Iteration 3874, loss = 0.00363339\n",
      "Iteration 3875, loss = 0.00363080\n",
      "Iteration 3876, loss = 0.00362821\n",
      "Iteration 3877, loss = 0.00362563\n",
      "Iteration 3878, loss = 0.00362304\n",
      "Iteration 3879, loss = 0.00362046\n",
      "Iteration 3880, loss = 0.00361788\n",
      "Iteration 3881, loss = 0.00361531\n",
      "Iteration 3882, loss = 0.00361273\n",
      "Iteration 3883, loss = 0.00361016\n",
      "Iteration 3884, loss = 0.00360759\n",
      "Iteration 3885, loss = 0.00360502\n",
      "Iteration 3886, loss = 0.00360245\n",
      "Iteration 3887, loss = 0.00359988\n",
      "Iteration 3888, loss = 0.00359732\n",
      "Iteration 3889, loss = 0.00359476\n",
      "Iteration 3890, loss = 0.00359220\n",
      "Iteration 3891, loss = 0.00358964\n",
      "Iteration 3892, loss = 0.00358709\n",
      "Iteration 3893, loss = 0.00358454\n",
      "Iteration 3894, loss = 0.00358198\n",
      "Iteration 3895, loss = 0.00357944\n",
      "Iteration 3896, loss = 0.00357689\n",
      "Iteration 3897, loss = 0.00357434\n",
      "Iteration 3898, loss = 0.00357180\n",
      "Iteration 3899, loss = 0.00356926\n",
      "Iteration 3900, loss = 0.00356672\n",
      "Iteration 3901, loss = 0.00356418\n",
      "Iteration 3902, loss = 0.00356165\n",
      "Iteration 3903, loss = 0.00355911\n",
      "Iteration 3904, loss = 0.00355658\n",
      "Iteration 3905, loss = 0.00355405\n",
      "Iteration 3906, loss = 0.00355153\n",
      "Iteration 3907, loss = 0.00354900\n",
      "Iteration 3908, loss = 0.00354648\n",
      "Iteration 3909, loss = 0.00354396\n",
      "Iteration 3910, loss = 0.00354144\n",
      "Iteration 3911, loss = 0.00353892\n",
      "Iteration 3912, loss = 0.00353640\n",
      "Iteration 3913, loss = 0.00353389\n",
      "Iteration 3914, loss = 0.00353138\n",
      "Iteration 3915, loss = 0.00352887\n",
      "Iteration 3916, loss = 0.00352636\n",
      "Iteration 3917, loss = 0.00352385\n",
      "Iteration 3918, loss = 0.00352135\n",
      "Iteration 3919, loss = 0.00351885\n",
      "Iteration 3920, loss = 0.00351635\n",
      "Iteration 3921, loss = 0.00351385\n",
      "Iteration 3922, loss = 0.00351135\n",
      "Iteration 3923, loss = 0.00350886\n",
      "Iteration 3924, loss = 0.00350636\n",
      "Iteration 3925, loss = 0.00350387\n",
      "Iteration 3926, loss = 0.00350139\n",
      "Iteration 3927, loss = 0.00349890\n",
      "Iteration 3928, loss = 0.00349641\n",
      "Iteration 3929, loss = 0.00349393\n",
      "Iteration 3930, loss = 0.00349145\n",
      "Iteration 3931, loss = 0.00348897\n",
      "Iteration 3932, loss = 0.00348649\n",
      "Iteration 3933, loss = 0.00348402\n",
      "Iteration 3934, loss = 0.00348155\n",
      "Iteration 3935, loss = 0.00347907\n",
      "Iteration 3936, loss = 0.00347660\n",
      "Iteration 3937, loss = 0.00347414\n",
      "Iteration 3938, loss = 0.00347167\n",
      "Iteration 3939, loss = 0.00346921\n",
      "Iteration 3940, loss = 0.00346674\n",
      "Iteration 3941, loss = 0.00346428\n",
      "Iteration 3942, loss = 0.00346183\n",
      "Iteration 3943, loss = 0.00345937\n",
      "Iteration 3944, loss = 0.00345692\n",
      "Iteration 3945, loss = 0.00345446\n",
      "Iteration 3946, loss = 0.00345201\n",
      "Iteration 3947, loss = 0.00344956\n",
      "Iteration 3948, loss = 0.00344712\n",
      "Iteration 3949, loss = 0.00344467\n",
      "Iteration 3950, loss = 0.00344223\n",
      "Iteration 3951, loss = 0.00343979\n",
      "Iteration 3952, loss = 0.00343735\n",
      "Iteration 3953, loss = 0.00343491\n",
      "Iteration 3954, loss = 0.00343247\n",
      "Iteration 3955, loss = 0.00343004\n",
      "Iteration 3956, loss = 0.00342761\n",
      "Iteration 3957, loss = 0.00342518\n",
      "Iteration 3958, loss = 0.00342275\n",
      "Iteration 3959, loss = 0.00342032\n",
      "Iteration 3960, loss = 0.00341790\n",
      "Iteration 3961, loss = 0.00341547\n",
      "Iteration 3962, loss = 0.00341305\n",
      "Iteration 3963, loss = 0.00341063\n",
      "Iteration 3964, loss = 0.00340822\n",
      "Iteration 3965, loss = 0.00340580\n",
      "Iteration 3966, loss = 0.00340339\n",
      "Iteration 3967, loss = 0.00340098\n",
      "Iteration 3968, loss = 0.00339857\n",
      "Iteration 3969, loss = 0.00339616\n",
      "Iteration 3970, loss = 0.00339375\n",
      "Iteration 3971, loss = 0.00339135\n",
      "Iteration 3972, loss = 0.00338894\n",
      "Iteration 3973, loss = 0.00338654\n",
      "Iteration 3974, loss = 0.00338414\n",
      "Iteration 3975, loss = 0.00338175\n",
      "Iteration 3976, loss = 0.00337935\n",
      "Iteration 3977, loss = 0.00337696\n",
      "Iteration 3978, loss = 0.00337457\n",
      "Iteration 3979, loss = 0.00337218\n",
      "Iteration 3980, loss = 0.00336979\n",
      "Iteration 3981, loss = 0.00336740\n",
      "Iteration 3982, loss = 0.00336502\n",
      "Iteration 3983, loss = 0.00336263\n",
      "Iteration 3984, loss = 0.00336025\n",
      "Iteration 3985, loss = 0.00335787\n",
      "Iteration 3986, loss = 0.00335550\n",
      "Iteration 3987, loss = 0.00335312\n",
      "Iteration 3988, loss = 0.00335075\n",
      "Iteration 3989, loss = 0.00334838\n",
      "Iteration 3990, loss = 0.00334601\n",
      "Iteration 3991, loss = 0.00334364\n",
      "Iteration 3992, loss = 0.00334127\n",
      "Iteration 3993, loss = 0.00333891\n",
      "Iteration 3994, loss = 0.00333654\n",
      "Iteration 3995, loss = 0.00333418\n",
      "Iteration 3996, loss = 0.00333182\n",
      "Iteration 3997, loss = 0.00332946\n",
      "Iteration 3998, loss = 0.00332711\n",
      "Iteration 3999, loss = 0.00332475\n",
      "Iteration 4000, loss = 0.00332240\n",
      "Iteration 4001, loss = 0.00332005\n",
      "Iteration 4002, loss = 0.00331770\n",
      "Iteration 4003, loss = 0.00331535\n",
      "Iteration 4004, loss = 0.00331301\n",
      "Iteration 4005, loss = 0.00331067\n",
      "Iteration 4006, loss = 0.00330832\n",
      "Iteration 4007, loss = 0.00330598\n",
      "Iteration 4008, loss = 0.00330365\n",
      "Iteration 4009, loss = 0.00330131\n",
      "Iteration 4010, loss = 0.00329897\n",
      "Iteration 4011, loss = 0.00329664\n",
      "Iteration 4012, loss = 0.00329431\n",
      "Iteration 4013, loss = 0.00329198\n",
      "Iteration 4014, loss = 0.00328965\n",
      "Iteration 4015, loss = 0.00328733\n",
      "Iteration 4016, loss = 0.00328500\n",
      "Iteration 4017, loss = 0.00328268\n",
      "Iteration 4018, loss = 0.00328036\n",
      "Iteration 4019, loss = 0.00327804\n",
      "Iteration 4020, loss = 0.00327572\n",
      "Iteration 4021, loss = 0.00327340\n",
      "Iteration 4022, loss = 0.00327109\n",
      "Iteration 4023, loss = 0.00326878\n",
      "Iteration 4024, loss = 0.00326647\n",
      "Iteration 4025, loss = 0.00326416\n",
      "Iteration 4026, loss = 0.00326185\n",
      "Iteration 4027, loss = 0.00325955\n",
      "Iteration 4028, loss = 0.00325724\n",
      "Iteration 4029, loss = 0.00325494\n",
      "Iteration 4030, loss = 0.00325264\n",
      "Iteration 4031, loss = 0.00325034\n",
      "Iteration 4032, loss = 0.00324804\n",
      "Iteration 4033, loss = 0.00324575\n",
      "Iteration 4034, loss = 0.00324346\n",
      "Iteration 4035, loss = 0.00324116\n",
      "Iteration 4036, loss = 0.00323887\n",
      "Iteration 4037, loss = 0.00323658\n",
      "Iteration 4038, loss = 0.00323430\n",
      "Iteration 4039, loss = 0.00323201\n",
      "Iteration 4040, loss = 0.00322973\n",
      "Iteration 4041, loss = 0.00322745\n",
      "Iteration 4042, loss = 0.00322517\n",
      "Iteration 4043, loss = 0.00322289\n",
      "Iteration 4044, loss = 0.00322061\n",
      "Iteration 4045, loss = 0.00321834\n",
      "Iteration 4046, loss = 0.00321606\n",
      "Iteration 4047, loss = 0.00321379\n",
      "Iteration 4048, loss = 0.00321152\n",
      "Iteration 4049, loss = 0.00320925\n",
      "Iteration 4050, loss = 0.00320699\n",
      "Iteration 4051, loss = 0.00320472\n",
      "Iteration 4052, loss = 0.00320246\n",
      "Iteration 4053, loss = 0.00320020\n",
      "Iteration 4054, loss = 0.00319794\n",
      "Iteration 4055, loss = 0.00319568\n",
      "Iteration 4056, loss = 0.00319342\n",
      "Iteration 4057, loss = 0.00319117\n",
      "Iteration 4058, loss = 0.00318891\n",
      "Iteration 4059, loss = 0.00318666\n",
      "Iteration 4060, loss = 0.00318441\n",
      "Iteration 4061, loss = 0.00318216\n",
      "Iteration 4062, loss = 0.00317992\n",
      "Iteration 4063, loss = 0.00317767\n",
      "Iteration 4064, loss = 0.00317543\n",
      "Iteration 4065, loss = 0.00317319\n",
      "Iteration 4066, loss = 0.00317095\n",
      "Iteration 4067, loss = 0.00316871\n",
      "Iteration 4068, loss = 0.00316647\n",
      "Iteration 4069, loss = 0.00316424\n",
      "Iteration 4070, loss = 0.00316200\n",
      "Iteration 4071, loss = 0.00315977\n",
      "Iteration 4072, loss = 0.00315754\n",
      "Iteration 4073, loss = 0.00315531\n",
      "Iteration 4074, loss = 0.00315309\n",
      "Iteration 4075, loss = 0.00315086\n",
      "Iteration 4076, loss = 0.00314864\n",
      "Iteration 4077, loss = 0.00314641\n",
      "Iteration 4078, loss = 0.00314419\n",
      "Iteration 4079, loss = 0.00314197\n",
      "Iteration 4080, loss = 0.00313976\n",
      "Iteration 4081, loss = 0.00313754\n",
      "Iteration 4082, loss = 0.00313533\n",
      "Iteration 4083, loss = 0.00313311\n",
      "Iteration 4084, loss = 0.00313090\n",
      "Iteration 4085, loss = 0.00312869\n",
      "Iteration 4086, loss = 0.00312649\n",
      "Iteration 4087, loss = 0.00312428\n",
      "Iteration 4088, loss = 0.00312208\n",
      "Iteration 4089, loss = 0.00311987\n",
      "Iteration 4090, loss = 0.00311767\n",
      "Iteration 4091, loss = 0.00311547\n",
      "Iteration 4092, loss = 0.00311327\n",
      "Iteration 4093, loss = 0.00311108\n",
      "Iteration 4094, loss = 0.00310888\n",
      "Iteration 4095, loss = 0.00310669\n",
      "Iteration 4096, loss = 0.00310450\n",
      "Iteration 4097, loss = 0.00310231\n",
      "Iteration 4098, loss = 0.00310012\n",
      "Iteration 4099, loss = 0.00309793\n",
      "Iteration 4100, loss = 0.00309574\n",
      "Iteration 4101, loss = 0.00309356\n",
      "Iteration 4102, loss = 0.00309138\n",
      "Iteration 4103, loss = 0.00308920\n",
      "Iteration 4104, loss = 0.00308702\n",
      "Iteration 4105, loss = 0.00308484\n",
      "Iteration 4106, loss = 0.00308267\n",
      "Iteration 4107, loss = 0.00308049\n",
      "Iteration 4108, loss = 0.00307832\n",
      "Iteration 4109, loss = 0.00307615\n",
      "Iteration 4110, loss = 0.00307398\n",
      "Iteration 4111, loss = 0.00307181\n",
      "Iteration 4112, loss = 0.00306964\n",
      "Iteration 4113, loss = 0.00306748\n",
      "Iteration 4114, loss = 0.00306531\n",
      "Iteration 4115, loss = 0.00306315\n",
      "Iteration 4116, loss = 0.00306099\n",
      "Iteration 4117, loss = 0.00305883\n",
      "Iteration 4118, loss = 0.00305667\n",
      "Iteration 4119, loss = 0.00305452\n",
      "Iteration 4120, loss = 0.00305236\n",
      "Iteration 4121, loss = 0.00305021\n",
      "Iteration 4122, loss = 0.00304806\n",
      "Iteration 4123, loss = 0.00304591\n",
      "Iteration 4124, loss = 0.00304376\n",
      "Iteration 4125, loss = 0.00304162\n",
      "Iteration 4126, loss = 0.00303947\n",
      "Iteration 4127, loss = 0.00303733\n",
      "Iteration 4128, loss = 0.00303519\n",
      "Iteration 4129, loss = 0.00303305\n",
      "Iteration 4130, loss = 0.00303091\n",
      "Iteration 4131, loss = 0.00302877\n",
      "Iteration 4132, loss = 0.00302663\n",
      "Iteration 4133, loss = 0.00302450\n",
      "Iteration 4134, loss = 0.00302237\n",
      "Iteration 4135, loss = 0.00302023\n",
      "Iteration 4136, loss = 0.00301810\n",
      "Iteration 4137, loss = 0.00301598\n",
      "Iteration 4138, loss = 0.00301385\n",
      "Iteration 4139, loss = 0.00301172\n",
      "Iteration 4140, loss = 0.00300960\n",
      "Iteration 4141, loss = 0.00300748\n",
      "Iteration 4142, loss = 0.00300536\n",
      "Iteration 4143, loss = 0.00300324\n",
      "Iteration 4144, loss = 0.00300112\n",
      "Iteration 4145, loss = 0.00299901\n",
      "Iteration 4146, loss = 0.00299689\n",
      "Iteration 4147, loss = 0.00299478\n",
      "Iteration 4148, loss = 0.00299267\n",
      "Iteration 4149, loss = 0.00299056\n",
      "Iteration 4150, loss = 0.00298845\n",
      "Iteration 4151, loss = 0.00298634\n",
      "Iteration 4152, loss = 0.00298423\n",
      "Iteration 4153, loss = 0.00298213\n",
      "Iteration 4154, loss = 0.00298003\n",
      "Iteration 4155, loss = 0.00297793\n",
      "Iteration 4156, loss = 0.00297583\n",
      "Iteration 4157, loss = 0.00297373\n",
      "Iteration 4158, loss = 0.00297163\n",
      "Iteration 4159, loss = 0.00296954\n",
      "Iteration 4160, loss = 0.00296744\n",
      "Iteration 4161, loss = 0.00296535\n",
      "Iteration 4162, loss = 0.00296326\n",
      "Iteration 4163, loss = 0.00296117\n",
      "Iteration 4164, loss = 0.00295908\n",
      "Iteration 4165, loss = 0.00295700\n",
      "Iteration 4166, loss = 0.00295491\n",
      "Iteration 4167, loss = 0.00295283\n",
      "Iteration 4168, loss = 0.00295075\n",
      "Iteration 4169, loss = 0.00294867\n",
      "Iteration 4170, loss = 0.00294659\n",
      "Iteration 4171, loss = 0.00294451\n",
      "Iteration 4172, loss = 0.00294243\n",
      "Iteration 4173, loss = 0.00294036\n",
      "Iteration 4174, loss = 0.00293828\n",
      "Iteration 4175, loss = 0.00293621\n",
      "Iteration 4176, loss = 0.00293414\n",
      "Iteration 4177, loss = 0.00293207\n",
      "Iteration 4178, loss = 0.00293001\n",
      "Iteration 4179, loss = 0.00292794\n",
      "Iteration 4180, loss = 0.00292588\n",
      "Iteration 4181, loss = 0.00292381\n",
      "Iteration 4182, loss = 0.00292175\n",
      "Iteration 4183, loss = 0.00291969\n",
      "Iteration 4184, loss = 0.00291763\n",
      "Iteration 4185, loss = 0.00291558\n",
      "Iteration 4186, loss = 0.00291352\n",
      "Iteration 4187, loss = 0.00291147\n",
      "Iteration 4188, loss = 0.00290941\n",
      "Iteration 4189, loss = 0.00290736\n",
      "Iteration 4190, loss = 0.00290531\n",
      "Iteration 4191, loss = 0.00290326\n",
      "Iteration 4192, loss = 0.00290122\n",
      "Iteration 4193, loss = 0.00289917\n",
      "Iteration 4194, loss = 0.00289713\n",
      "Iteration 4195, loss = 0.00289508\n",
      "Iteration 4196, loss = 0.00289304\n",
      "Iteration 4197, loss = 0.00289100\n",
      "Iteration 4198, loss = 0.00288896\n",
      "Iteration 4199, loss = 0.00288693\n",
      "Iteration 4200, loss = 0.00288489\n",
      "Iteration 4201, loss = 0.00288286\n",
      "Iteration 4202, loss = 0.00288083\n",
      "Iteration 4203, loss = 0.00287879\n",
      "Iteration 4204, loss = 0.00287676\n",
      "Iteration 4205, loss = 0.00287474\n",
      "Iteration 4206, loss = 0.00287271\n",
      "Iteration 4207, loss = 0.00287068\n",
      "Iteration 4208, loss = 0.00286866\n",
      "Iteration 4209, loss = 0.00286664\n",
      "Iteration 4210, loss = 0.00286462\n",
      "Iteration 4211, loss = 0.00286260\n",
      "Iteration 4212, loss = 0.00286058\n",
      "Iteration 4213, loss = 0.00285856\n",
      "Iteration 4214, loss = 0.00285654\n",
      "Iteration 4215, loss = 0.00285453\n",
      "Iteration 4216, loss = 0.00285252\n",
      "Iteration 4217, loss = 0.00285051\n",
      "Iteration 4218, loss = 0.00284850\n",
      "Iteration 4219, loss = 0.00284649\n",
      "Iteration 4220, loss = 0.00284448\n",
      "Iteration 4221, loss = 0.00284247\n",
      "Iteration 4222, loss = 0.00284047\n",
      "Iteration 4223, loss = 0.00283847\n",
      "Iteration 4224, loss = 0.00283646\n",
      "Iteration 4225, loss = 0.00283446\n",
      "Iteration 4226, loss = 0.00283247\n",
      "Iteration 4227, loss = 0.00283047\n",
      "Iteration 4228, loss = 0.00282847\n",
      "Iteration 4229, loss = 0.00282648\n",
      "Iteration 4230, loss = 0.00282448\n",
      "Iteration 4231, loss = 0.00282249\n",
      "Iteration 4232, loss = 0.00282050\n",
      "Iteration 4233, loss = 0.00281851\n",
      "Iteration 4234, loss = 0.00281653\n",
      "Iteration 4235, loss = 0.00281454\n",
      "Iteration 4236, loss = 0.00281256\n",
      "Iteration 4237, loss = 0.00281057\n",
      "Iteration 4238, loss = 0.00280859\n",
      "Iteration 4239, loss = 0.00280661\n",
      "Iteration 4240, loss = 0.00280463\n",
      "Iteration 4241, loss = 0.00280265\n",
      "Iteration 4242, loss = 0.00280068\n",
      "Iteration 4243, loss = 0.00279870\n",
      "Iteration 4244, loss = 0.00279673\n",
      "Iteration 4245, loss = 0.00279475\n",
      "Iteration 4246, loss = 0.00279278\n",
      "Iteration 4247, loss = 0.00279081\n",
      "Iteration 4248, loss = 0.00278885\n",
      "Iteration 4249, loss = 0.00278688\n",
      "Iteration 4250, loss = 0.00278491\n",
      "Iteration 4251, loss = 0.00278295\n",
      "Iteration 4252, loss = 0.00278099\n",
      "Iteration 4253, loss = 0.00277903\n",
      "Iteration 4254, loss = 0.00277707\n",
      "Iteration 4255, loss = 0.00277511\n",
      "Iteration 4256, loss = 0.00277315\n",
      "Iteration 4257, loss = 0.00277119\n",
      "Iteration 4258, loss = 0.00276924\n",
      "Iteration 4259, loss = 0.00276729\n",
      "Iteration 4260, loss = 0.00276533\n",
      "Iteration 4261, loss = 0.00276338\n",
      "Iteration 4262, loss = 0.00276144\n",
      "Iteration 4263, loss = 0.00275949\n",
      "Iteration 4264, loss = 0.00275754\n",
      "Iteration 4265, loss = 0.00275560\n",
      "Iteration 4266, loss = 0.00275365\n",
      "Iteration 4267, loss = 0.00275171\n",
      "Iteration 4268, loss = 0.00274977\n",
      "Iteration 4269, loss = 0.00274783\n",
      "Iteration 4270, loss = 0.00274589\n",
      "Iteration 4271, loss = 0.00274396\n",
      "Iteration 4272, loss = 0.00274202\n",
      "Iteration 4273, loss = 0.00274009\n",
      "Iteration 4274, loss = 0.00273815\n",
      "Iteration 4275, loss = 0.00273622\n",
      "Iteration 4276, loss = 0.00273429\n",
      "Iteration 4277, loss = 0.00273236\n",
      "Iteration 4278, loss = 0.00273044\n",
      "Iteration 4279, loss = 0.00272851\n",
      "Iteration 4280, loss = 0.00272659\n",
      "Iteration 4281, loss = 0.00272466\n",
      "Iteration 4282, loss = 0.00272274\n",
      "Iteration 4283, loss = 0.00272082\n",
      "Iteration 4284, loss = 0.00271890\n",
      "Iteration 4285, loss = 0.00271699\n",
      "Iteration 4286, loss = 0.00271507\n",
      "Iteration 4287, loss = 0.00271315\n",
      "Iteration 4288, loss = 0.00271124\n",
      "Iteration 4289, loss = 0.00270933\n",
      "Iteration 4290, loss = 0.00270742\n",
      "Iteration 4291, loss = 0.00270551\n",
      "Iteration 4292, loss = 0.00270360\n",
      "Iteration 4293, loss = 0.00270169\n",
      "Iteration 4294, loss = 0.00269979\n",
      "Iteration 4295, loss = 0.00269788\n",
      "Iteration 4296, loss = 0.00269598\n",
      "Iteration 4297, loss = 0.00269408\n",
      "Iteration 4298, loss = 0.00269218\n",
      "Iteration 4299, loss = 0.00269028\n",
      "Iteration 4300, loss = 0.00268838\n",
      "Iteration 4301, loss = 0.00268649\n",
      "Iteration 4302, loss = 0.00268459\n",
      "Iteration 4303, loss = 0.00268270\n",
      "Iteration 4304, loss = 0.00268081\n",
      "Iteration 4305, loss = 0.00267892\n",
      "Iteration 4306, loss = 0.00267703\n",
      "Iteration 4307, loss = 0.00267514\n",
      "Iteration 4308, loss = 0.00267325\n",
      "Iteration 4309, loss = 0.00267137\n",
      "Iteration 4310, loss = 0.00266948\n",
      "Iteration 4311, loss = 0.00266760\n",
      "Iteration 4312, loss = 0.00266572\n",
      "Iteration 4313, loss = 0.00266384\n",
      "Iteration 4314, loss = 0.00266196\n",
      "Iteration 4315, loss = 0.00266008\n",
      "Iteration 4316, loss = 0.00265821\n",
      "Iteration 4317, loss = 0.00265633\n",
      "Iteration 4318, loss = 0.00265446\n",
      "Iteration 4319, loss = 0.00265259\n",
      "Iteration 4320, loss = 0.00265072\n",
      "Iteration 4321, loss = 0.00264885\n",
      "Iteration 4322, loss = 0.00264698\n",
      "Iteration 4323, loss = 0.00264511\n",
      "Iteration 4324, loss = 0.00264325\n",
      "Iteration 4325, loss = 0.00264139\n",
      "Iteration 4326, loss = 0.00263952\n",
      "Iteration 4327, loss = 0.00263766\n",
      "Iteration 4328, loss = 0.00263580\n",
      "Iteration 4329, loss = 0.00263394\n",
      "Iteration 4330, loss = 0.00263209\n",
      "Iteration 4331, loss = 0.00263023\n",
      "Iteration 4332, loss = 0.00262838\n",
      "Iteration 4333, loss = 0.00262652\n",
      "Iteration 4334, loss = 0.00262467\n",
      "Iteration 4335, loss = 0.00262282\n",
      "Iteration 4336, loss = 0.00262097\n",
      "Iteration 4337, loss = 0.00261912\n",
      "Iteration 4338, loss = 0.00261728\n",
      "Iteration 4339, loss = 0.00261543\n",
      "Iteration 4340, loss = 0.00261359\n",
      "Iteration 4341, loss = 0.00261175\n",
      "Iteration 4342, loss = 0.00260990\n",
      "Iteration 4343, loss = 0.00260806\n",
      "Iteration 4344, loss = 0.00260623\n",
      "Iteration 4345, loss = 0.00260439\n",
      "Iteration 4346, loss = 0.00260255\n",
      "Iteration 4347, loss = 0.00260072\n",
      "Iteration 4348, loss = 0.00259889\n",
      "Iteration 4349, loss = 0.00259705\n",
      "Iteration 4350, loss = 0.00259522\n",
      "Iteration 4351, loss = 0.00259339\n",
      "Iteration 4352, loss = 0.00259157\n",
      "Iteration 4353, loss = 0.00258974\n",
      "Iteration 4354, loss = 0.00258791\n",
      "Iteration 4355, loss = 0.00258609\n",
      "Iteration 4356, loss = 0.00258427\n",
      "Iteration 4357, loss = 0.00258245\n",
      "Iteration 4358, loss = 0.00258063\n",
      "Iteration 4359, loss = 0.00257881\n",
      "Iteration 4360, loss = 0.00257699\n",
      "Iteration 4361, loss = 0.00257517\n",
      "Iteration 4362, loss = 0.00257336\n",
      "Iteration 4363, loss = 0.00257155\n",
      "Iteration 4364, loss = 0.00256973\n",
      "Iteration 4365, loss = 0.00256792\n",
      "Iteration 4366, loss = 0.00256611\n",
      "Iteration 4367, loss = 0.00256431\n",
      "Iteration 4368, loss = 0.00256250\n",
      "Iteration 4369, loss = 0.00256069\n",
      "Iteration 4370, loss = 0.00255889\n",
      "Iteration 4371, loss = 0.00255709\n",
      "Iteration 4372, loss = 0.00255529\n",
      "Iteration 4373, loss = 0.00255349\n",
      "Iteration 4374, loss = 0.00255169\n",
      "Iteration 4375, loss = 0.00254989\n",
      "Iteration 4376, loss = 0.00254809\n",
      "Iteration 4377, loss = 0.00254630\n",
      "Iteration 4378, loss = 0.00254451\n",
      "Iteration 4379, loss = 0.00254271\n",
      "Iteration 4380, loss = 0.00254092\n",
      "Iteration 4381, loss = 0.00253913\n",
      "Iteration 4382, loss = 0.00253735\n",
      "Iteration 4383, loss = 0.00253556\n",
      "Iteration 4384, loss = 0.00253377\n",
      "Iteration 4385, loss = 0.00253199\n",
      "Iteration 4386, loss = 0.00253021\n",
      "Iteration 4387, loss = 0.00252843\n",
      "Iteration 4388, loss = 0.00252664\n",
      "Iteration 4389, loss = 0.00252487\n",
      "Iteration 4390, loss = 0.00252309\n",
      "Iteration 4391, loss = 0.00252131\n",
      "Iteration 4392, loss = 0.00251954\n",
      "Iteration 4393, loss = 0.00251776\n",
      "Iteration 4394, loss = 0.00251599\n",
      "Iteration 4395, loss = 0.00251422\n",
      "Iteration 4396, loss = 0.00251245\n",
      "Iteration 4397, loss = 0.00251068\n",
      "Iteration 4398, loss = 0.00250892\n",
      "Iteration 4399, loss = 0.00250715\n",
      "Iteration 4400, loss = 0.00250539\n",
      "Iteration 4401, loss = 0.00250362\n",
      "Iteration 4402, loss = 0.00250186\n",
      "Iteration 4403, loss = 0.00250010\n",
      "Iteration 4404, loss = 0.00249834\n",
      "Iteration 4405, loss = 0.00249658\n",
      "Iteration 4406, loss = 0.00249483\n",
      "Iteration 4407, loss = 0.00249307\n",
      "Iteration 4408, loss = 0.00249132\n",
      "Iteration 4409, loss = 0.00248957\n",
      "Iteration 4410, loss = 0.00248781\n",
      "Iteration 4411, loss = 0.00248606\n",
      "Iteration 4412, loss = 0.00248432\n",
      "Iteration 4413, loss = 0.00248257\n",
      "Iteration 4414, loss = 0.00248082\n",
      "Iteration 4415, loss = 0.00247908\n",
      "Iteration 4416, loss = 0.00247733\n",
      "Iteration 4417, loss = 0.00247559\n",
      "Iteration 4418, loss = 0.00247385\n",
      "Iteration 4419, loss = 0.00247211\n",
      "Iteration 4420, loss = 0.00247037\n",
      "Iteration 4421, loss = 0.00246864\n",
      "Iteration 4422, loss = 0.00246690\n",
      "Iteration 4423, loss = 0.00246517\n",
      "Iteration 4424, loss = 0.00246343\n",
      "Iteration 4425, loss = 0.00246170\n",
      "Iteration 4426, loss = 0.00245997\n",
      "Iteration 4427, loss = 0.00245824\n",
      "Iteration 4428, loss = 0.00245652\n",
      "Iteration 4429, loss = 0.00245479\n",
      "Iteration 4430, loss = 0.00245307\n",
      "Iteration 4431, loss = 0.00245134\n",
      "Iteration 4432, loss = 0.00244962\n",
      "Iteration 4433, loss = 0.00244790\n",
      "Iteration 4434, loss = 0.00244618\n",
      "Iteration 4435, loss = 0.00244446\n",
      "Iteration 4436, loss = 0.00244274\n",
      "Iteration 4437, loss = 0.00244103\n",
      "Iteration 4438, loss = 0.00243931\n",
      "Iteration 4439, loss = 0.00243760\n",
      "Iteration 4440, loss = 0.00243589\n",
      "Iteration 4441, loss = 0.00243418\n",
      "Iteration 4442, loss = 0.00243247\n",
      "Iteration 4443, loss = 0.00243076\n",
      "Iteration 4444, loss = 0.00242905\n",
      "Iteration 4445, loss = 0.00242735\n",
      "Iteration 4446, loss = 0.00242564\n",
      "Iteration 4447, loss = 0.00242394\n",
      "Iteration 4448, loss = 0.00242224\n",
      "Iteration 4449, loss = 0.00242054\n",
      "Iteration 4450, loss = 0.00241884\n",
      "Iteration 4451, loss = 0.00241714\n",
      "Iteration 4452, loss = 0.00241545\n",
      "Iteration 4453, loss = 0.00241375\n",
      "Iteration 4454, loss = 0.00241206\n",
      "Iteration 4455, loss = 0.00241037\n",
      "Iteration 4456, loss = 0.00240867\n",
      "Iteration 4457, loss = 0.00240698\n",
      "Iteration 4458, loss = 0.00240530\n",
      "Iteration 4459, loss = 0.00240361\n",
      "Iteration 4460, loss = 0.00240192\n",
      "Iteration 4461, loss = 0.00240024\n",
      "Iteration 4462, loss = 0.00239856\n",
      "Iteration 4463, loss = 0.00239687\n",
      "Iteration 4464, loss = 0.00239519\n",
      "Iteration 4465, loss = 0.00239351\n",
      "Iteration 4466, loss = 0.00239184\n",
      "Iteration 4467, loss = 0.00239016\n",
      "Iteration 4468, loss = 0.00238848\n",
      "Iteration 4469, loss = 0.00238681\n",
      "Iteration 4470, loss = 0.00238514\n",
      "Iteration 4471, loss = 0.00238346\n",
      "Iteration 4472, loss = 0.00238179\n",
      "Iteration 4473, loss = 0.00238012\n",
      "Iteration 4474, loss = 0.00237846\n",
      "Iteration 4475, loss = 0.00237679\n",
      "Iteration 4476, loss = 0.00237513\n",
      "Iteration 4477, loss = 0.00237346\n",
      "Iteration 4478, loss = 0.00237180\n",
      "Iteration 4479, loss = 0.00237014\n",
      "Iteration 4480, loss = 0.00236848\n",
      "Iteration 4481, loss = 0.00236682\n",
      "Iteration 4482, loss = 0.00236516\n",
      "Iteration 4483, loss = 0.00236351\n",
      "Iteration 4484, loss = 0.00236185\n",
      "Iteration 4485, loss = 0.00236020\n",
      "Iteration 4486, loss = 0.00235854\n",
      "Iteration 4487, loss = 0.00235689\n",
      "Iteration 4488, loss = 0.00235524\n",
      "Iteration 4489, loss = 0.00235360\n",
      "Iteration 4490, loss = 0.00235195\n",
      "Iteration 4491, loss = 0.00235030\n",
      "Iteration 4492, loss = 0.00234866\n",
      "Iteration 4493, loss = 0.00234701\n",
      "Iteration 4494, loss = 0.00234537\n",
      "Iteration 4495, loss = 0.00234373\n",
      "Iteration 4496, loss = 0.00234209\n",
      "Iteration 4497, loss = 0.00234045\n",
      "Iteration 4498, loss = 0.00233882\n",
      "Iteration 4499, loss = 0.00233718\n",
      "Iteration 4500, loss = 0.00233555\n",
      "Iteration 4501, loss = 0.00233391\n",
      "Iteration 4502, loss = 0.00233228\n",
      "Iteration 4503, loss = 0.00233065\n",
      "Iteration 4504, loss = 0.00232902\n",
      "Iteration 4505, loss = 0.00232739\n",
      "Iteration 4506, loss = 0.00232577\n",
      "Iteration 4507, loss = 0.00232414\n",
      "Iteration 4508, loss = 0.00232252\n",
      "Iteration 4509, loss = 0.00232089\n",
      "Iteration 4510, loss = 0.00231927\n",
      "Iteration 4511, loss = 0.00231765\n",
      "Iteration 4512, loss = 0.00231603\n",
      "Iteration 4513, loss = 0.00231442\n",
      "Iteration 4514, loss = 0.00231280\n",
      "Iteration 4515, loss = 0.00231118\n",
      "Iteration 4516, loss = 0.00230957\n",
      "Iteration 4517, loss = 0.00230796\n",
      "Iteration 4518, loss = 0.00230634\n",
      "Iteration 4519, loss = 0.00230473\n",
      "Iteration 4520, loss = 0.00230313\n",
      "Iteration 4521, loss = 0.00230152\n",
      "Iteration 4522, loss = 0.00229991\n",
      "Iteration 4523, loss = 0.00229831\n",
      "Iteration 4524, loss = 0.00229670\n",
      "Iteration 4525, loss = 0.00229510\n",
      "Iteration 4526, loss = 0.00229350\n",
      "Iteration 4527, loss = 0.00229190\n",
      "Iteration 4528, loss = 0.00229030\n",
      "Iteration 4529, loss = 0.00228870\n",
      "Iteration 4530, loss = 0.00228710\n",
      "Iteration 4531, loss = 0.00228551\n",
      "Iteration 4532, loss = 0.00228391\n",
      "Iteration 4533, loss = 0.00228232\n",
      "Iteration 4534, loss = 0.00228073\n",
      "Iteration 4535, loss = 0.00227914\n",
      "Iteration 4536, loss = 0.00227755\n",
      "Iteration 4537, loss = 0.00227596\n",
      "Iteration 4538, loss = 0.00227438\n",
      "Iteration 4539, loss = 0.00227279\n",
      "Iteration 4540, loss = 0.00227121\n",
      "Iteration 4541, loss = 0.00226962\n",
      "Iteration 4542, loss = 0.00226804\n",
      "Iteration 4543, loss = 0.00226646\n",
      "Iteration 4544, loss = 0.00226488\n",
      "Iteration 4545, loss = 0.00226331\n",
      "Iteration 4546, loss = 0.00226173\n",
      "Iteration 4547, loss = 0.00226015\n",
      "Iteration 4548, loss = 0.00225858\n",
      "Iteration 4549, loss = 0.00225701\n",
      "Iteration 4550, loss = 0.00225544\n",
      "Iteration 4551, loss = 0.00225387\n",
      "Iteration 4552, loss = 0.00225230\n",
      "Iteration 4553, loss = 0.00225073\n",
      "Iteration 4554, loss = 0.00224916\n",
      "Iteration 4555, loss = 0.00224760\n",
      "Iteration 4556, loss = 0.00224603\n",
      "Iteration 4557, loss = 0.00224447\n",
      "Iteration 4558, loss = 0.00224291\n",
      "Iteration 4559, loss = 0.00224135\n",
      "Iteration 4560, loss = 0.00223979\n",
      "Iteration 4561, loss = 0.00223823\n",
      "Iteration 4562, loss = 0.00223667\n",
      "Iteration 4563, loss = 0.00223512\n",
      "Iteration 4564, loss = 0.00223356\n",
      "Iteration 4565, loss = 0.00223201\n",
      "Iteration 4566, loss = 0.00223046\n",
      "Iteration 4567, loss = 0.00222891\n",
      "Iteration 4568, loss = 0.00222736\n",
      "Iteration 4569, loss = 0.00222581\n",
      "Iteration 4570, loss = 0.00222426\n",
      "Iteration 4571, loss = 0.00222271\n",
      "Iteration 4572, loss = 0.00222117\n",
      "Iteration 4573, loss = 0.00221963\n",
      "Iteration 4574, loss = 0.00221808\n",
      "Iteration 4575, loss = 0.00221654\n",
      "Iteration 4576, loss = 0.00221500\n",
      "Iteration 4577, loss = 0.00221346\n",
      "Iteration 4578, loss = 0.00221193\n",
      "Iteration 4579, loss = 0.00221039\n",
      "Iteration 4580, loss = 0.00220885\n",
      "Iteration 4581, loss = 0.00220732\n",
      "Iteration 4582, loss = 0.00220579\n",
      "Iteration 4583, loss = 0.00220426\n",
      "Iteration 4584, loss = 0.00220273\n",
      "Iteration 4585, loss = 0.00220120\n",
      "Iteration 4586, loss = 0.00219967\n",
      "Iteration 4587, loss = 0.00219814\n",
      "Iteration 4588, loss = 0.00219662\n",
      "Iteration 4589, loss = 0.00219509\n",
      "Iteration 4590, loss = 0.00219357\n",
      "Iteration 4591, loss = 0.00219205\n",
      "Iteration 4592, loss = 0.00219053\n",
      "Iteration 4593, loss = 0.00218901\n",
      "Iteration 4594, loss = 0.00218749\n",
      "Iteration 4595, loss = 0.00218597\n",
      "Iteration 4596, loss = 0.00218446\n",
      "Iteration 4597, loss = 0.00218294\n",
      "Iteration 4598, loss = 0.00218143\n",
      "Iteration 4599, loss = 0.00217991\n",
      "Iteration 4600, loss = 0.00217840\n",
      "Iteration 4601, loss = 0.00217689\n",
      "Iteration 4602, loss = 0.00217538\n",
      "Iteration 4603, loss = 0.00217388\n",
      "Iteration 4604, loss = 0.00217237\n",
      "Iteration 4605, loss = 0.00217086\n",
      "Iteration 4606, loss = 0.00216936\n",
      "Iteration 4607, loss = 0.00216786\n",
      "Iteration 4608, loss = 0.00216636\n",
      "Iteration 4609, loss = 0.00216485\n",
      "Iteration 4610, loss = 0.00216335\n",
      "Iteration 4611, loss = 0.00216186\n",
      "Iteration 4612, loss = 0.00216036\n",
      "Iteration 4613, loss = 0.00215886\n",
      "Iteration 4614, loss = 0.00215737\n",
      "Iteration 4615, loss = 0.00215587\n",
      "Iteration 4616, loss = 0.00215438\n",
      "Iteration 4617, loss = 0.00215289\n",
      "Iteration 4618, loss = 0.00215140\n",
      "Iteration 4619, loss = 0.00214991\n",
      "Iteration 4620, loss = 0.00214842\n",
      "Iteration 4621, loss = 0.00214694\n",
      "Iteration 4622, loss = 0.00214545\n",
      "Iteration 4623, loss = 0.00214397\n",
      "Iteration 4624, loss = 0.00214248\n",
      "Iteration 4625, loss = 0.00214100\n",
      "Iteration 4626, loss = 0.00213952\n",
      "Iteration 4627, loss = 0.00213804\n",
      "Iteration 4628, loss = 0.00213656\n",
      "Iteration 4629, loss = 0.00213509\n",
      "Iteration 4630, loss = 0.00213361\n",
      "Iteration 4631, loss = 0.00213213\n",
      "Iteration 4632, loss = 0.00213066\n",
      "Iteration 4633, loss = 0.00212919\n",
      "Iteration 4634, loss = 0.00212772\n",
      "Iteration 4635, loss = 0.00212624\n",
      "Iteration 4636, loss = 0.00212478\n",
      "Iteration 4637, loss = 0.00212331\n",
      "Iteration 4638, loss = 0.00212184\n",
      "Iteration 4639, loss = 0.00212037\n",
      "Iteration 4640, loss = 0.00211891\n",
      "Iteration 4641, loss = 0.00211745\n",
      "Iteration 4642, loss = 0.00211598\n",
      "Iteration 4643, loss = 0.00211452\n",
      "Iteration 4644, loss = 0.00211306\n",
      "Iteration 4645, loss = 0.00211160\n",
      "Iteration 4646, loss = 0.00211014\n",
      "Iteration 4647, loss = 0.00210869\n",
      "Iteration 4648, loss = 0.00210723\n",
      "Iteration 4649, loss = 0.00210578\n",
      "Iteration 4650, loss = 0.00210432\n",
      "Iteration 4651, loss = 0.00210287\n",
      "Iteration 4652, loss = 0.00210142\n",
      "Iteration 4653, loss = 0.00209997\n",
      "Iteration 4654, loss = 0.00209852\n",
      "Iteration 4655, loss = 0.00209707\n",
      "Iteration 4656, loss = 0.00209563\n",
      "Iteration 4657, loss = 0.00209418\n",
      "Iteration 4658, loss = 0.00209274\n",
      "Iteration 4659, loss = 0.00209129\n",
      "Iteration 4660, loss = 0.00208985\n",
      "Iteration 4661, loss = 0.00208841\n",
      "Iteration 4662, loss = 0.00208697\n",
      "Iteration 4663, loss = 0.00208553\n",
      "Iteration 4664, loss = 0.00208409\n",
      "Iteration 4665, loss = 0.00208266\n",
      "Iteration 4666, loss = 0.00208122\n",
      "Iteration 4667, loss = 0.00207979\n",
      "Iteration 4668, loss = 0.00207836\n",
      "Iteration 4669, loss = 0.00207692\n",
      "Iteration 4670, loss = 0.00207549\n",
      "Iteration 4671, loss = 0.00207406\n",
      "Iteration 4672, loss = 0.00207263\n",
      "Iteration 4673, loss = 0.00207121\n",
      "Iteration 4674, loss = 0.00206978\n",
      "Iteration 4675, loss = 0.00206836\n",
      "Iteration 4676, loss = 0.00206693\n",
      "Iteration 4677, loss = 0.00206551\n",
      "Iteration 4678, loss = 0.00206409\n",
      "Iteration 4679, loss = 0.00206267\n",
      "Iteration 4680, loss = 0.00206125\n",
      "Iteration 4681, loss = 0.00205983\n",
      "Iteration 4682, loss = 0.00205841\n",
      "Iteration 4683, loss = 0.00205699\n",
      "Iteration 4684, loss = 0.00205558\n",
      "Iteration 4685, loss = 0.00205416\n",
      "Iteration 4686, loss = 0.00205275\n",
      "Iteration 4687, loss = 0.00205134\n",
      "Iteration 4688, loss = 0.00204993\n",
      "Iteration 4689, loss = 0.00204852\n",
      "Iteration 4690, loss = 0.00204711\n",
      "Iteration 4691, loss = 0.00204570\n",
      "Iteration 4692, loss = 0.00204430\n",
      "Iteration 4693, loss = 0.00204289\n",
      "Iteration 4694, loss = 0.00204149\n",
      "Iteration 4695, loss = 0.00204008\n",
      "Iteration 4696, loss = 0.00203868\n",
      "Iteration 4697, loss = 0.00203728\n",
      "Iteration 4698, loss = 0.00203588\n",
      "Iteration 4699, loss = 0.00203448\n",
      "Iteration 4700, loss = 0.00203309\n",
      "Iteration 4701, loss = 0.00203169\n",
      "Iteration 4702, loss = 0.00203029\n",
      "Iteration 4703, loss = 0.00202890\n",
      "Iteration 4704, loss = 0.00202751\n",
      "Iteration 4705, loss = 0.00202612\n",
      "Iteration 4706, loss = 0.00202472\n",
      "Iteration 4707, loss = 0.00202333\n",
      "Iteration 4708, loss = 0.00202195\n",
      "Iteration 4709, loss = 0.00202056\n",
      "Iteration 4710, loss = 0.00201917\n",
      "Iteration 4711, loss = 0.00201779\n",
      "Iteration 4712, loss = 0.00201640\n",
      "Iteration 4713, loss = 0.00201502\n",
      "Iteration 4714, loss = 0.00201364\n",
      "Iteration 4715, loss = 0.00201226\n",
      "Iteration 4716, loss = 0.00201088\n",
      "Iteration 4717, loss = 0.00200950\n",
      "Iteration 4718, loss = 0.00200812\n",
      "Iteration 4719, loss = 0.00200674\n",
      "Iteration 4720, loss = 0.00200537\n",
      "Iteration 4721, loss = 0.00200399\n",
      "Iteration 4722, loss = 0.00200262\n",
      "Iteration 4723, loss = 0.00200125\n",
      "Iteration 4724, loss = 0.00199988\n",
      "Iteration 4725, loss = 0.00199851\n",
      "Iteration 4726, loss = 0.00199714\n",
      "Iteration 4727, loss = 0.00199577\n",
      "Iteration 4728, loss = 0.00199440\n",
      "Iteration 4729, loss = 0.00199304\n",
      "Iteration 4730, loss = 0.00199167\n",
      "Iteration 4731, loss = 0.00199031\n",
      "Iteration 4732, loss = 0.00198895\n",
      "Iteration 4733, loss = 0.00198759\n",
      "Iteration 4734, loss = 0.00198623\n",
      "Iteration 4735, loss = 0.00198487\n",
      "Iteration 4736, loss = 0.00198351\n",
      "Iteration 4737, loss = 0.00198215\n",
      "Iteration 4738, loss = 0.00198080\n",
      "Iteration 4739, loss = 0.00197944\n",
      "Iteration 4740, loss = 0.00197809\n",
      "Iteration 4741, loss = 0.00197674\n",
      "Iteration 4742, loss = 0.00197539\n",
      "Iteration 4743, loss = 0.00197404\n",
      "Iteration 4744, loss = 0.00197269\n",
      "Iteration 4745, loss = 0.00197134\n",
      "Iteration 4746, loss = 0.00196999\n",
      "Iteration 4747, loss = 0.00196865\n",
      "Iteration 4748, loss = 0.00196730\n",
      "Iteration 4749, loss = 0.00196596\n",
      "Iteration 4750, loss = 0.00196462\n",
      "Iteration 4751, loss = 0.00196327\n",
      "Iteration 4752, loss = 0.00196193\n",
      "Iteration 4753, loss = 0.00196059\n",
      "Iteration 4754, loss = 0.00195926\n",
      "Iteration 4755, loss = 0.00195792\n",
      "Iteration 4756, loss = 0.00195658\n",
      "Iteration 4757, loss = 0.00195525\n",
      "Iteration 4758, loss = 0.00195391\n",
      "Iteration 4759, loss = 0.00195258\n",
      "Iteration 4760, loss = 0.00195125\n",
      "Iteration 4761, loss = 0.00194992\n",
      "Iteration 4762, loss = 0.00194859\n",
      "Iteration 4763, loss = 0.00194726\n",
      "Iteration 4764, loss = 0.00194593\n",
      "Iteration 4765, loss = 0.00194461\n",
      "Iteration 4766, loss = 0.00194328\n",
      "Iteration 4767, loss = 0.00194196\n",
      "Iteration 4768, loss = 0.00194064\n",
      "Iteration 4769, loss = 0.00193931\n",
      "Iteration 4770, loss = 0.00193799\n",
      "Iteration 4771, loss = 0.00193667\n",
      "Iteration 4772, loss = 0.00193535\n",
      "Iteration 4773, loss = 0.00193404\n",
      "Iteration 4774, loss = 0.00193272\n",
      "Iteration 4775, loss = 0.00193140\n",
      "Iteration 4776, loss = 0.00193009\n",
      "Iteration 4777, loss = 0.00192878\n",
      "Iteration 4778, loss = 0.00192746\n",
      "Iteration 4779, loss = 0.00192615\n",
      "Iteration 4780, loss = 0.00192484\n",
      "Iteration 4781, loss = 0.00192353\n",
      "Iteration 4782, loss = 0.00192223\n",
      "Iteration 4783, loss = 0.00192092\n",
      "Iteration 4784, loss = 0.00191961\n",
      "Iteration 4785, loss = 0.00191831\n",
      "Iteration 4786, loss = 0.00191701\n",
      "Iteration 4787, loss = 0.00191570\n",
      "Iteration 4788, loss = 0.00191440\n",
      "Iteration 4789, loss = 0.00191310\n",
      "Iteration 4790, loss = 0.00191180\n",
      "Iteration 4791, loss = 0.00191050\n",
      "Iteration 4792, loss = 0.00190921\n",
      "Iteration 4793, loss = 0.00190791\n",
      "Iteration 4794, loss = 0.00190662\n",
      "Iteration 4795, loss = 0.00190532\n",
      "Iteration 4796, loss = 0.00190403\n",
      "Iteration 4797, loss = 0.00190274\n",
      "Iteration 4798, loss = 0.00190145\n",
      "Iteration 4799, loss = 0.00190016\n",
      "Iteration 4800, loss = 0.00189887\n",
      "Iteration 4801, loss = 0.00189758\n",
      "Iteration 4802, loss = 0.00189629\n",
      "Iteration 4803, loss = 0.00189501\n",
      "Iteration 4804, loss = 0.00189372\n",
      "Iteration 4805, loss = 0.00189244\n",
      "Iteration 4806, loss = 0.00189116\n",
      "Iteration 4807, loss = 0.00188988\n",
      "Iteration 4808, loss = 0.00188860\n",
      "Iteration 4809, loss = 0.00188732\n",
      "Iteration 4810, loss = 0.00188604\n",
      "Iteration 4811, loss = 0.00188476\n",
      "Iteration 4812, loss = 0.00188349\n",
      "Iteration 4813, loss = 0.00188221\n",
      "Iteration 4814, loss = 0.00188094\n",
      "Iteration 4815, loss = 0.00187966\n",
      "Iteration 4816, loss = 0.00187839\n",
      "Iteration 4817, loss = 0.00187712\n",
      "Iteration 4818, loss = 0.00187585\n",
      "Iteration 4819, loss = 0.00187458\n",
      "Iteration 4820, loss = 0.00187332\n",
      "Iteration 4821, loss = 0.00187205\n",
      "Iteration 4822, loss = 0.00187078\n",
      "Iteration 4823, loss = 0.00186952\n",
      "Iteration 4824, loss = 0.00186826\n",
      "Iteration 4825, loss = 0.00186700\n",
      "Iteration 4826, loss = 0.00186573\n",
      "Iteration 4827, loss = 0.00186447\n",
      "Iteration 4828, loss = 0.00186321\n",
      "Iteration 4829, loss = 0.00186196\n",
      "Iteration 4830, loss = 0.00186070\n",
      "Iteration 4831, loss = 0.00185944\n",
      "Iteration 4832, loss = 0.00185819\n",
      "Iteration 4833, loss = 0.00185693\n",
      "Iteration 4834, loss = 0.00185568\n",
      "Iteration 4835, loss = 0.00185443\n",
      "Iteration 4836, loss = 0.00185318\n",
      "Iteration 4837, loss = 0.00185193\n",
      "Iteration 4838, loss = 0.00185068\n",
      "Iteration 4839, loss = 0.00184943\n",
      "Iteration 4840, loss = 0.00184819\n",
      "Iteration 4841, loss = 0.00184694\n",
      "Iteration 4842, loss = 0.00184570\n",
      "Iteration 4843, loss = 0.00184445\n",
      "Iteration 4844, loss = 0.00184321\n",
      "Iteration 4845, loss = 0.00184197\n",
      "Iteration 4846, loss = 0.00184073\n",
      "Iteration 4847, loss = 0.00183949\n",
      "Iteration 4848, loss = 0.00183825\n",
      "Iteration 4849, loss = 0.00183701\n",
      "Iteration 4850, loss = 0.00183578\n",
      "Iteration 4851, loss = 0.00183454\n",
      "Iteration 4852, loss = 0.00183331\n",
      "Iteration 4853, loss = 0.00183207\n",
      "Iteration 4854, loss = 0.00183084\n",
      "Iteration 4855, loss = 0.00182961\n",
      "Iteration 4856, loss = 0.00182838\n",
      "Iteration 4857, loss = 0.00182715\n",
      "Iteration 4858, loss = 0.00182592\n",
      "Iteration 4859, loss = 0.00182470\n",
      "Iteration 4860, loss = 0.00182347\n",
      "Iteration 4861, loss = 0.00182224\n",
      "Iteration 4862, loss = 0.00182102\n",
      "Iteration 4863, loss = 0.00181980\n",
      "Iteration 4864, loss = 0.00181858\n",
      "Iteration 4865, loss = 0.00181735\n",
      "Iteration 4866, loss = 0.00181613\n",
      "Iteration 4867, loss = 0.00181492\n",
      "Iteration 4868, loss = 0.00181370\n",
      "Iteration 4869, loss = 0.00181248\n",
      "Iteration 4870, loss = 0.00181126\n",
      "Iteration 4871, loss = 0.00181005\n",
      "Iteration 4872, loss = 0.00180884\n",
      "Iteration 4873, loss = 0.00180762\n",
      "Iteration 4874, loss = 0.00180641\n",
      "Iteration 4875, loss = 0.00180520\n",
      "Iteration 4876, loss = 0.00180399\n",
      "Iteration 4877, loss = 0.00180278\n",
      "Iteration 4878, loss = 0.00180157\n",
      "Iteration 4879, loss = 0.00180037\n",
      "Iteration 4880, loss = 0.00179916\n",
      "Iteration 4881, loss = 0.00179795\n",
      "Iteration 4882, loss = 0.00179675\n",
      "Iteration 4883, loss = 0.00179555\n",
      "Iteration 4884, loss = 0.00179435\n",
      "Iteration 4885, loss = 0.00179314\n",
      "Iteration 4886, loss = 0.00179194\n",
      "Iteration 4887, loss = 0.00179075\n",
      "Iteration 4888, loss = 0.00178955\n",
      "Iteration 4889, loss = 0.00178835\n",
      "Iteration 4890, loss = 0.00178715\n",
      "Iteration 4891, loss = 0.00178596\n",
      "Iteration 4892, loss = 0.00178477\n",
      "Iteration 4893, loss = 0.00178357\n",
      "Iteration 4894, loss = 0.00178238\n",
      "Iteration 4895, loss = 0.00178119\n",
      "Iteration 4896, loss = 0.00178000\n",
      "Iteration 4897, loss = 0.00177881\n",
      "Iteration 4898, loss = 0.00177762\n",
      "Iteration 4899, loss = 0.00177643\n",
      "Iteration 4900, loss = 0.00177525\n",
      "Iteration 4901, loss = 0.00177406\n",
      "Iteration 4902, loss = 0.00177288\n",
      "Iteration 4903, loss = 0.00177170\n",
      "Iteration 4904, loss = 0.00177051\n",
      "Iteration 4905, loss = 0.00176933\n",
      "Iteration 4906, loss = 0.00176815\n",
      "Iteration 4907, loss = 0.00176697\n",
      "Iteration 4908, loss = 0.00176579\n",
      "Iteration 4909, loss = 0.00176462\n",
      "Iteration 4910, loss = 0.00176344\n",
      "Iteration 4911, loss = 0.00176226\n",
      "Iteration 4912, loss = 0.00176109\n",
      "Iteration 4913, loss = 0.00175992\n",
      "Iteration 4914, loss = 0.00175874\n",
      "Iteration 4915, loss = 0.00175757\n",
      "Iteration 4916, loss = 0.00175640\n",
      "Iteration 4917, loss = 0.00175523\n",
      "Iteration 4918, loss = 0.00175406\n",
      "Iteration 4919, loss = 0.00175289\n",
      "Iteration 4920, loss = 0.00175173\n",
      "Iteration 4921, loss = 0.00175056\n",
      "Iteration 4922, loss = 0.00174940\n",
      "Iteration 4923, loss = 0.00174823\n",
      "Iteration 4924, loss = 0.00174707\n",
      "Iteration 4925, loss = 0.00174591\n",
      "Iteration 4926, loss = 0.00174475\n",
      "Iteration 4927, loss = 0.00174359\n",
      "Iteration 4928, loss = 0.00174243\n",
      "Iteration 4929, loss = 0.00174127\n",
      "Iteration 4930, loss = 0.00174011\n",
      "Iteration 4931, loss = 0.00173896\n",
      "Iteration 4932, loss = 0.00173780\n",
      "Iteration 4933, loss = 0.00173665\n",
      "Iteration 4934, loss = 0.00173549\n",
      "Iteration 4935, loss = 0.00173434\n",
      "Iteration 4936, loss = 0.00173319\n",
      "Iteration 4937, loss = 0.00173204\n",
      "Iteration 4938, loss = 0.00173089\n",
      "Iteration 4939, loss = 0.00172974\n",
      "Iteration 4940, loss = 0.00172859\n",
      "Iteration 4941, loss = 0.00172744\n",
      "Iteration 4942, loss = 0.00172630\n",
      "Iteration 4943, loss = 0.00172515\n",
      "Iteration 4944, loss = 0.00172401\n",
      "Iteration 4945, loss = 0.00172286\n",
      "Iteration 4946, loss = 0.00172172\n",
      "Iteration 4947, loss = 0.00172058\n",
      "Iteration 4948, loss = 0.00171944\n",
      "Iteration 4949, loss = 0.00171830\n",
      "Iteration 4950, loss = 0.00171716\n",
      "Iteration 4951, loss = 0.00171602\n",
      "Iteration 4952, loss = 0.00171489\n",
      "Iteration 4953, loss = 0.00171375\n",
      "Iteration 4954, loss = 0.00171262\n",
      "Iteration 4955, loss = 0.00171148\n",
      "Iteration 4956, loss = 0.00171035\n",
      "Iteration 4957, loss = 0.00170922\n",
      "Iteration 4958, loss = 0.00170809\n",
      "Iteration 4959, loss = 0.00170696\n",
      "Iteration 4960, loss = 0.00170583\n",
      "Iteration 4961, loss = 0.00170470\n",
      "Iteration 4962, loss = 0.00170357\n",
      "Iteration 4963, loss = 0.00170244\n",
      "Iteration 4964, loss = 0.00170132\n",
      "Iteration 4965, loss = 0.00170019\n",
      "Iteration 4966, loss = 0.00169907\n",
      "Iteration 4967, loss = 0.00169795\n",
      "Iteration 4968, loss = 0.00169683\n",
      "Iteration 4969, loss = 0.00169570\n",
      "Iteration 4970, loss = 0.00169458\n",
      "Iteration 4971, loss = 0.00169346\n",
      "Iteration 4972, loss = 0.00169235\n",
      "Iteration 4973, loss = 0.00169123\n",
      "Iteration 4974, loss = 0.00169011\n",
      "Iteration 4975, loss = 0.00168900\n",
      "Iteration 4976, loss = 0.00168788\n",
      "Iteration 4977, loss = 0.00168677\n",
      "Iteration 4978, loss = 0.00168565\n",
      "Iteration 4979, loss = 0.00168454\n",
      "Iteration 4980, loss = 0.00168343\n",
      "Iteration 4981, loss = 0.00168232\n",
      "Iteration 4982, loss = 0.00168121\n",
      "Iteration 4983, loss = 0.00168010\n",
      "Iteration 4984, loss = 0.00167900\n",
      "Iteration 4985, loss = 0.00167789\n",
      "Iteration 4986, loss = 0.00167678\n",
      "Iteration 4987, loss = 0.00167568\n",
      "Iteration 4988, loss = 0.00167457\n",
      "Iteration 4989, loss = 0.00167347\n",
      "Iteration 4990, loss = 0.00167237\n",
      "Iteration 4991, loss = 0.00167127\n",
      "Iteration 4992, loss = 0.00167017\n",
      "Iteration 4993, loss = 0.00166907\n",
      "Iteration 4994, loss = 0.00166797\n",
      "Iteration 4995, loss = 0.00166687\n",
      "Iteration 4996, loss = 0.00166577\n",
      "Iteration 4997, loss = 0.00166468\n",
      "Iteration 4998, loss = 0.00166358\n",
      "Iteration 4999, loss = 0.00166249\n",
      "Iteration 5000, loss = 0.00166139\n",
      "Iteration 5001, loss = 0.00166030\n",
      "Iteration 5002, loss = 0.00165921\n",
      "Iteration 5003, loss = 0.00165812\n",
      "Iteration 5004, loss = 0.00165703\n",
      "Iteration 5005, loss = 0.00165594\n",
      "Iteration 5006, loss = 0.00165485\n",
      "Iteration 5007, loss = 0.00165376\n",
      "Iteration 5008, loss = 0.00165268\n",
      "Iteration 5009, loss = 0.00165159\n",
      "Iteration 5010, loss = 0.00165051\n",
      "Iteration 5011, loss = 0.00164942\n",
      "Iteration 5012, loss = 0.00164834\n",
      "Iteration 5013, loss = 0.00164726\n",
      "Iteration 5014, loss = 0.00164618\n",
      "Iteration 5015, loss = 0.00164510\n",
      "Iteration 5016, loss = 0.00164402\n",
      "Iteration 5017, loss = 0.00164294\n",
      "Iteration 5018, loss = 0.00164186\n",
      "Iteration 5019, loss = 0.00164078\n",
      "Iteration 5020, loss = 0.00163971\n",
      "Iteration 5021, loss = 0.00163863\n",
      "Iteration 5022, loss = 0.00163756\n",
      "Iteration 5023, loss = 0.00163649\n",
      "Iteration 5024, loss = 0.00163541\n",
      "Iteration 5025, loss = 0.00163434\n",
      "Iteration 5026, loss = 0.00163327\n",
      "Iteration 5027, loss = 0.00163220\n",
      "Iteration 5028, loss = 0.00163113\n",
      "Iteration 5029, loss = 0.00163006\n",
      "Iteration 5030, loss = 0.00162900\n",
      "Iteration 5031, loss = 0.00162793\n",
      "Iteration 5032, loss = 0.00162686\n",
      "Iteration 5033, loss = 0.00162580\n",
      "Iteration 5034, loss = 0.00162473\n",
      "Iteration 5035, loss = 0.00162367\n",
      "Iteration 5036, loss = 0.00162261\n",
      "Iteration 5037, loss = 0.00162155\n",
      "Iteration 5038, loss = 0.00162049\n",
      "Iteration 5039, loss = 0.00161943\n",
      "Iteration 5040, loss = 0.00161837\n",
      "Iteration 5041, loss = 0.00161731\n",
      "Iteration 5042, loss = 0.00161625\n",
      "Iteration 5043, loss = 0.00161520\n",
      "Iteration 5044, loss = 0.00161414\n",
      "Iteration 5045, loss = 0.00161309\n",
      "Iteration 5046, loss = 0.00161203\n",
      "Iteration 5047, loss = 0.00161098\n",
      "Iteration 5048, loss = 0.00160993\n",
      "Iteration 5049, loss = 0.00160887\n",
      "Iteration 5050, loss = 0.00160782\n",
      "Iteration 5051, loss = 0.00160677\n",
      "Iteration 5052, loss = 0.00160573\n",
      "Iteration 5053, loss = 0.00160468\n",
      "Iteration 5054, loss = 0.00160363\n",
      "Iteration 5055, loss = 0.00160258\n",
      "Iteration 5056, loss = 0.00160154\n",
      "Iteration 5057, loss = 0.00160049\n",
      "Iteration 5058, loss = 0.00159945\n",
      "Iteration 5059, loss = 0.00159841\n",
      "Iteration 5060, loss = 0.00159736\n",
      "Iteration 5061, loss = 0.00159632\n",
      "Iteration 5062, loss = 0.00159528\n",
      "Iteration 5063, loss = 0.00159424\n",
      "Iteration 5064, loss = 0.00159320\n",
      "Iteration 5065, loss = 0.00159216\n",
      "Iteration 5066, loss = 0.00159113\n",
      "Iteration 5067, loss = 0.00159009\n",
      "Iteration 5068, loss = 0.00158905\n",
      "Iteration 5069, loss = 0.00158802\n",
      "Iteration 5070, loss = 0.00158699\n",
      "Iteration 5071, loss = 0.00158595\n",
      "Iteration 5072, loss = 0.00158492\n",
      "Iteration 5073, loss = 0.00158389\n",
      "Iteration 5074, loss = 0.00158286\n",
      "Iteration 5075, loss = 0.00158183\n",
      "Iteration 5076, loss = 0.00158080\n",
      "Iteration 5077, loss = 0.00157977\n",
      "Iteration 5078, loss = 0.00157874\n",
      "Iteration 5079, loss = 0.00157772\n",
      "Iteration 5080, loss = 0.00157669\n",
      "Iteration 5081, loss = 0.00157566\n",
      "Iteration 5082, loss = 0.00157464\n",
      "Iteration 5083, loss = 0.00157362\n",
      "Iteration 5084, loss = 0.00157259\n",
      "Iteration 5085, loss = 0.00157157\n",
      "Iteration 5086, loss = 0.00157055\n",
      "Iteration 5087, loss = 0.00156953\n",
      "Iteration 5088, loss = 0.00156851\n",
      "Iteration 5089, loss = 0.00156749\n",
      "Iteration 5090, loss = 0.00156647\n",
      "Iteration 5091, loss = 0.00156546\n",
      "Iteration 5092, loss = 0.00156444\n",
      "Iteration 5093, loss = 0.00156343\n",
      "Iteration 5094, loss = 0.00156241\n",
      "Iteration 5095, loss = 0.00156140\n",
      "Iteration 5096, loss = 0.00156038\n",
      "Iteration 5097, loss = 0.00155937\n",
      "Iteration 5098, loss = 0.00155836\n",
      "Iteration 5099, loss = 0.00155735\n",
      "Iteration 5100, loss = 0.00155634\n",
      "Iteration 5101, loss = 0.00155533\n",
      "Iteration 5102, loss = 0.00155432\n",
      "Iteration 5103, loss = 0.00155331\n",
      "Iteration 5104, loss = 0.00155231\n",
      "Iteration 5105, loss = 0.00155130\n",
      "Iteration 5106, loss = 0.00155030\n",
      "Iteration 5107, loss = 0.00154929\n",
      "Iteration 5108, loss = 0.00154829\n",
      "Iteration 5109, loss = 0.00154729\n",
      "Iteration 5110, loss = 0.00154628\n",
      "Iteration 5111, loss = 0.00154528\n",
      "Iteration 5112, loss = 0.00154428\n",
      "Iteration 5113, loss = 0.00154328\n",
      "Iteration 5114, loss = 0.00154228\n",
      "Iteration 5115, loss = 0.00154128\n",
      "Iteration 5116, loss = 0.00154029\n",
      "Iteration 5117, loss = 0.00153929\n",
      "Iteration 5118, loss = 0.00153830\n",
      "Iteration 5119, loss = 0.00153730\n",
      "Iteration 5120, loss = 0.00153631\n",
      "Iteration 5121, loss = 0.00153531\n",
      "Iteration 5122, loss = 0.00153432\n",
      "Iteration 5123, loss = 0.00153333\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      1.00      0.96        13\n",
      "           2       1.00      0.88      0.93        16\n",
      "           3       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.94        36\n",
      "   macro avg       0.93      0.96      0.94        36\n",
      "weighted avg       0.95      0.94      0.94        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 主成分資料\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# hidden_layers = (512,) # one hidden layer\n",
    "# activation = ’relu’ # the default\n",
    "hidden_layers = (30,)\n",
    "activation = 'logistic'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers, verbose = True, \\\n",
    "activation = activation, tol = 1e-6, max_iter = int(1e6))\n",
    "# solver = ’sgd’ # not efficient, need more tuning\n",
    "# solver = ’lbfgs’ # not suitable here\n",
    "solver = 'adam' # default solver\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts)\n",
    "clf_MLP.fit(X_train_pca, y_train)\n",
    "predictions = clf_MLP.predict(X_test_pca)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 輸出內容解析\n",
    "\n",
    "這個輸出部分提供了MLP模型訓練過程的一些信息，以及最終的分類報告。讓我們分析一下：\n",
    "\n",
    "1. **Iteration 1, loss = 1.07226324**:\n",
    "   - 這個部分顯示了訓練的第一次迭代，並給出了該迭代的損失（loss）值。損失值是訓練過程中用來衡量模型預測與實際值之間差異的一個指標。\n",
    "\n",
    "2. **Iteration 2, loss = 1.06394549**:\n",
    "   - 類似於第一次迭代，這裡顯示了第二次迭代的損失值。\n",
    "\n",
    "3. **...**:\n",
    "   - 依次類推，這些輸出列出了每次迭代的損失值，直到訓練結束或達到最大迭代次數。\n",
    "\n",
    "4. **accuracy**: 在最後的分類報告中，顯示了模型在測試數據上的準確率為0.97，即97%。\n",
    "\n",
    "5. **macro avg**: 分類報告中的macro avg列顯示了各個指標（精確率、召回率、F1分數等）的平均值。在這裡，macro avg的值為0.97。\n",
    "\n",
    "6. **weighted avg**: weighted avg列顯示了加權平均值，其中每個類別的權重由該類別在測試集中的樣本數決定。在這裡，加權平均準確率為0.97。\n",
    "\n",
    "總的來說，這段輸出部分提供了模型訓練過程中損失的變化情況以及最終模型在測試數據上的表現。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### **畫出測試資料的confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,12))\n",
    "score = 100*clf_MLP.score(X_test_pca, y_test)\n",
    "title = 'Testing score ={:.2f}%'.format(score)\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "clf_MLP,\n",
    "X_test_pca,\n",
    "y_test,\n",
    "xticks_rotation=45, #’vertical’,\n",
    "# display_labels=class_names,\n",
    "cmap=plt.cm.Blues,\n",
    "normalize='true',\n",
    "ax = ax\n",
    ")\n",
    "disp.ax_.set_title(title)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_evnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
