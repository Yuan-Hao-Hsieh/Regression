{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "df = pd.read_csv('face_data.csv')\n",
    "n_persons = df['target'].nunique() \n",
    "X = np.array(df.drop('target', axis=1)) # 400 x 4096\n",
    "y = np.array(df['target'])\n",
    " \n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size) # deafult test_size=0.25\n",
    " \n",
    "# prepare data for PyTorch Tensor\n",
    "X_train = torch.from_numpy(X_train).float() # convert to float tensor\n",
    "y_train = torch.from_numpy(y_train).float() # \n",
    "train_dataset = TensorDataset(X_train, y_train) # create your datset\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "test_dataset = TensorDataset(X_test, y_test) # create your datset\n",
    " \n",
    "# create dataloader for PyTorch\n",
    "batch_size = 64 # 32, 64, 128, 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # convert to dataloader\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(X_test), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=40, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    " \n",
    "# select device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = \"cpu\" # run faster than cuda in some cases\n",
    "print(\"Using {} device\".format(device))\n",
    " \n",
    "# Create a neural network\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(64*64, 512), # image length 64x64=4096,  fully connected layer\n",
    "            nn.ReLU(), # try to take ReLU out to see what happen\n",
    "            nn.Linear(512, 128), # second hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 40) # 40 classes,  fully connected layer\n",
    "            # nn.Softmax()\n",
    "        )\n",
    "    # Specify how data will pass through this model\n",
    "    def forward(self, x):\n",
    "        # out = self.mlp(x) \n",
    " \n",
    "        # Apply softmax to x here~\n",
    "        x = self.mlp(x)\n",
    "        out = F.log_softmax(x, dim=1) # it’s faster and has better numerical propertie than softmax\n",
    "        # out = F.softmax(x, dim=1)\n",
    "        return out\n",
    " \n",
    " \n",
    "# define model, optimizer, loss function\n",
    "model = MLP().to(device) # start an instance\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # default lreaning rate=1e-3\n",
    "loss_fun = nn.CrossEntropyLoss() # define loss function\n",
    " \n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "torch.Size([32, 40])\n",
      "torch.Size([32, 40])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(32, 64 * 64) \n",
    "m1 = nn.Linear(64*64, 512)\n",
    "output = m1(input)\n",
    "print(output.size())\n",
    "output = F.relu(output)\n",
    "print(output.size())\n",
    "m2 = nn.Linear(512, 40)\n",
    "output = m2(output)\n",
    "print(output.size())\n",
    "output = F.log_softmax(output, dim=1)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start trining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 0 | Batch 0 | Loss   3.71\n",
      "\tEpoch 0 | Batch 4 | Loss   3.75\n",
      "Epoch 0 | Loss   3.77 | train accuracy 0.0214\n",
      "\tEpoch 1 | Batch 0 | Loss   3.79\n",
      "\tEpoch 1 | Batch 4 | Loss   3.74\n",
      "Epoch 1 | Loss   3.73 | train accuracy 0.0321\n",
      "\tEpoch 2 | Batch 0 | Loss   3.67\n",
      "\tEpoch 2 | Batch 4 | Loss   3.66\n",
      "Epoch 2 | Loss   3.64 | train accuracy 0.0321\n",
      "\tEpoch 3 | Batch 0 | Loss   3.61\n",
      "\tEpoch 3 | Batch 4 | Loss   3.74\n",
      "Epoch 3 | Loss   3.64 | train accuracy 0.0536\n",
      "\tEpoch 4 | Batch 0 | Loss   3.59\n",
      "\tEpoch 4 | Batch 4 | Loss   3.60\n",
      "Epoch 4 | Loss   3.59 | train accuracy 0.0536\n",
      "\tEpoch 5 | Batch 0 | Loss   3.54\n",
      "\tEpoch 5 | Batch 4 | Loss   3.66\n",
      "Epoch 5 | Loss   3.58 | train accuracy 0.0607\n",
      "\tEpoch 6 | Batch 0 | Loss   3.45\n",
      "\tEpoch 6 | Batch 4 | Loss   3.67\n",
      "Epoch 6 | Loss   3.56 | train accuracy 0.0679\n",
      "\tEpoch 7 | Batch 0 | Loss   3.47\n",
      "\tEpoch 7 | Batch 4 | Loss   3.48\n",
      "Epoch 7 | Loss   3.48 | train accuracy 0.0929\n",
      "\tEpoch 8 | Batch 0 | Loss   3.42\n",
      "\tEpoch 8 | Batch 4 | Loss   3.33\n",
      "Epoch 8 | Loss   3.43 | train accuracy 0.1321\n",
      "\tEpoch 9 | Batch 0 | Loss   3.35\n",
      "\tEpoch 9 | Batch 4 | Loss   3.34\n",
      "Epoch 9 | Loss   3.39 | train accuracy 0.1321\n",
      "\tEpoch 10 | Batch 0 | Loss   3.27\n",
      "\tEpoch 10 | Batch 4 | Loss   3.40\n",
      "Epoch 10 | Loss   3.34 | train accuracy 0.2036\n",
      "\tEpoch 11 | Batch 0 | Loss   3.31\n",
      "\tEpoch 11 | Batch 4 | Loss   3.22\n",
      "Epoch 11 | Loss   3.25 | train accuracy 0.1607\n",
      "\tEpoch 12 | Batch 0 | Loss   3.21\n",
      "\tEpoch 12 | Batch 4 | Loss   3.35\n",
      "Epoch 12 | Loss   3.18 | train accuracy 0.2179\n",
      "\tEpoch 13 | Batch 0 | Loss   3.22\n",
      "\tEpoch 13 | Batch 4 | Loss   2.94\n",
      "Epoch 13 | Loss   3.04 | train accuracy 0.2750\n",
      "\tEpoch 14 | Batch 0 | Loss   2.96\n",
      "\tEpoch 14 | Batch 4 | Loss   2.87\n",
      "Epoch 14 | Loss   2.94 | train accuracy 0.3393\n",
      "\tEpoch 15 | Batch 0 | Loss   2.90\n",
      "\tEpoch 15 | Batch 4 | Loss   2.54\n",
      "Epoch 15 | Loss   2.80 | train accuracy 0.3321\n",
      "\tEpoch 16 | Batch 0 | Loss   2.57\n",
      "\tEpoch 16 | Batch 4 | Loss   2.66\n",
      "Epoch 16 | Loss   2.69 | train accuracy 0.3429\n",
      "\tEpoch 17 | Batch 0 | Loss   2.61\n",
      "\tEpoch 17 | Batch 4 | Loss   2.68\n",
      "Epoch 17 | Loss   2.61 | train accuracy 0.3929\n",
      "\tEpoch 18 | Batch 0 | Loss   2.55\n",
      "\tEpoch 18 | Batch 4 | Loss   2.65\n",
      "Epoch 18 | Loss   2.47 | train accuracy 0.4000\n",
      "\tEpoch 19 | Batch 0 | Loss   2.27\n",
      "\tEpoch 19 | Batch 4 | Loss   2.23\n",
      "Epoch 19 | Loss   2.26 | train accuracy 0.4679\n",
      "\tEpoch 20 | Batch 0 | Loss   2.21\n",
      "\tEpoch 20 | Batch 4 | Loss   1.95\n",
      "Epoch 20 | Loss   2.15 | train accuracy 0.4750\n",
      "\tEpoch 21 | Batch 0 | Loss   2.19\n",
      "\tEpoch 21 | Batch 4 | Loss   1.92\n",
      "Epoch 21 | Loss   2.04 | train accuracy 0.4857\n",
      "\tEpoch 22 | Batch 0 | Loss   1.80\n",
      "\tEpoch 22 | Batch 4 | Loss   1.96\n",
      "Epoch 22 | Loss   1.91 | train accuracy 0.5607\n",
      "\tEpoch 23 | Batch 0 | Loss   1.71\n",
      "\tEpoch 23 | Batch 4 | Loss   1.65\n",
      "Epoch 23 | Loss   1.77 | train accuracy 0.5786\n",
      "\tEpoch 24 | Batch 0 | Loss   1.63\n",
      "\tEpoch 24 | Batch 4 | Loss   1.85\n",
      "Epoch 24 | Loss   1.71 | train accuracy 0.5893\n",
      "\tEpoch 25 | Batch 0 | Loss   1.63\n",
      "\tEpoch 25 | Batch 4 | Loss   1.75\n",
      "Epoch 25 | Loss   1.64 | train accuracy 0.6500\n",
      "\tEpoch 26 | Batch 0 | Loss   1.45\n",
      "\tEpoch 26 | Batch 4 | Loss   1.38\n",
      "Epoch 26 | Loss   1.50 | train accuracy 0.6714\n",
      "\tEpoch 27 | Batch 0 | Loss   1.59\n",
      "\tEpoch 27 | Batch 4 | Loss   1.24\n",
      "Epoch 27 | Loss   1.45 | train accuracy 0.6500\n",
      "\tEpoch 28 | Batch 0 | Loss   1.42\n",
      "\tEpoch 28 | Batch 4 | Loss   1.33\n",
      "Epoch 28 | Loss   1.33 | train accuracy 0.6893\n",
      "\tEpoch 29 | Batch 0 | Loss   1.40\n",
      "\tEpoch 29 | Batch 4 | Loss   1.15\n",
      "Epoch 29 | Loss   1.26 | train accuracy 0.7143\n",
      "\tEpoch 30 | Batch 0 | Loss   1.29\n",
      "\tEpoch 30 | Batch 4 | Loss   1.51\n",
      "Epoch 30 | Loss   1.29 | train accuracy 0.7000\n",
      "\tEpoch 31 | Batch 0 | Loss   1.20\n",
      "\tEpoch 31 | Batch 4 | Loss   1.18\n",
      "Epoch 31 | Loss   1.24 | train accuracy 0.7071\n",
      "\tEpoch 32 | Batch 0 | Loss   1.04\n",
      "\tEpoch 32 | Batch 4 | Loss   1.06\n",
      "Epoch 32 | Loss   1.11 | train accuracy 0.7393\n",
      "\tEpoch 33 | Batch 0 | Loss   1.04\n",
      "\tEpoch 33 | Batch 4 | Loss   1.06\n",
      "Epoch 33 | Loss   1.09 | train accuracy 0.7357\n",
      "\tEpoch 34 | Batch 0 | Loss   0.98\n",
      "\tEpoch 34 | Batch 4 | Loss   0.77\n",
      "Epoch 34 | Loss   0.96 | train accuracy 0.7821\n",
      "\tEpoch 35 | Batch 0 | Loss   0.87\n",
      "\tEpoch 35 | Batch 4 | Loss   0.89\n",
      "Epoch 35 | Loss   0.92 | train accuracy 0.8214\n",
      "\tEpoch 36 | Batch 0 | Loss   0.89\n",
      "\tEpoch 36 | Batch 4 | Loss   0.83\n",
      "Epoch 36 | Loss   0.89 | train accuracy 0.7964\n",
      "\tEpoch 37 | Batch 0 | Loss   0.65\n",
      "\tEpoch 37 | Batch 4 | Loss   0.92\n",
      "Epoch 37 | Loss   0.87 | train accuracy 0.8071\n",
      "\tEpoch 38 | Batch 0 | Loss   0.81\n",
      "\tEpoch 38 | Batch 4 | Loss   0.75\n",
      "Epoch 38 | Loss   0.81 | train accuracy 0.8250\n",
      "\tEpoch 39 | Batch 0 | Loss   0.69\n",
      "\tEpoch 39 | Batch 4 | Loss   0.75\n",
      "Epoch 39 | Loss   0.78 | train accuracy 0.8536\n",
      "\tEpoch 40 | Batch 0 | Loss   0.66\n",
      "\tEpoch 40 | Batch 4 | Loss   0.72\n",
      "Epoch 40 | Loss   0.74 | train accuracy 0.8750\n",
      "\tEpoch 41 | Batch 0 | Loss   0.72\n",
      "\tEpoch 41 | Batch 4 | Loss   0.68\n",
      "Epoch 41 | Loss   0.72 | train accuracy 0.8429\n",
      "\tEpoch 42 | Batch 0 | Loss   0.73\n",
      "\tEpoch 42 | Batch 4 | Loss   0.43\n",
      "Epoch 42 | Loss   0.63 | train accuracy 0.8607\n",
      "\tEpoch 43 | Batch 0 | Loss   0.58\n",
      "\tEpoch 43 | Batch 4 | Loss   0.50\n",
      "Epoch 43 | Loss   0.59 | train accuracy 0.9000\n",
      "\tEpoch 44 | Batch 0 | Loss   0.66\n",
      "\tEpoch 44 | Batch 4 | Loss   0.65\n",
      "Epoch 44 | Loss   0.59 | train accuracy 0.9179\n",
      "\tEpoch 45 | Batch 0 | Loss   0.51\n",
      "\tEpoch 45 | Batch 4 | Loss   0.58\n",
      "Epoch 45 | Loss   0.56 | train accuracy 0.9179\n",
      "\tEpoch 46 | Batch 0 | Loss   0.57\n",
      "\tEpoch 46 | Batch 4 | Loss   0.54\n",
      "Epoch 46 | Loss   0.53 | train accuracy 0.9321\n",
      "\tEpoch 47 | Batch 0 | Loss   0.53\n",
      "\tEpoch 47 | Batch 4 | Loss   0.57\n",
      "Epoch 47 | Loss   0.51 | train accuracy 0.9286\n",
      "\tEpoch 48 | Batch 0 | Loss   0.50\n",
      "\tEpoch 48 | Batch 4 | Loss   0.46\n",
      "Epoch 48 | Loss   0.48 | train accuracy 0.9250\n",
      "\tEpoch 49 | Batch 0 | Loss   0.46\n",
      "\tEpoch 49 | Batch 4 | Loss   0.55\n",
      "Epoch 49 | Loss   0.46 | train accuracy 0.9536\n",
      "Finished ... Loss  0.4647 | train accuracy 0.9536\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    " \n",
    "epochs = 50 # Repeat the whole dataset epochs times\n",
    "model.train() # Sets the module in training mode. The training model allow the parameters to be updated during backpropagation.\n",
    "for epoch in range(epochs):\n",
    "# for epoch in tqdm(range(epochs)):\n",
    "    trainAcc = 0\n",
    "    samples = 0\n",
    "    losses = []\n",
    "    for batch_num, input_data in enumerate(train_loader):\n",
    "    # for batch_num, input_data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "         \n",
    "        x, y = input_data\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device)\n",
    " \n",
    "        # perform training based on the backpropagation\n",
    "        y_pre = model(x) # predict y\n",
    "        loss = loss_fun(y_pre, y.long()) # the loss function nn.CrossEntropyLoss()\n",
    "        losses.append(loss.item())\n",
    " \n",
    "        optimizer.zero_grad() # Zeros the gradients accumulated from the previous batch/step of the model\n",
    "        loss.backward() # Performs backpropagation and calculates the gradient for each parameter\n",
    "        optimizer.step() # updates the weights based on the gradients of the parameters.\n",
    "         \n",
    "        # Record the training accuracy for each batch\n",
    "        trainAcc += (y_pre.argmax(dim=1) == y).sum().item() # comparison\n",
    "        samples += y.size(0)\n",
    "        if batch_num % 4 == 0:\n",
    "            print('\\tEpoch %d | Batch %d | Loss %6.2f' % (epoch, batch_num, loss.item()))\n",
    "    print('Epoch %d | Loss %6.2f | train accuracy %.4f' % (epoch, sum(losses)/len(losses), trainAcc/samples))\n",
    " \n",
    "print('Finished ... Loss %7.4f | train accuracy %.4f' % (sum(losses)/len(losses), trainAcc/samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:0.767\n"
     ]
    }
   ],
   "source": [
    "model.eval() \n",
    "testAcc = 0\n",
    "samples = 0\n",
    "with torch.no_grad():\n",
    "    for x, y_truth in test_loader:\n",
    "        x = x.to(device).float()\n",
    "        y_truth = y_truth.to(device)\n",
    "        y_pre = model(x).argmax(dim=1) # the predictions for the batch\n",
    "        testAcc += (y_pre == y_truth).sum().item() # comparison\n",
    "        samples += y_truth.size(0)\n",
    " \n",
    "    print('Test Accuracy:{:.3f}'.format(testAcc/samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:0.767\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    " \n",
    "# use eval() in conjunction with a torch.no_grad() context, \n",
    "# meaning that gradient computation is turned off in evaluation mode\n",
    "model.eval() \n",
    "testAcc = 0\n",
    "samples = 0\n",
    " \n",
    "with open('mlp_att.csv', 'w') as f:\n",
    "    fieldnames = ['ImageId', 'Label', 'Ground_Truth']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, lineterminator = '\\n')\n",
    "    writer.writeheader()\n",
    "    image_id = 1\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for x, y_truth in test_loader:\n",
    "            x = x.to(device).float()\n",
    "            y_truth = y_truth.to(device).long()\n",
    "            yIdx = 0\n",
    "            y_pre = model(x).argmax(dim=1) # the predictions for the batch\n",
    "            testAcc += (y_pre == y_truth).sum().item() # comparison\n",
    "            samples += y_truth.size(0)\n",
    "            for y in y_pre:\n",
    "                writer.writerow({fieldnames[0]: image_id,fieldnames[1]: y.item(), fieldnames[2]: y_truth[yIdx].item()})\n",
    "                image_id += 1\n",
    "                yIdx += 1\n",
    " \n",
    "        print('Test Accuracy:{:.3f}'.format(testAcc/samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 範例 2：PyTorch 的深度機器學習 CNN 範例：以手寫數字辨識為例。程式部分參考官網範例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_openml\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_evnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
